<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns="http://purl.org/rss/1.0/"
    xmlns:content="http://purl.org/rss/1.0/modules/content/"
    xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
    xmlns:dc="http://purl.org/dc/elements/1.1/"
    xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
    xmlns:admin="http://webns.net/mvcb/"
    >

  <channel rdf:about="http://arxiv.org/">
    <title>cs updates on arXiv.org</title>
    <link>http://arxiv.org/</link>
    <description rdf:parseType="Literal">Computer Science (cs) updates on the arXiv.org e-print archive</description>
    <dc:language>en-us</dc:language>
    <dc:date>2018-08-30T20:30:00-05:00</dc:date>
    <dc:publisher>www-admin@arxiv.org</dc:publisher>
    <dc:subject>Computer Science</dc:subject>
    <syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
    <syn:updateFrequency>1</syn:updateFrequency>
    <syn:updatePeriod>daily</syn:updatePeriod>
    <items>
      <rdf:Seq>
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.09955" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.09964" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.09987" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.09996" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.09999" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10000" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10002" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10005" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10006" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10009" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10010" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10011" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10012" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10013" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10016" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10024" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10025" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10026" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10031" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10032" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10033" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10038" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10044" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10056" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10059" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10062" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10064" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10068" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10069" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10072" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10073" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10075" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10078" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10082" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10083" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10086" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10093" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10095" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10097" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10101" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10104" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10105" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10108" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10113" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10117" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10120" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10122" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10128" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10134" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10137" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10143" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10144" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10145" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10146" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10151" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10166" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10180" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10191" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10192" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10196" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10203" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10209" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10217" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10231" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10232" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10239" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10240" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10245" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10250" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10259" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10260" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10261" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10262" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10267" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10290" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10292" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10300" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10307" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10308" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10313" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10316" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10322" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10326" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10328" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10340" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10350" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10351" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10356" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10363" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10364" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10366" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10367" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10369" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10370" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10375" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10379" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10383" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10387" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10389" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10393" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10394" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10396" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10399" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10400" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10406" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10410" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10420" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10427" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10430" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10432" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.10437" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1301.1107" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1309.3014" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1602.05901" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1602.07273" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1604.04967" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1607.02951" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1701.05408" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1703.03391" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1703.08750" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1703.10593" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1704.04587" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1705.04253" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1706.02586" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1706.09806" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1707.02572" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1707.02933" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1707.07821" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1709.05369" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1709.08294" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1711.08571" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1801.04403" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1801.07939" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1802.00047" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1802.01355" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1802.05203" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1802.10238" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1803.04337" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1803.07813" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1803.11475" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1804.01861" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1804.01932" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1804.02960" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1804.05484" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1804.07873" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1805.02530" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1805.02836" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1805.04182" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1805.08716" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1805.09190" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1805.11898" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1806.01344" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1806.02059" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1806.02711" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1806.02908" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1806.06759" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1806.06827" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1806.08206" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1806.09246" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1807.00661" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1807.05153" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.02129" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.02933" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.03737" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.05163" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.05293" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.06052" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.06791" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.07380" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.08316" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.08518" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.08575" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.08671" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.08763" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.08794" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.09034" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.09124" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.09218" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.09492" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.09526" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.09772" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.09830" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.09888" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.09891" />
        <rdf:li rdf:resource="http://arxiv.org/abs/1808.09902" />
      </rdf:Seq>
    </items>
    <image rdf:resource="http://arxiv.org/icons/sfx.gif" />
  </channel>
  <image rdf:about="http://arxiv.org/icons/sfx.gif">
    <title>arXiv.org</title>
    <url>http://arxiv.org/icons/sfx.gif</url>
    <link>http://arxiv.org/</link>
  </image>
  <item rdf:about="http://arxiv.org/abs/1808.09955">
    <title>QuasarNET: Human-level spectral classification and redshifting with Deep Neural Networks. (arXiv:1808.09955v1 [astro-ph.IM])</title>
    <link>http://arxiv.org/abs/1808.09955</link>
    <description rdf:parseType="Literal">&lt;p&gt;We introduce QuasarNET, a deep convolutional neural network that performs
    classification and redshift estimation of astrophysical spectra with
    human-expert accuracy. We pose these two tasks as a \emph{feature detection}
    problem: presence or absence of spectral features determines the class, and
    their wavelength determines the redshift, very much like human-experts proceed.
    When ran on BOSS data to identify quasars through their emission lines,
    QuasarNET defines a sample $99.51\pm0.03$\% pure and $99.52\pm0.03$\% complete,
    well above the requirements of many analyses using these data. QuasarNET
    significantly reduces the problem of line-confusion that induces catastrophic
    redshift failures to below 0.2\%. We also extend QuasarNET to classify spectra
    with broad absorption line (BAL) features, achieving an accuracy of
    $98.0\pm0.4$\% for recognizing BAL and $97.0\pm0.2$\% for rejecting non-BAL
    quasars. QuasarNET is trained on data of low signal-to-noise and medium
    resolution, typical of current and future astrophysical surveys, and could be
    easily applied to classify spectra from current and upcoming surveys such as
    eBOSS, DESI and 4MOST.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Busca_N/0/1/0/all/0/1&quot;&gt;Nicolas Busca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Balland_C/0/1/0/all/0/1&quot;&gt;Christophe Balland&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.09964">
    <title>Semi-Metrification of the Dynamic Time Warping Distance. (arXiv:1808.09964v1 [cs.LG])</title>
    <link>http://arxiv.org/abs/1808.09964</link>
    <description rdf:parseType="Literal">&lt;p&gt;The dynamic time warping (dtw) distance fails to satisfy the triangle
    inequality and the identity of indiscernibles. As a consequence, the
    dtw-distance is not warping-invariant, which in turn results in peculiarities
    in data mining applications. This article converts the dtw-distance to a
    semi-metric and shows that its canonical extension is warping-invariant.
    Empirical results indicate that the nearest-neighbor classifier in the proposed
    semi-metric space performs comparable to the same classifier in the standard
    dtw-space. To overcome the undesirable peculiarities of dtw-spaces, this result
    suggest to further explore the semi-metric space for data mining applications.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_B/0/1/0/all/0/1&quot;&gt;Brijnesh J. Jain&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.09987">
    <title>Submodular Maximization with Packing Constraints in Parallel. (arXiv:1808.09987v1 [cs.DS])</title>
    <link>http://arxiv.org/abs/1808.09987</link>
    <description rdf:parseType="Literal">&lt;p&gt;We consider the problem of maximizing the multilinear extension of a
    submodular function subject to packing constraints in parallel. For monotone
    functions, we obtain a $1-1/e-\epsilon$ approximation using
    $O(\log(n/\epsilon)\log(m)/\epsilon^2)$ rounds of adaptivity and evaluations of
    the function and its gradient, where $m$ is the number of packing constraints
    and $n$ is the number of variables. For non-monotone functions, we obtain a
    $1/e-\epsilon$ approximation using
    $O(\log(n/\epsilon)\log(1/\epsilon)\log(n+m)/\epsilon^2)$ rounds of adaptivity
    and evaluations of the function and its gradient. Our results apply more
    generally to the problem of maximizing a diminishing returns submodular
    (DR-submodular) function subject to packing constraints.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ene_A/0/1/0/all/0/1&quot;&gt;Alina Ene&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Huy L. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vladu_A/0/1/0/all/0/1&quot;&gt;Adrian Vladu&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.09996">
    <title>Learning End-to-End Goal-Oriented Dialog with Multiple Answers. (arXiv:1808.09996v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.09996</link>
    <description rdf:parseType="Literal">&lt;p&gt;In a dialog, there can be multiple valid next utterances at any point. The
    present end-to-end neural methods for dialog do not take this into account.
    They learn with the assumption that at any time there is only one correct next
    utterance. In this work, we focus on this problem in the goal-oriented dialog
    setting where there are different paths to reach a goal. We propose a new
    method, that uses a combination of supervised learning and reinforcement
    learning approaches to address this issue. We also propose a new and more
    effective testbed, permuted-bAbI dialog tasks, by introducing multiple valid
    next utterances to the original-bAbI dialog tasks, which allows evaluation of
    goal-oriented dialog systems in a more realistic setting. We show that there is
    a significant drop in performance of existing end-to-end neural methods from
    81.5% per-dialog accuracy on original-bAbI dialog tasks to 30.3% on
    permuted-bAbI dialog tasks. We also show that our proposed method improves the
    performance and achieves 47.3% per-dialog accuracy on permuted-bAbI dialog
    tasks.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajendran_J/0/1/0/all/0/1&quot;&gt;Janarthanan Rajendran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganhotra_J/0/1/0/all/0/1&quot;&gt;Jatin Ganhotra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Satinder Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polymenakos_L/0/1/0/all/0/1&quot;&gt;Lazaros Polymenakos&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.09999">
    <title>MemComputing Integer Linear Programming. (arXiv:1808.09999v1 [cs.ET])</title>
    <link>http://arxiv.org/abs/1808.09999</link>
    <description rdf:parseType="Literal">&lt;p&gt;Integer linear programming (ILP) encompasses a very important class of
    optimization problems that are of great interest to both academia and industry.
    Several algorithms are available that attempt to explore the solution space of
    this class efficiently, while requiring a reasonable compute time. However,
    although these algorithms have reached various degrees of success over the
    years, they still face considerable challenges when confronted with
    particularly hard problem instances, such as those of the MIPLIB 2010 library.
    In this work we propose a radically different non-algorithmic approach to ILP
    based on a novel physics-inspired computing paradigm: Memcomputing. This
    paradigm is based on digital (hence scalable) machines represented by
    appropriate electrical circuits with memory. These machines can be either built
    in hardware or, as we do here, their equations of motion can be efficiently
    simulated on our traditional computers. We first describe a new circuit
    architecture of memcomputing machines specifically designed to solve for the
    linear inequalities representing a general ILP problem. We call these
    self-organizing algebraic circuits, since they self-organize dynamically to
    satisfy the correct (algebraic) linear inequalities. We then show simulations
    of these machines using MATLAB running on a single core of a Xeon processor for
    several ILP benchmark problems taken from the MIPLIB 2010 library, and compare
    our results against a renowned commercial solver. We show that our approach is
    very efficient when dealing with these hard problems. In particular, we find
    within minutes feasible solutions for one of these hard problems (f2000 from
    MIPLIB 2010) whose feasibility, to the best of our knowledge, has remained
    unknown for the past eight years.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Traversa_F/0/1/0/all/0/1&quot;&gt;Fabio L. Traversa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ventra_M/0/1/0/all/0/1&quot;&gt;Massimiliano Di Ventra&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10000">
    <title>Grammar Induction with Neural Language Models: An Unusual Replication. (arXiv:1808.10000v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10000</link>
    <description rdf:parseType="Literal">&lt;p&gt;A substantial thread of recent work on latent tree learning has attempted to
    develop neural network models with parse-valued latent variables and train them
    on non-parsing tasks, in the hope of having them discover interpretable tree
    structure. In a recent paper, Shen et al. (2018) introduce such a model and
    report near-state-of-the-art results on the target task of language modeling,
    and the first strong latent tree learning result on constituency parsing. In an
    attempt to reproduce these results, we discover issues that make the original
    results hard to trust, including tuning and even training on what is
    effectively the test set. Here, we attempt to reproduce these results in a fair
    experiment and to extend them to two new datasets. We find that the results of
    this work are robust: All variants of the model under study outperform all
    latent tree learning baselines, and perform competitively with symbolic grammar
    induction systems. We find that this model represents the first empirical
    success for latent tree learning, and that neural network language modeling
    warrants further study as a setting for grammar induction.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Htut_P/0/1/0/all/0/1&quot;&gt;Phu Mon Htut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1&quot;&gt;Samuel R. Bowman&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10002">
    <title>Interpretable Intuitive Physics Model. (arXiv:1808.10002v1 [cs.CV])</title>
    <link>http://arxiv.org/abs/1808.10002</link>
    <description rdf:parseType="Literal">&lt;p&gt;Humans have a remarkable ability to use physical commonsense and predict the
    effect of collisions. But do they understand the underlying factors? Can they
    predict if the underlying factors have changed? Interestingly, in most cases
    humans can predict the effects of similar collisions with different conditions
    such as changes in mass, friction, etc. It is postulated this is primarily
    because we learn to model physics with meaningful latent variables. This does
    not imply we can estimate the precise values of these meaningful variables
    (estimate exact values of mass or friction). Inspired by this observation, we
    propose an interpretable intuitive physics model where specific dimensions in
    the bottleneck layers correspond to different physical properties. In order to
    demonstrate that our system models these underlying physical properties, we
    train our model on collisions of different shapes (cube, cone, cylinder,
    spheres etc.) and test on collisions of unseen combinations of shapes.
    Furthermore, we demonstrate our model generalizes well even when similar scenes
    are simulated with different underlying properties.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Tian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davidson_J/0/1/0/all/0/1&quot;&gt;James Davidson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhinav Gupta&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10005">
    <title>Recognition and Drawing of Stick Graphs. (arXiv:1808.10005v1 [cs.CG])</title>
    <link>http://arxiv.org/abs/1808.10005</link>
    <description rdf:parseType="Literal">&lt;p&gt;A \emph{Stick graph} is an intersection graph of axis-aligned segments such
    that the left end-points of the horizontal segments and the bottom end-points
    of the vertical segments lie on a `ground line,&apos; a line with slope $-1$. It is
    an open question to decide in polynomial time whether a given bipartite graph
    $G$ with bipartition $A\cup B$ has a Stick representation where the vertices in
    $A$ and $B$ correspond to horizontal and vertical segments, respectively. We
    prove that $G$ has a Stick representation if and only if there are orderings of
    $A$ and $B$ such that $G$&apos;s bipartite adjacency matrix with rows $A$ and
    columns $B$ excludes three small `forbidden&apos; submatrices. This is similar to
    characterizations for other classes of bipartite intersection graphs.
    &lt;/p&gt;
    &lt;p&gt;We present an algorithm to test whether given orderings of $A$ and $B$ permit
    a Stick representation respecting those orderings, and to find such a
    representation if it exists. The algorithm runs in time linear in the size of
    the adjacency matrix. For the case when only the ordering of $A$ is given, we
    present an $O(|A|^3|B|^3)$-time algorithm. When neither ordering is given, we
    present some partial results about graphs that are, or are not, Stick
    representable.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luca_F/0/1/0/all/0/1&quot;&gt;Felice De Luca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1&quot;&gt;Md Iqbal Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobourov_S/0/1/0/all/0/1&quot;&gt;Stephen Kobourov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lubiw_A/0/1/0/all/0/1&quot;&gt;Anna Lubiw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_D/0/1/0/all/0/1&quot;&gt;Debajyoti Mondal&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10006">
    <title>Correcting Length Bias in Neural Machine Translation. (arXiv:1808.10006v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10006</link>
    <description rdf:parseType="Literal">&lt;p&gt;We study two problems in neural machine translation (NMT). First, in beam
    search, whereas a wider beam should in principle help translation, it often
    hurts NMT. Second, NMT has a tendency to produce translations that are too
    short. Here, we argue that these problems are closely related and both rooted
    in label bias. We show that correcting the brevity problem almost eliminates
    the beam problem; we compare some commonly-used methods for doing this, finding
    that a simple per-word reward works well; and we introduce a simple and quick
    way to tune this reward using the perceptron algorithm.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1&quot;&gt;Kenton Murray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1&quot;&gt;David Chiang&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10009">
    <title>Learning a Policy for Opportunistic Active Learning. (arXiv:1808.10009v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10009</link>
    <description rdf:parseType="Literal">&lt;p&gt;Active learning identifies data points to label that are expected to be the
    most useful in improving a supervised model. Opportunistic active learning
    incorporates active learning into interactive tasks that constrain possible
    queries during interactions. Prior work has shown that opportunistic active
    learning can be used to improve grounding of natural language descriptions in
    an interactive object retrieval task. In this work, we use reinforcement
    learning for such an object retrieval task, to learn a policy that effectively
    trades off task completion with model improvement that would benefit future
    tasks.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1&quot;&gt;Aishwarya Padmakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1&quot;&gt;Peter Stone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1&quot;&gt;Raymond J. Mooney&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10010">
    <title>Design of an Autonomous Precision Pollination Robot. (arXiv:1808.10010v1 [cs.RO])</title>
    <link>http://arxiv.org/abs/1808.10010</link>
    <description rdf:parseType="Literal">&lt;p&gt;Precision robotic pollination systems can not only fill the gap of declining
    natural pollinators, but can also surpass them in efficiency and uniformity,
    helping to feed the fast-growing human population on Earth. This paper presents
    the design and ongoing development of an autonomous robot named &quot;BrambleBee&quot;,
    which aims at pollinating bramble plants in a greenhouse environment. Partially
    inspired by the ecology and behavior of bees, BrambleBee employs
    state-of-the-art localization and mapping, visual perception, path planning,
    motion control, and manipulation techniques to create an efficient and robust
    autonomous pollination system.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohi_N/0/1/0/all/0/1&quot;&gt;Nicholas Ohi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lassak_K/0/1/0/all/0/1&quot;&gt;Kyle Lassak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watson_R/0/1/0/all/0/1&quot;&gt;Ryan Watson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strader_J/0/1/0/all/0/1&quot;&gt;Jared Strader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yixin Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chizhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hedrick_G/0/1/0/all/0/1&quot;&gt;Gabrielle Hedrick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_J/0/1/0/all/0/1&quot;&gt;Jennifer Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harper_S/0/1/0/all/0/1&quot;&gt;Scott Harper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reynolds_D/0/1/0/all/0/1&quot;&gt;Dylan Reynolds&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kilic_C/0/1/0/all/0/1&quot;&gt;Cagri Kilic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hikes_J/0/1/0/all/0/1&quot;&gt;Jacob Hikes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mills_S/0/1/0/all/0/1&quot;&gt;Sarah Mills&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castle_C/0/1/0/all/0/1&quot;&gt;Conner Castle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buzzo_B/0/1/0/all/0/1&quot;&gt;Benjamin Buzzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waterland_N/0/1/0/all/0/1&quot;&gt;Nicole Waterland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_J/0/1/0/all/0/1&quot;&gt;Jason Gross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;Yong-Lak Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yu Gu&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10011">
    <title>Fast and accessible first-principles calculations of vibrational properties of materials. (arXiv:1808.10011v1 [cond-mat.mtrl-sci])</title>
    <link>http://arxiv.org/abs/1808.10011</link>
    <description rdf:parseType="Literal">&lt;p&gt;We present example applications of an approach to first-principles
    calculations of vibrational properties of materials implemented within the
    Exabyte.io platform. We deploy models based on the Density Functional
    Perturbation Theory to extract the phonon dispersion relations and densities of
    states for an example set of 35 samples and find the results to be in agreement
    with prior similar calculations. We construct modeling workflows that are both
    accessible, accurate, and efficient with respect to the human time involved.
    This is achieved through efficient parallelization of the tasks for the
    individual vibrational modes. We report achieved speedups in the 10-100 range,
    approximately, and maximum attainable speedups in the 30-300 range,
    correspondingly. We analyze the execution times on the current up-to-date
    computational infrastructure centrally available from a public cloud provider.
    Results and all associated data, including the materials and simulation
    workflows, are made available online in an accessible, repeatable and
    extensible setting.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Bazhirov_T/0/1/0/all/0/1&quot;&gt;Timur Bazhirov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Abot_E/0/1/0/all/0/1&quot;&gt;E. X. Abot&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10012">
    <title>Reasoning about Actions and State Changes by Injecting Commonsense Knowledge. (arXiv:1808.10012v1 [cs.AI])</title>
    <link>http://arxiv.org/abs/1808.10012</link>
    <description rdf:parseType="Literal">&lt;p&gt;Comprehending procedural text, e.g., a paragraph describing photosynthesis,
    requires modeling actions and the state changes they produce, so that questions
    about entities at different timepoints can be answered. Although several recent
    systems have shown impressive progress in this task, their predictions can be
    globally inconsistent or highly improbable. In this paper, we show how the
    predicted effects of actions in the context of a paragraph can be improved in
    two ways: (1) by incorporating global, commonsense constraints (e.g., a
    non-existent entity cannot be destroyed), and (2) by biasing reading with
    preferences from large-scale corpora (e.g., trees rarely move). Unlike earlier
    methods, we treat the problem as a neural structured prediction task, allowing
    hard and soft constraints to steer the model away from unlikely predictions. We
    show that the new model significantly outperforms earlier systems on a
    benchmark dataset for procedural text comprehension (+8% relative gain), and
    that it also avoids some of the nonsensical predictions that earlier systems
    make.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1&quot;&gt;Niket Tandon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1&quot;&gt;Bhavana Dalvi Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grus_J/0/1/0/all/0/1&quot;&gt;Joel Grus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1&quot;&gt;Wen-tau Yih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1&quot;&gt;Antoine Bosselut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1&quot;&gt;Peter Clark&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10013">
    <title>Group calibration is a byproduct of unconstrained learning. (arXiv:1808.10013v1 [cs.LG])</title>
    <link>http://arxiv.org/abs/1808.10013</link>
    <description rdf:parseType="Literal">&lt;p&gt;Much recent work on fairness in machine learning has focused on how well a
    score function is calibrated in different groups within a given population,
    where each group is defined by restricting one or more sensitive attributes.
    &lt;/p&gt;
    &lt;p&gt;We investigate to which extent group calibration follows from unconstrained
    empirical risk minimization on its own, without the need for any explicit
    intervention. We show that under reasonable conditions, the deviation from
    satisfying group calibration is bounded by the excess loss of the empirical
    risk minimizer relative to the Bayes optimal score function. As a corollary, it
    follows that empirical risk minimization can simultaneously achieve calibration
    for many groups, a task that prior work deferred to highly complex algorithms.
    We complement our results with a lower bound, and a range of experimental
    findings.
    &lt;/p&gt;
    &lt;p&gt;Our results challenge the view that group calibration necessitates an active
    intervention, suggesting that often we ought to think of it as a byproduct of
    unconstrained machine learning.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lydia T. Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1&quot;&gt;Max Simchowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1&quot;&gt;Moritz Hardt&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10016">
    <title>Consistent Sampling with Replacement. (arXiv:1808.10016v1 [cs.DS])</title>
    <link>http://arxiv.org/abs/1808.10016</link>
    <description rdf:parseType="Literal">&lt;p&gt;We describe a very simple method for `consistent sampling&apos; that allows for
    sampling with replacement. The method extends previous approaches to consistent
    sampling, which assign a pseudorandom real number to each element, and sample
    those with the smallest associated numbers. When sampling with replacement, our
    extension gives the item sampled a new, larger, associated pseudorandom number,
    and returns it to the pool of items being sampled.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivest_R/0/1/0/all/0/1&quot;&gt;Ronald L. Rivest&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10024">
    <title>Hard Non-Monotonic Attention for Character-Level Transduction. (arXiv:1808.10024v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10024</link>
    <description rdf:parseType="Literal">&lt;p&gt;Character-level string-to-string transduction is an important component of
    various NLP tasks. The goal is to map an input string to an output string,
    where the strings may be of different lengths and have characters taken from
    different alphabets. Recent approaches have used sequence-to-sequence models
    with an attention mechanism to learn which parts of the input string the model
    should focus on during the generation of the output string. Both soft attention
    and hard monotonic attention have been used, but hard non-monotonic attention
    has only been used in other sequence modeling tasks such as image captioning
    and has required a stochastic approximation to compute the gradient. In this
    work, we introduce an exact, polynomial-time algorithm for marginalizing over
    the exponential number of non-monotonic alignments between two strings, showing
    that hard attention models can be viewed as neural reparameterizations of the
    classical IBM Model 1. We compare soft and hard non-monotonic attention
    experimentally and find that the exact algorithm significantly improves
    performance over the stochastic approximation and outperforms soft attention.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shijie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapiro_P/0/1/0/all/0/1&quot;&gt;Pamela Shapiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1&quot;&gt;Ryan Cotterell&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10025">
    <title>Retrieval-Based Neural Code Generation. (arXiv:1808.10025v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10025</link>
    <description rdf:parseType="Literal">&lt;p&gt;In models to generate program source code from natural language, representing
    this code in a tree structure has been a common approach. However, existing
    methods often fail to generate complex code correctly due to a lack of ability
    to memorize large and complex structures. We introduce ReCode, a method based
    on subtree retrieval that makes it possible to explicitly reference existing
    code examples within a neural code generation model. First, we retrieve
    sentences that are similar to input sentences using a dynamic-programming-based
    sentence similarity scoring method. Next, we extract n-grams of action
    sequences that build the associated abstract syntax tree. Finally, we increase
    the probability of actions that cause the retrieved n-gram action subtree to be
    in the predicted code. We show that our approach improves the performance on
    two code generation tasks by up to +2.6 BLEU.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayati_S/0/1/0/all/0/1&quot;&gt;Shirley Anugrah Hayati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olivier_R/0/1/0/all/0/1&quot;&gt;Raphael Olivier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avvaru_P/0/1/0/all/0/1&quot;&gt;Pravalika Avvaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1&quot;&gt;Pengcheng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomasic_A/0/1/0/all/0/1&quot;&gt;Anthony Tomasic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1&quot;&gt;Graham Neubig&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10026">
    <title>Physically-inspired Gaussian processes for transcriptional regulation in Drosophila melanogaster. (arXiv:1808.10026v1 [stat.ML])</title>
    <link>http://arxiv.org/abs/1808.10026</link>
    <description rdf:parseType="Literal">&lt;p&gt;The regulatory process in Drosophila melanogaster is thoroughly studied for
    understanding several principles in systems biology. Since transcriptional
    regulation of the Drosophila depends on spatiotemporal interactions between
    mRNA expressions and gap-gene proteins, proper physically-inspired stochastic
    models are required to describe the existing link between both biological
    quantities. Many studies have shown that the use of Gaussian processes (GPs)
    and differential equations yields promising inference results when modelling
    regulatory processes. In order to exploit the benefits of GPs, two types of
    physically-inspired GPs based on the reaction-diffusion equation are further
    investigated in this paper. The main difference between both approaches lies on
    whether the GP prior is placed: either over mRNA expressions or protein
    concentrations. Contrarily to other stochastic frameworks, discretising the
    spatial space is not required here. Both GP models are tested under different
    conditions depending on the availability of biological data. Finally, their
    performances are assessed using a high-resolution dataset describing the
    blastoderm stage of the early embryo of Drosophila.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lopez_Lopera_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s F. L&amp;#xf3;pez-Lopera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Durrande_N/0/1/0/all/0/1&quot;&gt;Nicolas Durrande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alvarez_M/0/1/0/all/0/1&quot;&gt;Mauricio A. Alvarez&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10031">
    <title>Recommendation Through Mixtures of Heterogeneous Item Relationships. (arXiv:1808.10031v1 [cs.IR])</title>
    <link>http://arxiv.org/abs/1808.10031</link>
    <description rdf:parseType="Literal">&lt;p&gt;Recommender Systems have proliferated as general-purpose approaches to model
    a wide variety of consumer interaction data. Specific instances make use of
    signals ranging from user feedback, item relationships, geographic locality,
    social influence (etc.). Typically, research proceeds by showing that making
    use of a specific signal (within a carefully designed model) allows for
    higher-fidelity recommendations on a particular dataset. Of course, the real
    situation is more nuanced, in which a combination of many signals may be at
    play, or favored in different proportion by individual users. Here we seek to
    develop a framework that is capable of combining such heterogeneous item
    relationships by simultaneously modeling (a) what modality of recommendation is
    a user likely to be susceptible to at a particular point in time; and (b) what
    is the best recommendation from each modality. Our method borrows ideas from
    mixtures-of-experts approaches as well as knowledge graph embeddings. We find
    that our approach naturally yields more accurate recommendations than
    alternatives, while also providing intuitive `explanations&apos; behind the
    recommendations it provides.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1&quot;&gt;Wang-Cheng Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_M/0/1/0/all/0/1&quot;&gt;Mengting Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1&quot;&gt;Julian McAuley&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10032">
    <title>The Impact of Preprocessing on Deep Representations for Iris Recognition on Unconstrained Environments. (arXiv:1808.10032v1 [cs.CV])</title>
    <link>http://arxiv.org/abs/1808.10032</link>
    <description rdf:parseType="Literal">&lt;p&gt;The use of iris as a biometric trait is widely used because of its high level
    of distinction and uniqueness. Nowadays, one of the major research challenges
    relies on the recognition of iris images obtained in visible spectrum under
    unconstrained environments. In this scenario, the acquired iris are affected by
    capture distance, rotation, blur, motion blur, low contrast and specular
    reflection, creating noises that disturb the iris recognition systems. Besides
    delineating the iris region, usually preprocessing techniques such as
    normalization and segmentation of noisy iris images are employed to minimize
    these problems. But these techniques inevitably run into some errors. In this
    context, we propose the use of deep representations, more specifically,
    architectures based on VGG and ResNet-50 networks, for dealing with the images
    using (and not) iris segmentation and normalization. We use transfer learning
    from the face domain and also propose a specific data augmentation technique
    for iris images. Our results show that the approach using non-normalized and
    only circle-delimited iris images reaches a new state of the art in the
    official protocol of the NICE.II competition, a subset of the UBIRIS database,
    one of the most challenging databases on unconstrained environments, reporting
    an average Equal Error Rate (EER) of 13.98% which represents an absolute
    reduction of about 5%.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanlorensi_L/0/1/0/all/0/1&quot;&gt;Luiz A. Zanlorensi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luz_E/0/1/0/all/0/1&quot;&gt;Eduardo Luz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laroca_R/0/1/0/all/0/1&quot;&gt;Rayson Laroca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Britto_A/0/1/0/all/0/1&quot;&gt;Alceu S. Britto Jr.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_L/0/1/0/all/0/1&quot;&gt;Luiz S. Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1&quot;&gt;David Menotti&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10033">
    <title>Use of Source Code Similarity Metrics in Software Defect Prediction. (arXiv:1808.10033v1 [cs.SE])</title>
    <link>http://arxiv.org/abs/1808.10033</link>
    <description rdf:parseType="Literal">&lt;p&gt;In recent years, defect prediction has received a great deal of attention in
    the empirical software engineering world. Predicting software defects before
    the maintenance phase is very important not only to decrease the maintenance
    costs but also increase the overall quality of a software product. There are
    different types of product, process, and developer based software metrics
    proposed so far to measure the defectiveness of a software system. This paper
    suggests to use a novel set of software metrics which are based on the
    similarities detected among the source code files in a software project. To
    find source code similarities among different files of a software system,
    plagiarism and clone detection techniques are used. Two simple similarity
    metrics are calculated for each file, considering its overall similarity to the
    defective and non defective files in the project. Using these similarity
    metrics, we predict whether a specific file is defective or not. Our
    experiments on 10 open source data sets show that depending on the amount of
    detected similarity, proposed metrics could achieve significantly better
    performance compared to the existing static code metrics in terms of the area
    under the curve (AUC).
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okutan_A/0/1/0/all/0/1&quot;&gt;Ahmet Okutan&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10038">
    <title>Theoretical Linear Convergence of Unfolded ISTA and its Practical Weights and Thresholds. (arXiv:1808.10038v1 [cs.LG])</title>
    <link>http://arxiv.org/abs/1808.10038</link>
    <description rdf:parseType="Literal">&lt;p&gt;In recent years, unfolding iterative algorithms as neural networks has become
    an empirical success in solving sparse recovery problems. However, its
    theoretical understanding is still immature, which prevents us from fully
    utilizing the power of neural networks. In this work, we study unfolded ISTA
    (Iterative Shrinkage Thresholding Algorithm) for sparse signal recovery. We
    introduce a weight structure that is necessary for asymptotic convergence to
    the true sparse signal. With this structure, unfolded ISTA can attain a linear
    convergence, which is better than the sublinear convergence of ISTA/FISTA in
    general cases. Furthermore, we propose to incorporate thresholding in the
    network to perform support selection, which is easy to implement and able to
    boost the convergence rate both theoretically and empirically. Extensive
    simulations, including sparse vector recovery and a compressive sensing
    experiment on real image data, corroborate our theoretical results and
    demonstrate their practical usefulness.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaohan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jialin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1&quot;&gt;Wotao Yin&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10044">
    <title>AAD: Adaptive Anomaly Detection through traffic surveillance videos. (arXiv:1808.10044v1 [cs.CV])</title>
    <link>http://arxiv.org/abs/1808.10044</link>
    <description rdf:parseType="Literal">&lt;p&gt;Anomaly detection through video analysis is of great importance to detect any
    anomalous vehicle/human behavior at a traffic intersection. While most existing
    works use neural networks and conventional machine learning methods based on
    provided dataset, we will use object recognition (Faster R-CNN) to identify
    objects labels and their corresponding location in the video scene as the first
    step to implement anomaly detection. Then, the optical flow will be utilized to
    identify adaptive traffic flows in each region of the frame. Basically, we
    propose an alternative method for unusual activity detection using an adaptive
    anomaly detection framework. Compared to the baseline method described in the
    reference paper, our method is more efficient and yields the comparable
    accuracy.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajestani_M/0/1/0/all/0/1&quot;&gt;Mohammmad Farhadi Bajestani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abadi_S/0/1/0/all/0/1&quot;&gt;Seyed Soroush Heidari Rahmat Abadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fard_S/0/1/0/all/0/1&quot;&gt;Seyed Mostafa Derakhshandeh Fard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khodadadeh_R/0/1/0/all/0/1&quot;&gt;Roozbeh Khodadadeh&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10056">
    <title>Differentially Private Change-Point Detection. (arXiv:1808.10056v1 [math.ST])</title>
    <link>http://arxiv.org/abs/1808.10056</link>
    <description rdf:parseType="Literal">&lt;p&gt;The change-point detection problem seeks to identify distributional changes
    at an unknown change-point k* in a stream of data. This problem appears in many
    important practical settings involving personal data, including
    biosurveillance, fault detection, finance, signal detection, and security
    systems. The field of differential privacy offers data analysis tools that
    provide powerful worst-case privacy guarantees. We study the statistical
    problem of change-point detection through the lens of differential privacy. We
    give private algorithms for both online and offline change-point detection,
    analyze these algorithms theoretically, and provide empirical validation of our
    results.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cummings_R/0/1/0/all/0/1&quot;&gt;Rachel Cummings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Krehbiel_S/0/1/0/all/0/1&quot;&gt;Sara Krehbiel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mei_Y/0/1/0/all/0/1&quot;&gt;Yajun Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tuo_R/0/1/0/all/0/1&quot;&gt;Rui Tuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wanrong Zhang&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10059">
    <title>Zero-Shot Adaptive Transfer for Conversational Language Understanding. (arXiv:1808.10059v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10059</link>
    <description rdf:parseType="Literal">&lt;p&gt;Conversational agents such as Alexa and Google Assistant constantly need to
    increase their language understanding capabilities by adding new domains. A
    massive amount of labeled data is required for training each new domain. While
    domain adaptation approaches alleviate the annotation cost, prior approaches
    suffer from increased training time and suboptimal concept alignments. To
    tackle this, we introduce a novel Zero-Shot Adaptive Transfer method for slot
    tagging that utilizes the slot description for transferring reusable concepts
    across domains, and enjoys efficient training without any explicit concept
    alignments. Extensive experimentation over a dataset of 10 domains relevant to
    our commercial personal digital assistant shows that our model outperforms
    previous state-of-the-art systems by a large margin, and achieves an even
    higher improvement in the low data regime.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sungjin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_R/0/1/0/all/0/1&quot;&gt;Rahul Jha&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10062">
    <title>Timelines for In-Code Discovery of Zero-Day Vulnerabilities and Supply-Chain Attacks. (arXiv:1808.10062v1 [cs.CR])</title>
    <link>http://arxiv.org/abs/1808.10062</link>
    <description rdf:parseType="Literal">&lt;p&gt;Zero-day vulnerabilities can be accidentally or maliciously placed in code
    and can remain in place for years. In this study, we address an aspect of their
    longevity by considering the likelihood that they will be discovered in the
    code across versions. We approximate well-disguised vulnerabilities as only
    being discoverable if the relevant lines of code are explicitly examined, and
    obvious vulnerabilities as being discoverable if any part of the relevant file
    is examined. We analyze the version-to-version changes in three types of open
    source software (Mozilla Firefox, GNU/Linus, and glibc) to understand the rate
    at which the various pieces of code are amended and find that much of the
    revision behavior can be captured with a simple intuitive model. We use that
    model and the data from over a billion unique lines of code in 87 different
    versions of software to specify the bounds for in-code discoverability of
    vulnerabilities - from expertly hidden to obviously observable.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lohn_A/0/1/0/all/0/1&quot;&gt;Andrew J. Lohn&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10064">
    <title>Configuration Space Singularities of The Delta Manipulator. (arXiv:1808.10064v1 [cs.RO])</title>
    <link>http://arxiv.org/abs/1808.10064</link>
    <description rdf:parseType="Literal">&lt;p&gt;We investigate the configuration space of the Delta-Manipulator, identify 24
    points in the configuration space, where the Jacobian of the Constraint
    Equations looses rank and show, that these are not manifold points of the Real
    Algebraic Set, which is defined by the Constraint Equations.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diesse_M/0/1/0/all/0/1&quot;&gt;Marc Diesse&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10068">
    <title>A polynomial-time algorithm for median-closed semilinear constraints. (arXiv:1808.10068v1 [cs.CC])</title>
    <link>http://arxiv.org/abs/1808.10068</link>
    <description rdf:parseType="Literal">&lt;p&gt;A subset of Q^n is called semilinear (or piecewise linear) if it is Boolean
    combination of linear half-spaces. We study the computational complexity of the
    constraint satisfaction problem (CSP) over the rationals when all the
    constraints are semilinear. When the sets are convex the CSP is polynomial-time
    equivalent to linear programming. A semilinear relation is convex if and only
    if it is preserved by taking averages. Our main result is a polynomial-time
    algorithm for the CSP of semilinear constraints that are preserved by applying
    medians. We also prove that this class is maximally tractable in the sense that
    any larger class of semilinear relations has an NP-hard CSP. To illustrate, our
    class contains all relations that can be expressed by linear inequalities with
    at most two variables (so-called TVPI constraints), but it also contains many
    non-convex relations, for example constraints of the form x in S for arbitrary
    finite subset S of Q, or more generally disjunctive constraints of the form x &amp;lt;
    c or y &amp;lt; d for constants c and d.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bodirsky_M/0/1/0/all/0/1&quot;&gt;Manuel Bodirsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mamino_M/0/1/0/all/0/1&quot;&gt;Marcello Mamino&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10069">
    <title>IOTA Feasibility and Perspectives for Enabling Vehicular Applications. (arXiv:1808.10069v1 [cs.CR])</title>
    <link>http://arxiv.org/abs/1808.10069</link>
    <description rdf:parseType="Literal">&lt;p&gt;The emergence of distributed ledger technologies in the vehicular
    applications&apos; arena is decisively contributing to their improvement and shaping
    of the public opinion about their future. The Tangle is a technology at its
    infancy, but showing enormous potential to become a key solution by addressing
    several of the blockchain&apos;s limitations. This paper focuses the use of the
    Tangle to improve the security of both in-vehicle and off-vehicle functions in
    vehicular applications. To this end, key operational performance parameters are
    identified, evaluated and discussed with emphasis on their limitations and
    potential impact in future vehicular applications.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartolomeu_P/0/1/0/all/0/1&quot;&gt;Paulo C. Bartolomeu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vieira_E/0/1/0/all/0/1&quot;&gt;Emanuel Vieira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_J/0/1/0/all/0/1&quot;&gt;Joaquim Ferreira&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10072">
    <title>Super-Resolution for Hyperspectral and Multispectral Image Fusion Accounting for Seasonal Spectral Variability. (arXiv:1808.10072v1 [cs.CV])</title>
    <link>http://arxiv.org/abs/1808.10072</link>
    <description rdf:parseType="Literal">&lt;p&gt;Image fusion combines data from different heterogeneous sources to obtain
    more precise information about an underlying scene. Hyperspectral-multispectral
    (HS-MS) image fusion is currently attracting great interest in remote sensing
    since it allows the generation of high spatial resolution HS images,
    circumventing the main limitation of this imaging modality. Existing HS-MS
    fusion algorithms, however, neglect the spectral variability often existing
    between images acquired at different time instants. This time difference causes
    variations in spectral signatures of the underlying constituent materials due
    to different acquisition and seasonal conditions. This paper introduces a novel
    HS-MS image fusion strategy that combines an unmixing-based formulation with an
    explicit parametric model for typical spectral variability between the two
    images. Simulations with synthetic and real data show that the proposed
    strategy leads to a significant performance improvement under spectral
    variability and state-of-the-art performance otherwise.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borsoi_R/0/1/0/all/0/1&quot;&gt;Ricardo Augusto Borsoi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imbiriba_T/0/1/0/all/0/1&quot;&gt;Tales Imbiriba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bermudez_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Carlos Moreira Bermudez&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10073">
    <title>Rational Neural Networks for Approximating Jump Discontinuities of Graph Convolution Operator. (arXiv:1808.10073v1 [cs.LG])</title>
    <link>http://arxiv.org/abs/1808.10073</link>
    <description rdf:parseType="Literal">&lt;p&gt;For node level graph encoding, a recent important state-of-art method is the
    graph convolutional networks (GCN), which nicely integrate local vertex
    features and graph topology in the spectral domain. However, current studies
    suffer from several drawbacks: (1) graph CNNs relies on Chebyshev polynomial
    approximation which results in oscillatory approximation at jump
    discontinuities; (2) Increasing the order of Chebyshev polynomial can reduce
    the oscillations issue, but also incurs unaffordable computational cost; (3)
    Chebyshev polynomials require degree $\Omega$(poly(1/$\epsilon$)) to
    approximate a jump signal such as $|x|$, while rational function only needs
    $\mathcal{O}$(poly log(1/$\epsilon$))\cite{liang2016deep,telgarsky2017neural}.
    However, it&apos;s non-trivial to apply rational approximation without increasing
    computational complexity due to the denominator. In this paper, the superiority
    of rational approximation is exploited for graph signal recovering. RatioanlNet
    is proposed to integrate rational function and neural networks. We show that
    rational function of eigenvalues can be rewritten as a function of graph
    Laplacian, which can avoid multiplication by the eigenvector matrix. Focusing
    on the analysis of approximation on graph convolution operation, a graph signal
    regression task is formulated. Under graph signal regression task, its time
    complexity can be significantly reduced by graph Fourier transform. To overcome
    the local minimum problem of neural networks model, a relaxed Remez algorithm
    is utilized to initialize the weight parameters. Convergence rate of
    RatioanlNet and polynomial based methods on jump signal is analyzed for a
    theoretical guarantee. The extensive experimental results demonstrated that our
    approach could effectively characterize the jump discontinuities, outperforming
    competing methods by a substantial margin on both synthetic and real-world
    graphs.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhiqian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_R/0/1/0/all/0/1&quot;&gt;Rongjie Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuchao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chang-Tien Lu&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10075">
    <title>Towards Effective Deep Embedding for Zero-Shot Learning. (arXiv:1808.10075v1 [cs.CV])</title>
    <link>http://arxiv.org/abs/1808.10075</link>
    <description rdf:parseType="Literal">&lt;p&gt;Zero-shot learning (ZSL) attempts to recognize visual samples of unseen
    classes by virtue of the semantic descriptions of those classes. We posit that
    the key to ZSL is to exploit an effective embedding space where 1) visual
    samples can be tightly centred around the semantic descriptions of classes that
    they belong to; 2) visual samples of different classes are separated from each
    other with a large enough margin. Towards this goal, we present a simple but
    surprisingly effective deep embedding model. In our model, we separately embed
    visual samples and semantic descriptions into a latent intermediate space such
    that visual samples not only coincide with associated semantic descriptions,
    but also can be correctly discriminated by a trainable linear classifier. By
    doing this, visual samples can be tightly centred around associated semantic
    descriptions and more importantly, they can be separated from other semantic
    descriptions with a large margin, thus leading to a new state-of-the-art for
    ZSL. Furthermore, due to lacking training samples, the generalization capacity
    of the learned embedding space to unseen classes can be further improved. To
    this end, we propose to upgrade our model with a refining strategy which
    progressively calibrates the embedding space based upon some test samples
    chosen from unseen classes with high-confidence pseudo labels, and ultimately
    improves the generalization capacity greatly. Experimental results on five
    benchmarks demonstrate the great advantage of our model over current
    state-of-the-art competitors. For example, on AwA1 dataset, our model improves
    the recognition accuracy on unseen classes by 16.9% in conventional ZSL setting
    and even by 38.6% in the generalized ZSL setting.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingqiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chunhua Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yannning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1&quot;&gt;Anton Van Den Hengel&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10078">
    <title>Discriminative Learning of Similarity and Group Equivariant Representations. (arXiv:1808.10078v1 [stat.ML])</title>
    <link>http://arxiv.org/abs/1808.10078</link>
    <description rdf:parseType="Literal">&lt;p&gt;One of the most fundamental problems in machine learning is to compare
    examples: Given a pair of objects we want to return a value which indicates
    degree of (dis)similarity. Similarity is often task specific, and pre-defined
    distances can perform poorly, leading to work in metric learning. However,
    being able to learn a similarity-sensitive distance function also presupposes
    access to a rich, discriminative representation for the objects at hand. In
    this dissertation we present contributions towards both ends. In the first part
    of the thesis, assuming good representations for the data, we present a
    formulation for metric learning that makes a more direct attempt to optimize
    for the k-NN accuracy as compared to prior work. We also present extensions of
    this formulation to metric learning for kNN regression, asymmetric similarity
    learning and discriminative learning of Hamming distance. In the second part,
    we consider a situation where we are on a limited computational budget i.e.
    optimizing over a space of possible metrics would be infeasible, but access to
    a label aware distance metric is still desirable. We present a simple, and
    computationally inexpensive approach for estimating a well motivated metric
    that relies only on gradient estimates, discussing theoretical and experimental
    results. In the final part, we address representational issues, considering
    group equivariant convolutional neural networks (GCNNs). Equivariance to
    symmetry transformations is explicitly encoded in GCNNs; a classical CNN being
    the simplest example. In particular, we present a SO(3)-equivariant neural
    network architecture for spherical data, that operates entirely in Fourier
    space, while also providing a formalism for the design of fully Fourier neural
    networks that are equivariant to the action of any continuous compact group.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trivedi_S/0/1/0/all/0/1&quot;&gt;Shubhendu Trivedi&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10082">
    <title>Decentralized Detection with Robust Information Privacy Protection. (arXiv:1808.10082v1 [cs.IT])</title>
    <link>http://arxiv.org/abs/1808.10082</link>
    <description rdf:parseType="Literal">&lt;p&gt;We consider a decentralized detection network whose aim is to infer a public
    hypothesis of interest. However, the raw sensor observations also allow the
    fusion center to infer private hypotheses that we wish to protect. We consider
    the case where there are an uncountable number of private hypotheses belonging
    to an uncertainty set, and develop local privacy mappings at every sensor so
    that the sanitized sensor information minimizes the Bayes error of detecting
    the public hypothesis at the fusion center, while achieving information privacy
    for all private hypotheses. We introduce the concept of a most favorable
    hypothesis (MFH) and show how to find a MFH in the set of private hypotheses.
    By protecting the information privacy of the MFH, information privacy for every
    other private hypothesis is also achieved. We provide an iterative algorithm to
    find the optimal local privacy mappings, and derive some theoretical properties
    of these privacy mappings. Simulation results demonstrate that our proposed
    approach allows the fusion center to infer the public hypothesis with low error
    while protecting information privacy of all the private hypotheses.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Meng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_W/0/1/0/all/0/1&quot;&gt;Wee Peng Tay&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10083">
    <title>Differential and integral invariants under Mobius transformation. (arXiv:1808.10083v1 [cs.GR])</title>
    <link>http://arxiv.org/abs/1808.10083</link>
    <description rdf:parseType="Literal">&lt;p&gt;One of the most challenging problems in the domain of 2-D image or 3-D shape
    is to handle the non-rigid deformation. From the perspective of transformation
    groups, the conformal transformation is a key part of the diffeomorphism.
    According to the Liouville Theorem, an important part of the conformal
    transformation is the Mobius transformation, so we focus on Mobius
    transformation and propose two differential expressions that are invariable
    under 2-D and 3-D Mobius transformation respectively. Next, we analyze the
    absoluteness and relativity of invariance on them and their components. After
    that, we propose integral invariants under Mobius transformation based on the
    two differential expressions. Finally, we propose a conjecture about the
    structure of differential invariants under conformal transformation according
    to our observation on the composition of the above two differential invariants.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;He Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_H/0/1/0/all/0/1&quot;&gt;Hanlin Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1&quot;&gt;You Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hua Li&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10086">
    <title>Artifacts Detection and Error Block Analysis from Broadcasted Videos. (arXiv:1808.10086v1 [cs.CV])</title>
    <link>http://arxiv.org/abs/1808.10086</link>
    <description rdf:parseType="Literal">&lt;p&gt;With the advancement of IPTV and HDTV technology, previous subtle errors in
    videos are now becoming more prominent because of the structure oriented and
    compression based artifacts. In this paper, we focus towards the development of
    a real-time video quality check system. Light weighted edge gradient magnitude
    information is incorporated to acquire the statistical information and the
    distorted frames are then estimated based on the characteristics of their
    surrounding frames. Then we apply the prominent texture patterns to classify
    them in different block errors and analyze them not only in video error
    detection application but also in error concealment, restoration and retrieval.
    Finally, evaluating the performance through experiments on prominent datasets
    and broadcasted videos show that the proposed algorithm is very much efficient
    to detect errors for video broadcast and surveillance applications in terms of
    computation time and analysis of distorted frames.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1&quot;&gt;Md Mehedi Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1&quot;&gt;Tasneem Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_K/0/1/0/all/0/1&quot;&gt;Kiok Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chae_O/0/1/0/all/0/1&quot;&gt;Oksam Chae&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10093">
    <title>CNN-PS: CNN-based Photometric Stereo for General Non-Convex Surfaces. (arXiv:1808.10093v1 [cs.CV])</title>
    <link>http://arxiv.org/abs/1808.10093</link>
    <description rdf:parseType="Literal">&lt;p&gt;Most conventional photometric stereo algorithms inversely solve a BRDF-based
    image formation model. However, the actual imaging process is often far more
    complex due to the global light transport on the non-convex surfaces. This
    paper presents a photometric stereo network that directly learns relationships
    between the photometric stereo input and surface normals of a scene. For
    handling unordered, arbitrary number of input images, we merge all the input
    data to the intermediate representation called {\it observation map} that has a
    fixed shape, is able to be fed into a CNN. To improve both training and
    prediction, we take into account the rotational pseudo-invariance of the
    observation map that is derived from the isotropic constraint. For training the
    network, we create a synthetic photometric stereo dataset that is generated by
    a physics-based renderer, therefore the global light transport is considered.
    Our experimental results on both synthetic and real datasets show that our
    method outperforms conventional BRDF-based photometric stereo algorithms
    especially when scenes are highly non-convex.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ikehata_S/0/1/0/all/0/1&quot;&gt;Satoshi Ikehata&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10095">
    <title>MES-P: an Emotional Tonal Speech Dataset in Mandarin Chinese with Distal and Proximal Labels. (arXiv:1808.10095v1 [cs.SD])</title>
    <link>http://arxiv.org/abs/1808.10095</link>
    <description rdf:parseType="Literal">&lt;p&gt;Emotion shapes all aspects of our interpersonal and intellectual experiences.
    Its automatic analysis has there-fore many applications, e.g., human-machine
    interface. In this paper, we propose an emotional tonal speech dataset, namely
    Mandarin Chinese Emotional Speech Dataset - Portrayed (MES-P), with both distal
    and proximal labels. In contrast with state of the art emotional speech
    datasets which are only focused on perceived emotions, the proposed MES-P
    dataset includes not only perceived emotions with their proximal labels but
    also intended emotions with distal labels, thereby making it possible to study
    human emotional intelligence, i.e. people emotion expression ability and their
    skill of understanding emotions, thus explicitly accounting for perception
    differences between intended and perceived emotions in speech signals and
    enabling studies of emotional misunderstandings which often occur in real life.
    Furthermore, the proposed MES-P dataset also captures a main feature of tonal
    languages, i.e., tonal variations, and provides recorded emotional speech
    samples whose tonal variations match the tonal distribution in real life
    Mandarin Chinese. Besides, the proposed MES-P dataset features emotion
    intensity variations as well, and includes both moderate and intense versions
    of recordings for joy, anger, and sadness in addition to neutral speech.
    Ratings of the collected speech samples are made in valence-arousal space
    through continuous coordinate locations, resulting in an emotional distribution
    pattern in 2D VA space. The consistency between the speakers&apos; emotional
    intentions and the listeners&apos; perceptions is also studied using Cohen&apos;s Kappa
    coefficients. Finally, we also carry out extensive experiments using a baseline
    on MES-P for automatic emotion recognition and compare the results with human
    emotion intelligence.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zhongzhe Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Ying Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_W/0/1/0/all/0/1&quot;&gt;Weibei Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1&quot;&gt;Zhi Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liming Chen&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10097">
    <title>Profiling and Improving the Duty-Cycling Performance of Linux-based IoT Devices. (arXiv:1808.10097v1 [cs.OS])</title>
    <link>http://arxiv.org/abs/1808.10097</link>
    <description rdf:parseType="Literal">&lt;p&gt;Minimizing the energy consumption of Linux-based devices is an essential step
    towards their wide deployment in various IoT scenarios. Energy saving methods
    such as duty-cycling aim to address this constraint by limiting the amount of
    time the device is powered on. In this work we study and improve the amount of
    time a Linux-based IoT device is powered on to accomplish its tasks. We analyze
    the processes of system boot up and shutdown on two platforms, the Raspberry Pi
    3 and Zero Wireless, and enhance duty-cycling performance by identifying and
    disabling time consuming or unnecessary units initialized in the userspace. We
    also study whether SD card speed and SD card capacity utilization affect boot
    up duration and energy consumption. In addition, we propose Pallex, a parallel
    execution framework built on top of the \texttt{systemd init} system to run a
    user application concurrently with userspace initialization. We validate the
    performance impact of Pallex when applied to various IoT application scenarios:
    (i) capturing an image, (ii) capturing and encrypting an image, (iii) capturing
    and classifying an image using the the k-nearest neighbor algorithm, and (iv)
    capturing images and sending them to a cloud server. Our results show that
    system lifetime is increased by 18.3%, 16.8%, 13.9% and 30.2%, for these
    application scenarios, respectively.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amirtharaj_I/0/1/0/all/0/1&quot;&gt;Immanuel Amirtharaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groot_T/0/1/0/all/0/1&quot;&gt;Tai Groot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dezfouli_B/0/1/0/all/0/1&quot;&gt;Behnam Dezfouli&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10101">
    <title>DP-ADMM: ADMM-based Distributed Learning with Differential Privacy. (arXiv:1808.10101v1 [cs.LG])</title>
    <link>http://arxiv.org/abs/1808.10101</link>
    <description rdf:parseType="Literal">&lt;p&gt;Distributed machine learning is making great changes in a wide variety of
    domains but also brings privacy risk from the exchanged information during the
    learning process. This paper focuses on a class of regularized empirical risk
    minimization problems, and develops a privacy-preserving distributed learning
    algorithm. We use Alternating Direction Method of Multipliers (ADMM) to
    decentralize the learning algorithm, and apply Gaussian mechanisms locally to
    guarantee differential privacy. However, simply combining ADMM and local
    randomization mechanisms would result in an unconvergent algorithm with bad
    performance, especially when the introduced noise is large to guarantee a low
    total privacy loss. Besides, this approach cannot be applied to the learning
    problems with non-smooth objective functions. To figure out these concerns, we
    propose an improved ADMM-based differentially private distributed learning
    algorithm: DP-ADMM, where an approximate augmented Lagrangian function and
    Gaussian mechanisms with time-varying variance are utilized. We also apply the
    moment accountant method to bound the total privacy loss. Our theoretical
    analysis proves that DP-ADMM can be applied to a general class of convex
    learning problems, provides differential privacy guarantee, and achieves an
    $O(1/\sqrt{t})$ rate of convergence, where $t$ is the number of iterations. Our
    evaluations demonstrate that our approach can achieve good accuracy and
    effectiveness even with a low total privacy leakage.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zonghao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Rui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yanmin Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_Tin_E/0/1/0/all/0/1&quot;&gt;Eric Chan-Tin&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10104">
    <title>Modeling OWL with Rules: The ROWL Protege Plugin. (arXiv:1808.10104v1 [cs.AI])</title>
    <link>http://arxiv.org/abs/1808.10104</link>
    <description rdf:parseType="Literal">&lt;p&gt;In our experience, some ontology users find it much easier to convey logical
    statements using rules rather than OWL (or description logic) axioms. Based on
    recent theoretical developments on transformations between rules and
    description logics, we develop ROWL, a Protege plugin that allows users to
    enter OWL axioms by way of rules; the plugin then automatically converts these
    rules into OWL DL axioms if possible, and prompts the user in case such a
    conversion is not possible without weakening the semantics of the rule.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1&quot;&gt;Md. Kamruzzaman Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carral_D/0/1/0/all/0/1&quot;&gt;David Carral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krisnadhi_A/0/1/0/all/0/1&quot;&gt;Adila A. Krisnadhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hitzler_P/0/1/0/all/0/1&quot;&gt;Pascal Hitzler&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10105">
    <title>OWLAx: A Protege Plugin to Support Ontology Axiomatization through Diagramming. (arXiv:1808.10105v1 [cs.AI])</title>
    <link>http://arxiv.org/abs/1808.10105</link>
    <description rdf:parseType="Literal">&lt;p&gt;Once the conceptual overview, in terms of a somewhat informal class diagram,
    has been designed in the course of engineering an ontology, the process of
    adding many of the appropriate logical axioms is mostly a routine task. We
    provide a Protege plugin which supports this task, together with a visual user
    interface, based on established methods for ontology design pattern modeling.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1&quot;&gt;Md. Kamruzzaman Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krisnadhi_A/0/1/0/all/0/1&quot;&gt;Adila A. Krisnadhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hitzler_P/0/1/0/all/0/1&quot;&gt;Pascal Hitzler&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10108">
    <title>Rule-based OWL Modeling with ROWLTab Protege Plugin. (arXiv:1808.10108v1 [cs.AI])</title>
    <link>http://arxiv.org/abs/1808.10108</link>
    <description rdf:parseType="Literal">&lt;p&gt;It has been argued that it is much easier to convey logical statements using
    rules rather than OWL (or description logic (DL)) axioms. Based on recent
    theoretical developments on transformations between rules and DLs, we have
    developed ROWLTab, a Protege plugin that allows users to enter OWL axioms by
    way of rules; the plugin then automatically converts these rules into OWL 2 DL
    axioms if possible, and prompts the user in case such a conversion is not
    possible without weakening the semantics of the rule. In this paper, we present
    ROWLTab, together with a user evaluation of its effectiveness compared to
    entering axioms using the standard Protege interface. Our evaluation shows that
    modeling with ROWLTab is much quicker than the standard interface, while at the
    same time, also less prone to errors for hard modeling tasks.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1&quot;&gt;Md. Kamruzzaman Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krisnadhi_A/0/1/0/all/0/1&quot;&gt;Adila Krisnadhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carral_D/0/1/0/all/0/1&quot;&gt;David Carral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hitzler_P/0/1/0/all/0/1&quot;&gt;Pascal Hitzler&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10113">
    <title>Story Ending Generation with Incremental Encoding and Commonsense Knowledge. (arXiv:1808.10113v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10113</link>
    <description rdf:parseType="Literal">&lt;p&gt;Story ending generation is a strong indication of story comprehension. This
    task requires not only to understand the context clues which plays the most
    important role in planning the plot, but also to handle implicit knowledge to
    make a reasonable, coherent story.
    &lt;/p&gt;
    &lt;p&gt;In this paper, we devise a novel model for story ending generation. The model
    adopts an incremental encoding scheme with multi-source attention to deal with
    context clues spanning in the story context. In addition, the model is
    empowered with commonsense knowledge through multi-source attention to produce
    reasonable story endings.
    &lt;/p&gt;
    &lt;p&gt;Experiments show that our model can generate more reasonable story endings
    than state-of-the-art baselines.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1&quot;&gt;Jian Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yansen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1&quot;&gt;Minlie Huang&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10117">
    <title>Space-Time Block Coding Based Beamforming for Beam Squint Compensation. (arXiv:1808.10117v1 [cs.IT])</title>
    <link>http://arxiv.org/abs/1808.10117</link>
    <description rdf:parseType="Literal">&lt;p&gt;In this paper, the beam squint problem, which causes significant variations
    in radiated beam gain over frequencies in millimeter wave communication system,
    is investigated. A constant modulus beamformer design, which is formulated to
    maximize the expected average beam gain within the bandwidth with limited
    variation over frequencies within the bandwidth, is proposed. A semidefinite
    relaxation (SDR) method is developed to solve the optimization problem under
    the constant modulus constraints. Depending on the eigenvalues of the optimal
    solution, either direct beamforming or transmit diversity based beamforming is
    employed for data transmissions. Through numerical results, the proposed
    transmission scheme can compensate for beam squint effectively and improve
    system throughput. Overall, a transmission scheme for beam squint compensation
    in wideband wireless communication systems is provided.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Ximei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_D/0/1/0/all/0/1&quot;&gt;Deli Qiao&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10120">
    <title>ExpIt-OOS: Towards Learning from Planning in Imperfect Information Games. (arXiv:1808.10120v1 [cs.AI])</title>
    <link>http://arxiv.org/abs/1808.10120</link>
    <description rdf:parseType="Literal">&lt;p&gt;The current state of the art in playing many important perfect information
    games, including Chess and Go, combines planning and deep reinforcement
    learning with self-play. We extend this approach to imperfect information games
    and present ExIt-OOS, a novel approach to playing imperfect information games
    within the Expert Iteration framework and inspired by AlphaZero. We use Online
    Outcome Sampling, an online search algorithm for imperfect information games in
    place of MCTS. While training online, our neural strategy is used to improve
    the accuracy of playouts in OOS, allowing a learning and planning feedback loop
    for imperfect information games.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitchen_A/0/1/0/all/0/1&quot;&gt;Andy Kitchen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benedetti_M/0/1/0/all/0/1&quot;&gt;Michela Benedetti&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10122">
    <title>Learning Neural Templates for Text Generation. (arXiv:1808.10122v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10122</link>
    <description rdf:parseType="Literal">&lt;p&gt;While neural, encoder-decoder models have had significant empirical success
    in text generation, there remain several unaddressed problems with this style
    of generation. Encoder-decoder models are largely (a) uninterpretable, and (b)
    difficult to control in terms of their phrasing or content. This work proposes
    a neural generation system using a hidden semi-markov model (HSMM) decoder,
    which learns latent, discrete templates jointly with learning to generate. We
    show that this model learns useful templates, and that these templates make
    generation both more interpretable and controllable. Furthermore, we show that
    this approach scales to real data sets and achieves strong performance nearing
    that of encoder-decoder text generation models.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiseman_S/0/1/0/all/0/1&quot;&gt;Sam Wiseman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shieber_S/0/1/0/all/0/1&quot;&gt;Stuart M. Shieber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1&quot;&gt;Alexander M. Rush&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10128">
    <title>Semi-Supervised Training for Improving Data Efficiency in End-to-End Speech Synthesis. (arXiv:1808.10128v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10128</link>
    <description rdf:parseType="Literal">&lt;p&gt;Although end-to-end text-to-speech (TTS) models such as Tacotron have shown
    excellent results, they typically require a sizable set of high-quality &amp;lt;text,
    audio&amp;gt; pairs for training, which are expensive to collect. In this paper, we
    propose a semi-supervised training framework to improve the data efficiency of
    Tacotron. The idea is to allow Tacotron to utilize textual and acoustic
    knowledge contained in large, publicly-available text and speech corpora.
    Importantly, these external data are unpaired and potentially noisy.
    Specifically, first we embed each word in the input text into word vectors and
    condition the Tacotron encoder on them. We then use an unpaired speech corpus
    to pre-train the Tacotron decoder in the acoustic domain. Finally, we fine-tune
    the model using available paired data. We demonstrate that the proposed
    framework enables Tacotron to generate intelligible speech using less than half
    an hour of paired training data.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1&quot;&gt;Yu-An Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1&quot;&gt;Wei-Ning Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skerry_Ryan_R/0/1/0/all/0/1&quot;&gt;RJ Skerry-Ryan&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10134">
    <title>Baidu Apollo Auto-Calibration System - An Industry-Level Data-Driven and Learning based Vehicle Longitude Dynamic Calibrating Algorithm. (arXiv:1808.10134v1 [cs.RO])</title>
    <link>http://arxiv.org/abs/1808.10134</link>
    <description rdf:parseType="Literal">&lt;p&gt;For any autonomous driving vehicle, control module determines its road
    performance and safety, i.e. its precision and stability should stay within a
    carefully-designed range. Nonetheless, control algorithms require vehicle
    dynamics (such as longitudinal dynamics) as inputs, which, unfortunately, are
    obscure to calibrate in real time. As a result, to achieve reasonable
    performance, most, if not all, research-oriented autonomous vehicles do manual
    calibrations in a one-by-one fashion. Since manual calibration is not
    sustainable once entering into mass production stage for industrial purposes,
    we here introduce a machine-learning based auto-calibration system for
    autonomous driving vehicles. In this paper, we will show how we build a
    data-driven longitudinal calibration procedure using machine learning
    techniques. We first generated offline calibration tables from human driving
    data. The offline table serves as an initial guess for later uses and it only
    needs twenty-minutes data collection and process. We then used an
    online-learning algorithm to appropriately update the initial table (the
    offline table) based on real-time performance analysis. This longitudinal
    auto-calibration system has been deployed to more than one hundred Baidu Apollo
    self-driving vehicles (including hybrid family vehicles and electronic
    delivery-only vehicles) since April 2018. By August 27, 2018, it had been
    tested for more than two thousands hours, ten thousands kilometers (6,213
    miles) and yet proven to be effective.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Dingfeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1&quot;&gt;Xiao Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1&quot;&gt;Qi Kong&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10137">
    <title>Recognizing Generating Subgraphs in Graphs without Cycles of Lengths 6 and 7. (arXiv:1808.10137v1 [cs.CC])</title>
    <link>http://arxiv.org/abs/1808.10137</link>
    <description rdf:parseType="Literal">&lt;p&gt;Let $B$ be an induced complete bipartite subgraph of $G$ on vertex sets of
    bipartition $B_{X}$ and $B_{Y}$. The subgraph $B$ is {\it generating} if there
    exists an independent set $S$ such that each of $S \cup B_{X}$ and $S \cup
    B_{Y}$ is a maximal independent set in the graph. If $B$ is generating, it
    \textit{produces} the restriction $w(B_{X})=w(B_{Y})$. Let $w:V(G)
    \longrightarrow\mathbb{R}$ be a weight function. We say that $G$ is
    $w$-well-covered if all maximal independent sets are of the same weight. The
    graph $G$ is $w$-well-covered if and only if $w$ satisfies all restrictions
    produced by all generating subgraphs of $G$. Therefore, generating subgraphs
    play an important role in characterizing weighted well-covered graphs. It is an
    \textbf{NP}-complete problem to decide whether a subgraph is generating, even
    when the subgraph is isomorphic to $K_{1,1}$ \cite{bnz:related}. We present a
    polynomial algorithm for recognizing generating subgraphs for graphs without
    cycles of lengths 6 and 7.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tankus_D/0/1/0/all/0/1&quot;&gt;David Tankus&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10143">
    <title>Direct Output Connection for a High-Rank Language Model. (arXiv:1808.10143v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10143</link>
    <description rdf:parseType="Literal">&lt;p&gt;This paper proposes a state-of-the-art recurrent neural network (RNN)
    language model that combines probability distributions computed not only from a
    final RNN layer but also from middle layers. Our proposed method raises the
    expressive power of a language model based on the matrix factorization
    interpretation of language modeling introduced by Yang et al. (2018). The
    proposed method improves the current state-of-the-art language model and
    achieves the best score on the Penn Treebank and WikiText-2, which are the
    standard benchmark datasets. Moreover, we indicate our proposed method
    contributes to two application tasks: machine translation and headline
    generation. Our code is publicly available at: https://github.com/nttcslab-
    nlp/doc_lm.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takase_S/0/1/0/all/0/1&quot;&gt;Sho Takase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1&quot;&gt;Jun Suzuki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagata_M/0/1/0/all/0/1&quot;&gt;Masaaki Nagata&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10144">
    <title>Contribution of Glottal Waveform in Speech Emotion: A Comparative Pairwise Investigation. (arXiv:1808.10144v1 [cs.SD])</title>
    <link>http://arxiv.org/abs/1808.10144</link>
    <description rdf:parseType="Literal">&lt;p&gt;In this work, we investigated the contribution of the glottal waveform in
    human vocal emotion expressing. Seven emotional states including moderate and
    intense versions of three emotional families as anger, joy, and sadness, plus a
    neutral state are considered, with speech samples in Mandarin Chinese. The
    glottal waveform extracted from speech samples of different emotion states are
    first analyzed in both time domain and frequency domain to discover their
    differences. Comparative emotion classifications are then taken out based on
    features extracted from original whole speech signal and only glottal wave
    signal. In experiments of generation of a performance-driven hierarchical
    classifier architecture, and pairwise classification on individual emotional
    states, the low difference between accuracies obtained from speech signal and
    glottal signal proved that a majority of emotional cues in speech could be
    conveyed through glottal waveform. The best distinguishable emotional pair by
    glottal waveform is intense anger against moderate sadness, with the accuracy
    of 92.45%. It is also concluded in this work that glottal waveform represent
    better valence cues than arousal cues of emotion.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zhongzhe Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Ying Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1&quot;&gt;Zhi Tao&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10145">
    <title>On the Composability of Statistically Secure Random Oblivious Transfer. (arXiv:1808.10145v1 [cs.CR])</title>
    <link>http://arxiv.org/abs/1808.10145</link>
    <description rdf:parseType="Literal">&lt;p&gt;We show that stand-alone statistically secure random oblivious transfer
    protocols based on two-party stateless primitives are statistically universally
    composable. I.e. they are simulatable secure with an unlimited adversary, an
    unlimited simulator and an unlimited environment machine. Our result implies
    that several previous oblivious transfer protocols in the literature which were
    proven secure under weaker, non-composable definitions of security can actually
    be used in arbitrary statistically secure applications without lowering the
    security.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dowsley_R/0/1/0/all/0/1&quot;&gt;Rafael Dowsley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_Quade_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rn M&amp;#xfc;ller-Quade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nascimento_A/0/1/0/all/0/1&quot;&gt;Anderson C. A. Nascimento&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10146">
    <title>Dense Scene Flow from Stereo Disparity and Optical Flow. (arXiv:1808.10146v1 [cs.CV])</title>
    <link>http://arxiv.org/abs/1808.10146</link>
    <description rdf:parseType="Literal">&lt;p&gt;Scene flow describes 3D motion in a 3D scene. It can either be modeled as a
    single task, or it can be reconstructed from the auxiliary tasks of stereo
    depth and optical flow estimation. While the second method can achieve
    real-time performance by using real-time auxiliary methods, it will typically
    produce non-dense results. In this representation of a basic combination
    approach for scene flow estimation, we will tackle the problem of non-density
    by interpolation.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuster_R/0/1/0/all/0/1&quot;&gt;Ren&amp;#xe9; Schuster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasenmuller_O/0/1/0/all/0/1&quot;&gt;Oliver Wasenm&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1&quot;&gt;Didier Stricker&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10151">
    <title>VirtualIdentity: Privacy-Preserving User Profiling. (arXiv:1808.10151v1 [cs.CR])</title>
    <link>http://arxiv.org/abs/1808.10151</link>
    <description rdf:parseType="Literal">&lt;p&gt;User profiling from user generated content (UGC) is a common practice that
    supports the business models of many social media companies. Existing systems
    require that the UGC is fully exposed to the module that constructs the user
    profiles. In this paper we show that it is possible to build user profiles
    without ever accessing the user&apos;s original data, and without exposing the
    trained machine learning models for user profiling -- which are the
    intellectual property of the company -- to the users of the social media site.
    We present VirtualIdentity, an application that uses secure multi-party
    cryptographic protocols to detect the age, gender and personality traits of
    users by classifying their user-generated text and personal pictures with
    trained support vector machine models in a privacy-preserving manner.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sisi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poon_W/0/1/0/all/0/1&quot;&gt;Wing-Sea Poon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farnadi_G/0/1/0/all/0/1&quot;&gt;Golnoosh Farnadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horst_C/0/1/0/all/0/1&quot;&gt;Caleb Horst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thompson_K/0/1/0/all/0/1&quot;&gt;Kebra Thompson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nickels_M/0/1/0/all/0/1&quot;&gt;Michael Nickels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dowsley_R/0/1/0/all/0/1&quot;&gt;Rafael Dowsley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nascimento_A/0/1/0/all/0/1&quot;&gt;Anderson C. A. Nascimento&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cock_M/0/1/0/all/0/1&quot;&gt;Martine De Cock&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10166">
    <title>Uncovering intimate and casual relationships from mobile phone communication. (arXiv:1808.10166v1 [cs.SI])</title>
    <link>http://arxiv.org/abs/1808.10166</link>
    <description rdf:parseType="Literal">&lt;p&gt;We analyze a large-scale mobile phone call dataset with the metadata of the
    mobile phone users, including age, gender, and billing locality, to uncover the
    nature of relationships between peers or individuals of similar ages. We show
    that in addition to the age and gender of users, the information about the
    ranks of users to each other in their egocentric networks is crucial in
    characterizing intimate and casual relationships of peers. The opposite-gender
    pairs in intimate relationships are found to show the highest levels of call
    frequency and daily regularity, consistent with small-scale studies on romantic
    partners. This is followed by the same-gender pairs in intimate relationships,
    while the lowest call frequency and daily regularity are observed for the pairs
    in casual relationships. We also find that older pairs tend to call less
    frequently and less regularly than younger pairs, while the average call
    durations exhibit a more complex dependence on age. We expect that a more
    detailed analysis can help us better characterize the nature of peer
    relationships and distinguish various types of relations, such as siblings,
    friends, and romantic partners, more clearly.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fudolig_M/0/1/0/all/0/1&quot;&gt;Mikaela Irene D. Fudolig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monsivais_D/0/1/0/all/0/1&quot;&gt;Daniel Monsivais&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_K/0/1/0/all/0/1&quot;&gt;Kunal Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_H/0/1/0/all/0/1&quot;&gt;Hang-Hyun Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaski_K/0/1/0/all/0/1&quot;&gt;Kimmo Kaski&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10180">
    <title>A Variational Feature Encoding Method of 3D Object for Probabilistic Semantic SLAM. (arXiv:1808.10180v1 [cs.RO])</title>
    <link>http://arxiv.org/abs/1808.10180</link>
    <description rdf:parseType="Literal">&lt;p&gt;This paper presents a feature encoding method of complex 3D objects for
    high-level semantic features. Recent approaches to object recognition methods
    become important for semantic simultaneous localization and mapping (SLAM).
    However, there is a lack of consideration of the probabilistic observation
    model for 3D objects, as the shape of a 3D object basically follows a complex
    probability distribution. Furthermore, since the mobile robot equipped with a
    range sensor observes only a single view, much information of the object shape
    is discarded. These limitations are the major obstacles to semantic SLAM and
    view-independent loop closure using 3D object shapes as features. In order to
    enable the numerical analysis for the Bayesian inference, we approximate the
    true observation model of 3D objects to tractable distributions. Since the
    observation likelihood can be obtained from the generative model, we formulate
    the true generative model for 3D object with the Bayesian networks. To capture
    these complex distributions, we apply a variational auto-encoder. To analyze
    the approximated distributions and encoded features, we perform classification
    with maximum likelihood estimation and shape retrieval.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;H. W. Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;B. H. Lee&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10191">
    <title>Sensitivity, Affine Transforms and Quantum Communication Complexity. (arXiv:1808.10191v1 [cs.CC])</title>
    <link>http://arxiv.org/abs/1808.10191</link>
    <description rdf:parseType="Literal">&lt;p&gt;$\newcommand{\F}{\mathbb{F}} $We study the Boolean function parameters
    sensitivity ($s$), block sensitivity ($bs$), and alternation ($alt$) under
    specially designed affine transforms. For a function $f:\F_2^n \to \{0,1\}$,
    and $A = Mx+b$ for $M \in \F_2^{n \times n}$ and $b \in \F_2^n$, the result of
    the transformation $g$ is defined as $\forall x \in \F_2^n, g(x) = f(Mx+b)$.
    &lt;/p&gt;
    &lt;p&gt;We study alternation under linear shifts (when $M$ is the identity matrix)
    called the shift invariant alternation (denoted by $salt(f)$). By a result of
    Lin and Zhang (2017), it follows that $bs(f) \le O(salt(f)^2s(f))$. Thus, to
    settle the Sensitivity Conjecture, it suffices to argue that $\forall~f,
    salt(f) \le poly(s(f))$. However, we exhibit an explicit family of functions
    for which $salt(f)$ is $2^{\Omega(s(f))}$.
    &lt;/p&gt;
    &lt;p&gt;We show an affine transform $A$, such that the corresponding function $g$
    satisfies $bs(f,0^n) \le s(g)$, using which we proving that for $F(x,y) := f(x
    \land y)$, the bounded error quantum communication complexity of $F$ with prior
    entanglement, $Q^*_{1/3}(F) = \Omega(\sqrt{bs(f,0^n)})$. Our proof builds on
    ideas from Sherstov (2010) where we use specific properties of the above affine
    transformation. Using this, we show,
    &lt;/p&gt;
    &lt;p&gt;* For a fixed prime $p$ and $0 &amp;lt; \epsilon &amp;lt; 1$, any $f$ with $deg_p(f) \le
    (1-\epsilon)\log n$ must satisfy $Q^*_{1/3}(F) = \Omega\left
    (\frac{n^{\epsilon/2}}{\log n} \right )$. Here, $deg_p(f)$ denotes the degree
    of the multilinear polynomial of $f$ over $\F_p$ .
    &lt;/p&gt;
    &lt;p&gt;* For $f$ such that there exists primes $p$ and $q$ with $deg_q(f) \ge
    \Omega(deg_p(f)^\delta)$ for $\delta &amp;gt; 2$, the deterministic communication
    complexity - $D(F)$ and $Q^*_{1/3}(F)$ are polynomially related. In particular,
    this holds when $deg_p(f) = O(1)$. Thus, for this class of functions, this
    answers an open question (see Buhrman and deWolf (2001)) about the relation
    between the two measures.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinesh_K/0/1/0/all/0/1&quot;&gt;Krishnamoorthy Dinesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarma_J/0/1/0/all/0/1&quot;&gt;Jayalal Sarma&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10192">
    <title>Towards a Better Metric for Evaluating Question Generation Systems. (arXiv:1808.10192v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10192</link>
    <description rdf:parseType="Literal">&lt;p&gt;There has always been criticism for using $n$-gram based similarity metrics,
    such as BLEU, NIST, \textit{etc}, for evaluating the performance of NLG
    systems. However, these metrics continue to remain popular and are recently
    being used for evaluating the performance of systems which automatically
    generate questions from documents, knowledge graphs, images, \textit{etc}.
    Given the rising interest in such automatic question generation (AQG) systems,
    it is important to objectively examine whether these metrics are suitable for
    this task. In particular, it is important to verify whether such metrics used
    for evaluating AQG systems focus on \textit{answerability} of the generated
    question by preferring questions which contain all relevant information such as
    question type (Wh-types), entities, relations, \textit{etc}. In this work, we
    show that current automatic evaluation metrics based on $n$-gram similarity do
    not always correlate well with human judgments about \textit{answerability} of
    a question. To alleviate this problem and as a first step towards better
    evaluation metrics for AQG, we introduce a scoring function to capture
    \textit{answerability} and show that when this scoring function is integrated
    with existing metrics, they correlate significantly better with human
    judgments. The scripts and data developed as a part of this work are made
    publicly available at https://github.com/PrekshaNema25/Answerability-Metric.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nema_P/0/1/0/all/0/1&quot;&gt;Preksha Nema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1&quot;&gt;Mitesh M. Khapra&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10196">
    <title>Pronoun Translation in English-French Machine Translation: An Analysis of Error Types. (arXiv:1808.10196v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10196</link>
    <description rdf:parseType="Literal">&lt;p&gt;Pronouns are a long-standing challenge in machine translation. We present a
    study of the performance of a range of rule-based, statistical and neural MT
    systems on pronoun translation based on an extensive manual evaluation using
    the PROTEST test suite, which enables a fine-grained analysis of different
    pronoun types and sheds light on the difficulties of the task. We find that the
    rule-based approaches in our corpus perform poorly as a result of
    oversimplification, whereas SMT and early NMT systems exhibit significant
    shortcomings due to a lack of awareness of the functional and referential
    properties of pronouns. A recent Transformer-based NMT system with
    cross-sentence context shows very promising results on non-anaphoric pronouns
    and intra-sentential anaphora, but there is still considerable room for
    improvement in examples with cross-sentence dependencies.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hardmeier_C/0/1/0/all/0/1&quot;&gt;Christian Hardmeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillou_L/0/1/0/all/0/1&quot;&gt;Liane Guillou&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10203">
    <title>Maximum Eccentric Connectivity Index for Graphs with Given Diameter. (arXiv:1808.10203v1 [cs.DM])</title>
    <link>http://arxiv.org/abs/1808.10203</link>
    <description rdf:parseType="Literal">&lt;p&gt;The eccentricity of a vertex $v$ in a graph $G$ is the maximum distance
    between $v$ and any other vertex of $G$. The diameter of a graph $G$ is the
    maximum eccentricity of a vertex in $G$. The eccentric connectivity index of a
    connected graph is the sum over all vertices of the product between
    eccentricity and degree. Given two integers $n$ and $D$ with $D\leq n-1$, we
    characterize those graphs which have the largest eccentric connectivity index
    among all connected graphs of order $n$ and diameter $D$. As a corollary, we
    also characterize those graphs which have the largest eccentric connectivity
    index among all connected graphs of a given order $n$.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauweele_P/0/1/0/all/0/1&quot;&gt;Pierre Hauweele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hertz_A/0/1/0/all/0/1&quot;&gt;Alain Hertz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melot_H/0/1/0/all/0/1&quot;&gt;Hadrien M&amp;#xe9;lot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ries_B/0/1/0/all/0/1&quot;&gt;Bernard Ries&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Devillez_G/0/1/0/all/0/1&quot;&gt;Gauvain Devillez&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10209">
    <title>Leadership in Singleton Congestion Games: What is Hard and What is Easy. (arXiv:1808.10209v1 [cs.GT])</title>
    <link>http://arxiv.org/abs/1808.10209</link>
    <description rdf:parseType="Literal">&lt;p&gt;We study the problem of computing Stackelberg equilibria Stackelberg games
    whose underlying structure is in congestion games, focusing on the case where
    each player can choose a single resource (a.k.a. singleton congestion games)
    and one of them acts as leader. In particular, we address the cases where the
    players either have the same action spaces (i.e., the set of resources they can
    choose is the same for all of them) or different ones, and where their costs
    are either monotonic functions of the resource congestion or not. We show that,
    in the case where the players have different action spaces, the cost the leader
    incurs in a Stackelberg equilibrium cannot be approximated in polynomial time
    up to within any polynomial factor in the size of the game unless P = NP,
    independently of the cost functions being monotonic or not. We show that a
    similar result also holds when the players have nonmonotonic cost functions,
    even if their action spaces are the same. Differently, we prove that the case
    with identical action spaces and monotonic cost functions is easy, and propose
    polynomial-time algorithm for it. We also improve an algorithm for the
    computation of a socially optimal equilibrium in singleton congestion games
    with the same action spaces without leadership, and extend it to the
    computation of a Stackelberg equilibrium for the case where the leader is
    restricted to pure strategies. For the cases in which the problem of finding an
    equilibrium is hard, we show how, in the optimistic setting where the followers
    break ties in favor of the leader, the problem can be formulated via
    mixed-integer linear programming techniques, which computational experiments
    show to scale quite well.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castiglioni_M/0/1/0/all/0/1&quot;&gt;Matteo Castiglioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchesi_A/0/1/0/all/0/1&quot;&gt;Alberto Marchesi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gatti_N/0/1/0/all/0/1&quot;&gt;Nicola Gatti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coniglio_S/0/1/0/all/0/1&quot;&gt;Stefano Coniglio&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10217">
    <title>Competitive Data Trading in Wireless-Powered Internet of Things (IoT) Crowdsensing Systems with Blockchain. (arXiv:1808.10217v1 [cs.NI])</title>
    <link>http://arxiv.org/abs/1808.10217</link>
    <description rdf:parseType="Literal">&lt;p&gt;With the explosive growth of smart IoT devices at the edge of the Internet,
    embedding sensors on mobile devices for massive data collection and collective
    environment sensing has been envisioned as a cost-effective solution for IoT
    applications. However, existing IoT platforms and framework rely on dedicated
    middleware for (semi-) centralized task dispatching, data storage and incentive
    provision. Consequently, they are usually expensive to deploy, have limited
    adaptability to diverse requirements, and face a series of data security and
    privacy issues. In this paper, we employ permissionless blockchains to
    construct a purely decentralized platform for data storage and trading in a
    wireless-powered IoT crowdsensing system. In the system, IoT sensors use power
    wirelessly transferred from RF-energy beacons for data sensing and transmission
    to an access point. The data is then forwarded to the blockchain for
    distributed ledger services, i.e., data/transaction verification, recording,
    and maintenance. Due to coupled interference of wireless transmission and
    transaction fee incurred from blockchain&apos;s distributed ledger services,
    rational sensors have to decide on their transmission rates to maximize
    individual utility. Thus, we formulate a noncooperative game model to analyze
    this competitive situation among the sensors. We provide the analytical
    condition for the existence of the Nash equilibrium as well as a series of
    insightful numerical results about the equilibrium strategies in the game.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shaohan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1&quot;&gt;Dusit Niyato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dong In Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Ping Wang&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10231">
    <title>Asheetoxy: A Taxonomy for Classifying Negative Spreadsheet-related Phenomena. (arXiv:1808.10231v1 [cs.SE])</title>
    <link>http://arxiv.org/abs/1808.10231</link>
    <description rdf:parseType="Literal">&lt;p&gt;Spreadsheets (sometimes also called Excel programs) are powerful tools which
    play a business-critical role in many organizations. However, due to faulty
    spreadsheets many bad decisions have been taken in recent years. Since then, a
    number of researchers have been studying spreadsheet errors. However, one issue
    that hinders discussion among researchers and professionals is the lack of a
    commonly accepted taxonomy.
    &lt;/p&gt;
    &lt;p&gt;Albeit a number of taxonomies for spreadsheet errors have been proposed in
    previous work, a major issue is that they use the term error that itself is
    already ambiguous. Furthermore, to apply most existing taxonomies, detailed
    knowledge about the underlying process and knowledge about the &quot;brain state&quot; of
    the acting spreadsheet users is required. Due to these limitations, known
    error-like phenomena in freely available spreadsheet corpora cannot be
    classified with these taxonomies.
    &lt;/p&gt;
    &lt;p&gt;We propose Asheetoxy, a simple and phenomenon-oriented taxonomy that avoids
    the problematic term error altogether. An initial study with 7 participants
    indicates that even non-spreadsheet researchers similarly classify real-world
    spreadsheet phenomena using Asheetoxy.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulesz_D/0/1/0/all/0/1&quot;&gt;Daniel Kulesz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1&quot;&gt;Stefan Wagner&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10232">
    <title>Automated Scene Flow Data Generation for Training and Verification. (arXiv:1808.10232v1 [cs.CV])</title>
    <link>http://arxiv.org/abs/1808.10232</link>
    <description rdf:parseType="Literal">&lt;p&gt;Scene flow describes the 3D position as well as the 3D motion of each pixel
    in an image. Such algorithms are the basis for many state-of-the-art autonomous
    or automated driving functions. For verification and training large amounts of
    ground truth data is required, which is not available for real data. In this
    paper, we demonstrate a technology to create synthetic data with dense and
    precise scene flow ground truth.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasenmuller_O/0/1/0/all/0/1&quot;&gt;Oliver Wasenm&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuster_R/0/1/0/all/0/1&quot;&gt;Ren&amp;#xe9; Schuster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1&quot;&gt;Didier Stricker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leiss_K/0/1/0/all/0/1&quot;&gt;Karl Leiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rger Pfister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganus_O/0/1/0/all/0/1&quot;&gt;Oleksandra Ganus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tatsch_J/0/1/0/all/0/1&quot;&gt;Julian Tatsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savkin_A/0/1/0/all/0/1&quot;&gt;Artem Savkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brasch_N/0/1/0/all/0/1&quot;&gt;Nikolas Brasch&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10239">
    <title>Learning to adapt: a meta-learning approach for speaker adaptation. (arXiv:1808.10239v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10239</link>
    <description rdf:parseType="Literal">&lt;p&gt;The performance of automatic speech recognition systems can be improved by
    adapting an acoustic model to compensate for the mismatch between training and
    testing conditions, for example by adapting to unseen speakers. The success of
    speaker adaptation methods relies on selecting weights that are suitable for
    adaptation and using good adaptation schedules to update these weights in order
    not to overfit to the adaptation data. In this paper we investigate a
    principled way of adapting all the weights of the acoustic model using a
    meta-learning. We show that the meta-learner can learn to perform supervised
    and unsupervised speaker adaptation and that it outperforms a strong baseline
    adapting LHUC parameters when adapting a DNN AM with 1.5M parameters. We also
    report initial experiments on adapting TDNN AMs, where the meta-learner
    achieves comparable performance with LHUC.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klejch_O/0/1/0/all/0/1&quot;&gt;Ond&amp;#x159;ej Klejch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fainberg_J/0/1/0/all/0/1&quot;&gt;Joachim Fainberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bell_P/0/1/0/all/0/1&quot;&gt;Peter Bell&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10240">
    <title>Most Permissive Semantics of Boolean Networks. (arXiv:1808.10240v1 [cs.FL])</title>
    <link>http://arxiv.org/abs/1808.10240</link>
    <description rdf:parseType="Literal">&lt;p&gt;As shown in [3], the usual update modes of Boolean networks (BNs), including
    synchronous and (generalized) asynchronous, fail to capture behaviours
    introduced by multivalued refinements. Thus, update modes do not allow a
    correct abstract reasoning on dynamics of biological systems, as they may lead
    to reject valid BN models. We introduce a new semantics for interpreting BNs
    which meets with a correct abstraction of any multivalued refinements, with any
    update mode. This semantics subsumes all the usual updating modes, while
    enabling new behaviours achievable by more concrete models. Moreover, it
    appears that classical dynamical analyses of reachability and attractors have a
    simpler computational complexity: -- reachability can be assessed in a
    polynomial number of iterations (instead of being PSPACE-complete with update
    modes); -- attractors are hypercubes, and deciding the existence of attractors
    with a given upper-bounded dimension is in NP (instead of PSPACE-complete with
    update modes). The computation of iterations is in NP in the very general case,
    and is linear when local functions are monotonic, or with some usual
    representations of functions of BNs (binary decision diagrams, Petri nets,
    automata networks, etc.). In brief, the most permissive semantics of BNs
    enables a correct abstract reasoning on dynamics of BNs, with a greater
    tractability than previously introduced update modes. This technical report
    lists the main definitions and properties of the most permissive semantics of
    BNs, and draw some remaining open questions.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatain_T/0/1/0/all/0/1&quot;&gt;Thomas Chatain&lt;/a&gt; (MEXICO), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haar_S/0/1/0/all/0/1&quot;&gt;Stefan Haar&lt;/a&gt; (MEXICO), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pauleve_L/0/1/0/all/0/1&quot;&gt;Lo&amp;#xef;c Paulev&amp;#xe9;&lt;/a&gt; (BioInfo - LRI)</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10245">
    <title>Comparative Studies of Detecting Abusive Language on Twitter. (arXiv:1808.10245v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10245</link>
    <description rdf:parseType="Literal">&lt;p&gt;The context-dependent nature of online aggression makes annotating large
    collections of data extremely difficult. Previously studied datasets in abusive
    language detection have been insufficient in size to efficiently train deep
    learning models. Recently, Hate and Abusive Speech on Twitter, a dataset much
    greater in size and reliability, has been released. However, this dataset has
    not been comprehensively studied to its potential. In this paper, we conduct
    the first comparative study of various learning models on Hate and Abusive
    Speech on Twitter, and discuss the possibility of using additional features and
    context data for improvements. Experimental results show that bidirectional GRU
    networks trained on word-level features, with Latent Topic Clustering modules,
    is the most accurate model scoring 0.805 F1.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Younghun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Seunghyun Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1&quot;&gt;Kyomin Jung&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10250">
    <title>SonarSnoop: Active Acoustic Side-Channel Attacks. (arXiv:1808.10250v1 [cs.CR])</title>
    <link>http://arxiv.org/abs/1808.10250</link>
    <description rdf:parseType="Literal">&lt;p&gt;We report the first active acoustic side-channel attack. Speakers are used to
    emit human inaudible acoustic signals and the echo is recorded via microphones,
    turning the acoustic system of a smart phone into a sonar system. The echo
    signal can be used to profile user interaction with the device. For example, a
    victim&apos;s finger movements can be inferred to steal Android phone unlock
    patterns. In our empirical study, the number of candidate unlock patterns that
    an attacker must try to authenticate herself to a Samsung S4 Android phone can
    be reduced by up to 70% using this novel acoustic side-channel. Our approach
    can be easily applied to other application scenarios and device types. Overall,
    our work highlights a new family of security threats.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Peng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagci_I/0/1/0/all/0/1&quot;&gt;Ibrahim Ethem Bagci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roedig_U/0/1/0/all/0/1&quot;&gt;Utz Roedig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jeff Yan&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10259">
    <title>Analyze Unstructured Data Patterns for Conceptual Representation. (arXiv:1808.10259v1 [cs.IR])</title>
    <link>http://arxiv.org/abs/1808.10259</link>
    <description rdf:parseType="Literal">&lt;p&gt;Online news media provides aggregated news and stories from different sources
    all over the world and up-to-date news coverage. The main goal of this study is
    to have a solution that considered as a homogeneous source for the news and to
    represent the news in a new conceptual framework. Furthermore, the user can
    easily find different updated news in a fast way through the designed
    interface. The Mobile App implementation is based on modeling the multi-level
    conceptual analysis discipline. Discovering main concepts of any domain is
    captured from the hidden unstructured data that are analyzed by the proposed
    solution. Concepts are discovered through analyzing data patterns to be
    structured into a tree-based interface for easy navigation for the end user,
    through the discovered news concepts. Our final experiment results showing that
    analyzing the news before displaying to the end-user and restructuring the
    final output in a conceptual multilevel structure, that producing new display
    frame for the end user to find the related information to his interest.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aqle_A/0/1/0/all/0/1&quot;&gt;Aboubakr Aqle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Thani_D/0/1/0/all/0/1&quot;&gt;Dena Al-Thani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaoua_A/0/1/0/all/0/1&quot;&gt;Ali Jaoua&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10260">
    <title>Understanding Latent Factors Using a GWAP. (arXiv:1808.10260v1 [cs.IR])</title>
    <link>http://arxiv.org/abs/1808.10260</link>
    <description rdf:parseType="Literal">&lt;p&gt;Recommender systems relying on latent factor models often appear as black
    boxes to their users. Semantic descriptions for the factors might help to
    mitigate this problem. Achieving this automatically is, however, a
    non-straightforward task due to the models&apos; statistical nature. We present an
    output-agreement game that represents factors by means of sample items and
    motivates players to create such descriptions. A user study shows that the
    collected output actually reflects real-world characteristics of the factors.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunkel_J/0/1/0/all/0/1&quot;&gt;Johannes Kunkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loepp_B/0/1/0/all/0/1&quot;&gt;Benedikt Loepp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziegler_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Ziegler&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10261">
    <title>Centroid estimation based on symmetric KL divergence for Multinomial text classification problem. (arXiv:1808.10261v1 [cs.IR])</title>
    <link>http://arxiv.org/abs/1808.10261</link>
    <description rdf:parseType="Literal">&lt;p&gt;We define a new method to estimate centroid for text classification based on
    the symmetric KL-divergence between the distribution of words in training
    documents and their class centroids. Experiments on several standard data sets
    indicate that the new method achieves substantial improvements over the
    traditional classifiers.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiangning Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matzinger_H/0/1/0/all/0/1&quot;&gt;Heinrich Matzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_H/0/1/0/all/0/1&quot;&gt;Haoyan Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mi Zhou&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10262">
    <title>Capacity of Locally Recoverable Codes. (arXiv:1808.10262v1 [cs.IT])</title>
    <link>http://arxiv.org/abs/1808.10262</link>
    <description rdf:parseType="Literal">&lt;p&gt;Motivated by applications in distributed storage, the notion of a locally
    recoverable code (LRC) was introduced a few years back. In an LRC, any
    coordinate of a codeword is recoverable by accessing only a small number of
    other coordinates. While different properties of LRCs have been well-studied,
    their performance on channels with random erasures or errors has been mostly
    unexplored. In this note, we analyze the performance of LRCs over such
    stochastic channels. In particular, for input-symmetric discrete memoryless
    channels, we give a tight characterization of the gap to Shannon capacity when
    LRCs are used over the channel.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1&quot;&gt;Arya Mazumdar&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10267">
    <title>Multi-Source Syntactic Neural Machine Translation. (arXiv:1808.10267v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10267</link>
    <description rdf:parseType="Literal">&lt;p&gt;We introduce a novel multi-source technique for incorporating source syntax
    into neural machine translation using linearized parses. This is achieved by
    employing separate encoders for the sequential and parsed versions of the same
    source sentence; the resulting representations are then combined using a
    hierarchical attention mechanism. The proposed model improves over both seq2seq
    and parsed baselines by over 1 BLEU on the WMT17 English-German task. Further
    analysis shows that our multi-source syntactic model is able to translate
    successfully without any parsed input, unlike standard parsed methods. In
    addition, performance does not deteriorate as much on long sentences as for the
    baselines.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Currey_A/0/1/0/all/0/1&quot;&gt;Anna Currey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heafield_K/0/1/0/all/0/1&quot;&gt;Kenneth Heafield&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10290">
    <title>Acquiring Annotated Data with Cross-lingual Explicitation for Implicit Discourse Relation Classification. (arXiv:1808.10290v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10290</link>
    <description rdf:parseType="Literal">&lt;p&gt;Implicit discourse relation classification is one of the most challenging and
    important tasks in discourse parsing, due to the lack of connective as strong
    linguistic cues. A principle bottleneck to further improvement is the shortage
    of training data (ca.~16k instances in the PDTB). Shi et al. (2017) proposed to
    acquire additional data by exploiting connectives in translation: human
    translators mark discourse relations which are implicit in the source language
    explicitly in the translation. Using back-translations of such explicitated
    connectives improves discourse relation parsing performance. This paper
    addresses the open question of whether the choice of the translation language
    matters, and whether multiple translations into different languages can be
    effectively used to improve the quality of the additional data.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Wei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yung_F/0/1/0/all/0/1&quot;&gt;Frances Yung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1&quot;&gt;Vera Demberg&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10292">
    <title>A study of integer sorting on multicores. (arXiv:1808.10292v1 [cs.DC])</title>
    <link>http://arxiv.org/abs/1808.10292</link>
    <description rdf:parseType="Literal">&lt;p&gt;Integer sorting on multicores and GPUs can be realized by a variety of
    approaches that include variants of distribution-based methods such as
    radix-sort, comparison-oriented algorithms such as deterministic regular
    sampling and random sampling parallel sorting, and network-based algorithms
    such as Batcher&apos;s bitonic sorting algorithm.
    &lt;/p&gt;
    &lt;p&gt;In this work we present an experimental study of integer sorting on multicore
    processors. We have implemented serial and parallel radix-sort for various
    radixes, deterministic regular oversampling and random oversampling parallel
    sorting, and also some previously little explored or unexplored variants of
    bitonic-sort and odd-even transposition sort.
    &lt;/p&gt;
    &lt;p&gt;The study uses multithreading and multiprocessing parallel programming
    libraries with the C language implementations working under Open MPI,
    MulticoreBSP, and BSPlib utilizing the same source code.
    &lt;/p&gt;
    &lt;p&gt;A secondary objective is to attempt to model the performance of these
    algorithm implementations under the MBSP (Multi-memory BSP) model. We first
    provide some general high-level observations on the performance of these
    implementations. If we can conclude anything is that accurate prediction of
    performance by taking into consideration architecture dependent features such
    as the structure and characteristics of multiple memory hierarchies is
    difficult and more often than not untenable. To some degree this is affected by
    the overhead imposed by the high-level library used in the programming effort.
    We can still draw however some reliable conclusions and reason about the
    performance of these implementations using the MBSP model, thus making MBSP
    useful and usable.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerbessiotis_A/0/1/0/all/0/1&quot;&gt;Alexandros V. Gerbessiotis&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10300">
    <title>Self-stabilizing Overlays for high-dimensional Monotonic Searchability. (arXiv:1808.10300v1 [cs.DC])</title>
    <link>http://arxiv.org/abs/1808.10300</link>
    <description rdf:parseType="Literal">&lt;p&gt;We extend the concept of monotonic searchability for self-stabilizing systems
    from one to multiple dimensions. A system is self-stabilizing if it can recover
    to a legitimate state from any initial illegal state. These kind of systems are
    most often used in distributed applications. Monotonic searchability provides
    guarantees when searching for nodes while the recovery process is going on.
    More precisely, if a search request started at some node $u$ succeeds in
    reaching its destination $v$, then all future search requests from $u$ to $v$
    succeed as well. Although there already exists a self-stabilizing protocol for
    a two-dimensional topology and an universal approach for monotonic
    searchability, it is not clear how both of these concepts fit together
    effectively. The latter concept even comes with some restrictive assumptions on
    messages, which is not the case for our protocol. We propose a simple novel
    protocol for a self-stabilizing two-dimensional quadtree that satisfies
    monotonic searchability. Our protocol can easily be extended to higher
    dimensions and offers routing in $\mathcal O(\log n)$ hops for any search
    request.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldmann_M/0/1/0/all/0/1&quot;&gt;Michael Feldmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolb_C/0/1/0/all/0/1&quot;&gt;Christina Kolb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheideler_C/0/1/0/all/0/1&quot;&gt;Christian Scheideler&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10307">
    <title>Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation. (arXiv:1808.10307v1 [cs.CR])</title>
    <link>http://arxiv.org/abs/1808.10307</link>
    <description rdf:parseType="Literal">&lt;p&gt;Deep learning models have consistently outperformed traditional machine
    learning models in various classification tasks, including image
    classification. As such, they have become increasingly prevalent in many real
    world applications including those where security is of great concern. Such
    popularity, however, may attract attackers to exploit the vulnerabilities of
    the deployed deep learning models and launch attacks against security-sensitive
    applications. In this paper, we focus on a specific type of data poisoning
    attack, which we refer to as a {\em backdoor injection attack}. The main goal
    of the adversary performing such attack is to generate and inject a backdoor
    into a deep learning model that can be triggered to recognize certain embedded
    patterns with a target label of the attacker&apos;s choice. Additionally, a backdoor
    injection attack should occur in a stealthy manner, without undermining the
    efficacy of the victim model. Specifically, we propose two approaches for
    generating a backdoor that is hardly perceptible yet effective in poisoning the
    model. We consider two attack settings, with backdoor injection carried out
    either before model training or during model updating. We carry out extensive
    experimental evaluations under various assumptions on the adversary model, and
    demonstrate that such attacks can be effective and achieve a high attack
    success rate (above $90\%$) at a small cost of model accuracy loss (below
    $1\%$) with a small injection rate (around $1\%$), even under the weakest
    assumption wherein the adversary has no knowledge either of the original
    training data or the classifier model.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1&quot;&gt;Cong Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1&quot;&gt;Haoti Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Squicciarini_A/0/1/0/all/0/1&quot;&gt;Anna Squicciarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Sencun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1&quot;&gt;David Miller&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10308">
    <title>Inadequate Risk Analysis Might Jeopardize The Functional Safety of Modern Systems. (arXiv:1808.10308v1 [cs.SE])</title>
    <link>http://arxiv.org/abs/1808.10308</link>
    <description rdf:parseType="Literal">&lt;p&gt;In the early 90s, researchers began to focus on security as an important
    property to address in combination with safety. Over the years, researchers
    have proposed approaches to harmonize activities within the safety and security
    disciplines. Despite the academic efforts to identify interdependencies and to
    propose combined approaches for safety and security, there is still a lack of
    integration between safety and security practices in the industrial context, as
    they have separate standards and independent processes often addressed and
    assessed by different organizational teams and authorities. Specifically,
    security concerns are generally not covered in any detail in safety standards
    potentially resulting in successfully safety-certified systems that still are
    open for security threats from e.g., malicious intents from internal and
    external personnel and hackers that may jeopardize safety. In recent years
    security has again received an increasing attention of being an important issue
    also in safety assurance, as the open interconnected nature of emerging systems
    makes them susceptible to security threats at a much higher degree than
    existing more confined products.This article presents initial ideas on how to
    extend safety work to include aspects of security during the context
    establishment and initial risk assessment procedures. The ambition of our
    proposal is to improve safety and increase efficiency and effectiveness of the
    safety work within the frames of the current safety standards, i.e., raised
    security awareness in compliance with the current safety standards. We believe
    that our proposal is useful to raise the security awareness in industrial
    contexts, although it is not a complete harmonization of safety and security
    disciplines, as it merely provides applicable guidance to increase security
    awareness in a safety context.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanninen_K/0/1/0/all/0/1&quot;&gt;Kaj H&amp;#xe4;nninen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hansson_H/0/1/0/all/0/1&quot;&gt;Hans Hansson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thane_H/0/1/0/all/0/1&quot;&gt;Henrik Thane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saadatmand_M/0/1/0/all/0/1&quot;&gt;Mehrdad Saadatmand&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10313">
    <title>RoI-based Robotic Grasp Detection in Object Overlapping Scenes Using Convolutional Neural Network. (arXiv:1808.10313v1 [cs.RO])</title>
    <link>http://arxiv.org/abs/1808.10313</link>
    <description rdf:parseType="Literal">&lt;p&gt;Grasp detection is an essential skill for widespread use of robots. Recent
    works demonstrate the advanced performance of Convolutional Neural Network
    (CNN) on robotic grasp detection. However, a significant shortcoming of
    existing grasp detection algorithms is that they all ignore the affiliation
    between grasps and targets. In this paper, we propose a robotic grasp detection
    algorithm based on Region of Interest (RoI) to simultaneously detect targets
    and their grasps in object overlapping scenes. Our proposed algorithm uses
    Regions of Interest (RoIs) to detect grasps while doing classification and
    location regression of targets. To train the network, we contribute a much
    bigger multi-object grasp dataset than Cornell Grasp Dataset, which is based on
    Visual Manipulation Relationship Dataset. Experimental results demonstrate that
    our algorithm achieves 24.0% miss rate at 1FPPI and 70.5% mAP with grasp on our
    dataset. Robotic experiments demonstrate that our proposed algorithm can help
    robots grasp specified target in multi-object scenes at 84% success rate.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanbo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1&quot;&gt;Xuguang Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xinwen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1&quot;&gt;Nanning Zheng&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10316">
    <title>Fully Dynamic MIS in Uniformly Sparse Graphs. (arXiv:1808.10316v1 [cs.DS])</title>
    <link>http://arxiv.org/abs/1808.10316</link>
    <description rdf:parseType="Literal">&lt;p&gt;We consider the problem of maintaining a maximal independent set (MIS) in a
    dynamic graph subject to edge insertions and deletions. Recently, Assadi, Onak,
    Schieber and Solomon (STOC 2018) showed that an MIS can be maintained in
    sublinear (in the dynamically changing number of edges) amortized update time.
    In this paper we significantly improve the update time for uniformly sparse
    graphs. Specifically, for graphs with arboricity $\alpha$, the amortized update
    time of our algorithm is $O(\alpha^2 \cdot \log^2 n)$, where $n$ is the number
    of vertices. For low arboricity graphs, which include, for example, minor-free
    graphs as well as some classes of `real world&apos; graphs, our update time is
    polylogarithmic. Our update time improves the result of Assadi et al. for all
    graphs with arboricity bounded by $m^{3/8 - \epsilon}$, for any constant
    $\epsilon &amp;gt; 0$. This covers much of the range of possible values for
    arboricity, as the arboricity of a general graph cannot exceed $m^{1/2}$.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Onak_K/0/1/0/all/0/1&quot;&gt;Krzysztof Onak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schieber_B/0/1/0/all/0/1&quot;&gt;Baruch Schieber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solomon_S/0/1/0/all/0/1&quot;&gt;Shay Solomon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wein_N/0/1/0/all/0/1&quot;&gt;Nicole Wein&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10322">
    <title>PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors. (arXiv:1808.10322v1 [cs.CV])</title>
    <link>http://arxiv.org/abs/1808.10322</link>
    <description rdf:parseType="Literal">&lt;p&gt;We present PPF-FoldNet for unsupervised learning of 3D local descriptors on
    pure point cloud geometry. Based on the folding-based auto-encoding of well
    known point pair features, PPF-FoldNet offers many desirable properties: it
    necessitates neither supervision, nor a sensitive local reference frame,
    benefits from point-set sparsity, is end-to-end, fast, and can extract powerful
    rotation invariant descriptors. Thanks to a novel feature visualization, its
    evolution can be monitored to provide interpretable insights. Our extensive
    experiments demonstrate that despite having six degree-of-freedom invariance
    and lack of training labels, our network achieves state of the art results in
    standard benchmark datasets and outperforms its competitors when rotations and
    varying point densities are present. PPF-FoldNet achieves $9\%$ higher recall
    on standard benchmarks, $23\%$ higher recall when rotations are introduced into
    the same datasets and finally, a margin of $&amp;gt;35\%$ is attained when point
    density is significantly decreased.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Haowen Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1&quot;&gt;Tolga Birdal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1&quot;&gt;Slobodan Ilic&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10326">
    <title>Generalize Symbolic Knowledge With Neural Rule Engine. (arXiv:1808.10326v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10326</link>
    <description rdf:parseType="Literal">&lt;p&gt;Neural-symbolic learning aims to take the advantages of both neural networks
    and symbolic knowledge to build better intelligent systems. As neural networks
    have dominated the state-of-the-art results in a wide range of NLP tasks, it
    attracts considerable attention to improve the performance of neural models by
    integrating symbolic knowledge. Different from existing works, this paper
    investigates the combination of these two powerful paradigms from the
    knowledge-driven side. We propose Neural Rule Engine (NRE), which can learn
    knowledge explicitly from logic rules and then generalize them implicitly with
    neural networks. NRE is implemented with neural module networks in which each
    module represents an action of the logic rule. The experiments show that NRE
    could greatly improve the generalization abilities of logic rules with a
    significant increase on recall. Meanwhile, the precision is still maintained at
    a high level.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hengru Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhengdong Lu&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10328">
    <title>Asymptotically Optimal Codes Correcting Fixed-Length Duplication Errors in DNA Storage Systems. (arXiv:1808.10328v1 [cs.IT])</title>
    <link>http://arxiv.org/abs/1808.10328</link>
    <description rdf:parseType="Literal">&lt;p&gt;A (tandem) duplication of length $ k $ is an insertion of an exact copy of a
    substring of length $ k $ next to its original position. This and related types
    of impairments are of relevance in modeling communication in the presence of
    synchronization errors, as well as in several information storage applications.
    We demonstrate that Levenshtein&apos;s construction of binary codes correcting
    insertions of zeros is, with minor modifications, applicable also to channels
    with arbitrary alphabets and with duplication errors of arbitrary (but fixed)
    length $ k $. Furthermore, we derive bounds on the cardinality of optimal $ q
    $-ary codes correcting up to $ t $ duplications of length $ k $, and establish
    the following corollaries in the asymptotic regime of growing block-length: 1.)
    the presented family of codes is optimal for every $ q, t, k $, in the sense of
    the asymptotic scaling of code redundancy; 2.) the upper bound, when
    specialized to $ q = 2 $, $ k = 1 $, improves upon Levenshtein&apos;s bound for
    every $ t \geq 3 $; 3.) the bounds coincide for $ t = 1 $, thus yielding the
    exact asymptotic behavior of the size of optimal single-duplication-correcting
    codes.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovacevic_M/0/1/0/all/0/1&quot;&gt;Mladen Kova&amp;#x10d;evi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1&quot;&gt;Vincent Y. F. Tan&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10340">
    <title>A Coordinate-Free Construction of Scalable Natural Gradient. (arXiv:1808.10340v1 [cs.LG])</title>
    <link>http://arxiv.org/abs/1808.10340</link>
    <description rdf:parseType="Literal">&lt;p&gt;Most neural networks are trained using first-order optimization methods,
    which are sensitive to the parameterization of the model. Natural gradient
    descent is invariant to smooth reparameterizations because it is defined in a
    coordinate-free way, but tractable approximations are typically defined in
    terms of coordinate systems, and hence may lose the invariance properties. We
    analyze the invariance properties of the Kronecker-Factored Approximate
    Curvature (K-FAC) algorithm by constructing the algorithm in a coordinate-free
    way. We explicitly construct a Riemannian metric under which the natural
    gradient matches the K-FAC update; invariance to affine transformations of the
    activations follows immediately. We extend our framework to analyze the
    invariance properties of K-FAC applied to convolutional networks and recurrent
    neural networks, as well as metrics other than the usual Fisher metric.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luk_K/0/1/0/all/0/1&quot;&gt;Kevin Luk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1&quot;&gt;Roger Grosse&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10350">
    <title>IEA: Inner Ensemble Average within a convolutional neural network. (arXiv:1808.10350v1 [cs.LG])</title>
    <link>http://arxiv.org/abs/1808.10350</link>
    <description rdf:parseType="Literal">&lt;p&gt;Ensemble learning is a method of combining multiple trained models to improve
    the model accuracy. We introduce the usage of such methods, specifically
    ensemble average inside Convolutional Neural Networks (CNNs) architectures. By
    Inner Average Ensemble (IEA) of multiple convolutional neural layers (CNLs)
    replacing the single CNLs inside the CNN architecture, the accuracy of the CNN
    increased. A visual and a similarity score analysis of the features generated
    from IEA explains why it boosts the model performance. Empirical results using
    different benchmarking datasets and well-known deep model architectures shows
    that IEA outperforms the ordinary CNL used in CNNs.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1&quot;&gt;Abduallah A. Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Claudel_C/0/1/0/all/0/1&quot;&gt;Christian Claudel&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10351">
    <title>Large-Scale Cover Song Detection in Digital Music Libraries Using Metadata, Lyrics and Audio Features. (arXiv:1808.10351v1 [cs.IR])</title>
    <link>http://arxiv.org/abs/1808.10351</link>
    <description rdf:parseType="Literal">&lt;p&gt;Cover song detection is a very relevant task in Music Information Retrieval
    (MIR) studies and has been mainly addressed using audio-based systems. Despite
    its potential impact in industrial contexts, low performances and lack of
    scalability have prevented such systems from being adopted in practice for
    large applications. In this work, we investigate whether textual music
    information (such as metadata and lyrics) can be used along with audio for
    large-scale cover identification problem in a wide digital music library. We
    benchmark this problem using standard text and state of the art audio
    similarity measures. Our studies shows that these methods can significantly
    increase the accuracy and scalability of cover detection systems on Million
    Song Dataset (MSD) and Second Hand Song (SHS) datasets. By only leveraging
    standard tf-idf based text similarity measures on song titles and lyrics, we
    achieved 35.5% of absolute increase in mean average precision compared to the
    current scalable audio content-based state of the art methods on MSD. These
    experimental results suggests that new methodologies can be encouraged among
    researchers to leverage and identify more sophisticated NLP-based techniques to
    improve current cover song identification systems in digital music libraries
    with metadata.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Correya_A/0/1/0/all/0/1&quot;&gt;Albin Andrew Correya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1&quot;&gt;Romain Hennequin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arcos_M/0/1/0/all/0/1&quot;&gt;Micka&amp;#xeb;l Arcos&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10356">
    <title>Gaussian Mixture Generative Adversarial Networks for Diverse Datasets, and the Unsupervised Clustering of Images. (arXiv:1808.10356v1 [cs.LG])</title>
    <link>http://arxiv.org/abs/1808.10356</link>
    <description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) have been shown to produce
    realistically looking synthetic images with remarkable success, yet their
    performance seems less impressive when the training set is highly diverse. In
    order to provide a better fit to the target data distribution when the dataset
    includes many different classes, we propose a variant of the basic GAN model,
    called Gaussian Mixture GAN (GM-GAN), where the probability distribution over
    the latent space is a mixture of Gaussians. We also propose a supervised
    variant which is capable of conditional sample synthesis. In order to evaluate
    the model&apos;s performance, we propose a new scoring method which separately takes
    into account two (typically conflicting) measures - diversity vs. quality of
    the generated data. Through a series of empirical experiments, using both
    synthetic and real-world datasets, we quantitatively show that GM-GANs
    outperform baselines, both when evaluated using the commonly used Inception
    Score, and when evaluated using our own alternative scoring method. In
    addition, we qualitatively demonstrate how the \textit{unsupervised} variant of
    GM-GAN tends to map latent vectors sampled from different Gaussians in the
    latent space to samples of different classes in the data space. We show how
    this phenomenon can be exploited for the task of unsupervised clustering, and
    provide quantitative evaluation showing the superiority of our method for the
    unsupervised clustering of image datasets. Finally, we demonstrate a feature
    which further sets our model apart from other GAN models: the option to control
    the quality-diversity trade-off by altering, post-training, the probability
    distribution of the latent space. This allows one to sample higher quality and
    lower diversity samples, or vice versa, according to one&apos;s needs.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Yosef_M/0/1/0/all/0/1&quot;&gt;Matan Ben-Yosef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinshall_D/0/1/0/all/0/1&quot;&gt;Daphna Weinshall&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10363">
    <title>IDE-Independent Program Comprehension Tools via Source File Overwriting. (arXiv:1808.10363v1 [cs.SE])</title>
    <link>http://arxiv.org/abs/1808.10363</link>
    <description rdf:parseType="Literal">&lt;p&gt;Traditionally, we have two possibilities to design tools for program
    comprehension and analysis. The first option is to create a standalone program,
    independent of any source code editor. This way, the act of source code editing
    is separated from the act of viewing the code analysis results. The second
    option is to create a plugin for a specific IDE (integrated development
    environment) - in this case, a separate version must be created for each IDE.
    We propose an approach where information about source code elements is written
    directly into source files as annotations or special comments. Before
    committing to a version control system, the annotations are removed from the
    source code to avoid code pollution. We briefly evaluate the approach and
    delineate its limitations.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sulir_M/0/1/0/all/0/1&quot;&gt;Mat&amp;#xfa;&amp;#x161; Sul&amp;#xed;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poruban_J/0/1/0/all/0/1&quot;&gt;Jaroslav Porub&amp;#xe4;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoricak_O/0/1/0/all/0/1&quot;&gt;Ondrej Zori&amp;#x10d;&amp;#xe1;k&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10364">
    <title>Algorithms and Bounds for Drawing Directed Graphs. (arXiv:1808.10364v1 [cs.DS])</title>
    <link>http://arxiv.org/abs/1808.10364</link>
    <description rdf:parseType="Literal">&lt;p&gt;In this paper we present a new approach to visualize directed graphs and
    their hierarchies that completely departs from the classical four-phase
    framework of Sugiyama and computes readable hierarchical visualizations that
    contain the complete reachability information of a graph. Additionally, our
    approach has the advantage that only the necessary edges are drawn in the
    drawing, thus reducing the visual complexity of the resulting drawing.
    Furthermore, most problems involved in our framework require only polynomial
    time. Our framework offers a suite of solutions depending upon the
    requirements, and it consists of only two steps: (a) the cycle removal step (if
    the graph contains cycles) and (b) the channel decomposition and hierarchical
    drawing step. Our framework does not introduce any dummy vertices and it keeps
    the vertices of a channel vertically aligned. The time complexity of the main
    drawing algorithms of our framework is $O(kn)$, where $k$ is the number of
    channels, typically much smaller than $n$ (the number of vertices).
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortali_G/0/1/0/all/0/1&quot;&gt;Giacomo Ortali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tollis_I/0/1/0/all/0/1&quot;&gt;Ioannis G. Tollis&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10366">
    <title>$\beta$-Stars or On Extending a Drawing of a Connected Subgraph. (arXiv:1808.10366v1 [cs.CG])</title>
    <link>http://arxiv.org/abs/1808.10366</link>
    <description rdf:parseType="Literal">&lt;p&gt;We consider the problem of extending the drawing of a subgraph of a given
    plane graph to a drawing of the entire graph using straight-line and polyline
    edges. We define the notion of star complexity of a polygon and show that a
    drawing $\Gamma_H$ of an induced connected subgraph $H$ can be extended with at
    most $\min\{ h/2, \beta + \log_2(h) + 1\}$ bends per edge, where $\beta$ is the
    largest star complexity of a face of $\Gamma_H$ and $h$ is the size of the
    largest face of $H$. This result significantly improves the previously known
    upper bound of $72|V(H)|$ [5] for the case where $H$ is connected. We also show
    that our bound is worst case optimal up to a small additive constant.
    Additionally, we provide an indication of complexity of the problem of testing
    whether a star-shaped inner face can be extended to a straight-line drawing of
    the graph; this is in contrast to the fact that the same problem is solvable in
    linear time for the case of star-shaped outer face [9] and convex inner face
    [13].
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mchedlidze_T/0/1/0/all/0/1&quot;&gt;Tamara Mchedlidze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urhausen_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xf4;me Urhausen&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10367">
    <title>Parametric Topology Optimization with Multi-Resolution Finite Element Models. (arXiv:1808.10367v1 [cs.NA])</title>
    <link>http://arxiv.org/abs/1808.10367</link>
    <description rdf:parseType="Literal">&lt;p&gt;We present a methodical procedure for topology optimization under uncertainty
    with multi-resolution finite element models. We use our framework in a
    bi-fidelity setting where a coarse and a fine mesh corresponding to low- and
    high-resolution models are available. The inexpensive low-resolution model is
    used to explore the parameter space and approximate the parameterized
    high-resolution model and its sensitivity where parameters are considered in
    both structural load and stiffness. We provide error bounds for bi-fidelity
    finite element (FE) approximations and their sensitivities and conduct
    numerical studies to verify these theoretical estimates. We demonstrate our
    approach on benchmark compliance minimization problems where we show
    significant reduction in computational cost for expensive problems such as
    topology optimization under manufacturing variability while generating almost
    identical designs to those obtained with single resolution mesh. We also
    compute the parametric Von-Mises stress for the generated designs via our
    bi-fidelity FE approximation and compare them with standard Monte Carlo
    simulations. The implementation of our algorithm which extends the well-known
    88-line topology optimization code in MATLAB is provided.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keshavarzzadeh_V/0/1/0/all/0/1&quot;&gt;Vahid Keshavarzzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirby_R/0/1/0/all/0/1&quot;&gt;Robert M. Kirby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayan_A/0/1/0/all/0/1&quot;&gt;Akil Narayan&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10369">
    <title>Robot_gym: accelerated robot training through simulation in the cloud with ROS and Gazebo. (arXiv:1808.10369v1 [cs.RO])</title>
    <link>http://arxiv.org/abs/1808.10369</link>
    <description rdf:parseType="Literal">&lt;p&gt;Rather than programming, training allows robots to achieve behaviors that
    generalize better and are capable to respond to real-world needs. However, such
    training requires a big amount of experimentation which is not always feasible
    for a physical robot. In this work, we present robot_gym, a framework to
    accelerate robot training through simulation in the cloud that makes use of
    roboticists&apos; tools, simplifying the development and deployment processes on
    real robots. We unveil that, for simple tasks, simple 3DoF robots require more
    than 140 attempts to learn. For more complex, 6DoF robots, the number of
    attempts increases to more than 900 for the same task. We demonstrate that our
    framework, for simple tasks, accelerates the robot training time by more than
    33% while maintaining similar levels of accuracy and repeatability.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vilches_V/0/1/0/all/0/1&quot;&gt;V&amp;#xed;ctor Mayoral Vilches&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordero_A/0/1/0/all/0/1&quot;&gt;Alejandro Hern&amp;#xe1;ndez Cordero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calvo_A/0/1/0/all/0/1&quot;&gt;Asier Bilbao Calvo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ugarte_I/0/1/0/all/0/1&quot;&gt;Irati Zamalloa Ugarte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kojcev_R/0/1/0/all/0/1&quot;&gt;Risto Kojcev&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10370">
    <title>Improved approximation algorithms for hitting 3-vertex paths. (arXiv:1808.10370v1 [cs.DS])</title>
    <link>http://arxiv.org/abs/1808.10370</link>
    <description rdf:parseType="Literal">&lt;p&gt;We study the problem of deleting a minimum cost set of vertices from a given
    vertex-weighted graph in such a way that the resulting graph has no induced
    path on three vertices. This problem is often called cluster vertex deletion in
    the literature and admits a straightforward 3-approximation algorithm since it
    is a special case of the vertex cover problem on a 3-uniform hypergraph.
    Recently, You, Wang, and Cao described an efficient 5/2-approximation algorithm
    for the unweighted version of the problem. Our main result is a
    9/4-approximation algorithm for arbitrary weights, using the local ratio
    technique. We further conjecture that the problem admits a 2-approximation
    algorithm and give some support for the conjecture. This is in sharp contrast
    with the fact that the similar problem of deleting vertices to eliminate all
    triangles in a graph is known to be UGC-hard to approximate to within a ratio
    better than 3, as proved by Guruswami and Lee.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiorini_S/0/1/0/all/0/1&quot;&gt;Samuel Fiorini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joret_G/0/1/0/all/0/1&quot;&gt;Gwena&amp;#xeb;l Joret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaudt_O/0/1/0/all/0/1&quot;&gt;Oliver Schaudt&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10375">
    <title>High-Performance Multi-Mode Ptychography Reconstruction on Distributed GPUs. (arXiv:1808.10375v1 [physics.comp-ph])</title>
    <link>http://arxiv.org/abs/1808.10375</link>
    <description rdf:parseType="Literal">&lt;p&gt;Ptychography is an emerging imaging technique that is able to provide
    wavelength-limited spatial resolution from specimen with extended lateral
    dimensions. As a scanning microscopy method, a typical two-dimensional image
    requires a number of data frames. As a diffraction-based imaging technique, the
    real-space image has to be recovered through iterative reconstruction
    algorithms. Due to these two inherent aspects, a ptychographic reconstruction
    is generally a computation-intensive and time-consuming process, which limits
    the throughput of this method. We report an accelerated version of the
    multi-mode difference map algorithm for ptychography reconstruction using
    multiple distributed GPUs. This approach leverages available scientific
    computing packages in Python, including mpi4py and PyCUDA, with the core
    computation functions implemented in CUDA C. We find that interestingly even
    with MPI collective communications, the weak scaling in the number of GPU nodes
    can still remain nearly constant. Most importantly, for realistic diffraction
    measurements, we observe a speedup ranging from a factor of $10$ to $10^3$
    depending on the data size, which reduces the reconstruction time remarkably
    from hours to typically about 1 minute and is thus critical for real-time data
    processing and visualization.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhihua Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yao-Lung L. Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaojing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hanfei Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ha_S/0/1/0/all/0/1&quot;&gt;Sungsoo Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chu_Y/0/1/0/all/0/1&quot;&gt;Yong S. Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Campbell_S/0/1/0/all/0/1&quot;&gt;Stuart I. Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Meifeng Lin&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10379">
    <title>Asymptotic analysis of the Friedkin-Johnsen model when the matrix of the susceptibility weights approaches the identity matrix. (arXiv:1808.10379v1 [cs.SI])</title>
    <link>http://arxiv.org/abs/1808.10379</link>
    <description rdf:parseType="Literal">&lt;p&gt;In this paper we analyze the Friedkin-Johnsen model of opinions [1] when the
    coefficients weighting the agent susceptibilities to interpersonal influence
    approach 1. We will show that in this case, under suitable assumptions, the
    model converges to a quasi-consensus condition among the agents. In general the
    achieved consensus value will be different to the one obtained by the
    corresponding DeGroot model [2].
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pironti_A/0/1/0/all/0/1&quot;&gt;Alfredo Pironti&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10383">
    <title>Deep Chronnectome Learning via Full Bidirectional Long Short-Term Memory Networks for MCI Diagnosis. (arXiv:1808.10383v1 [cs.CV])</title>
    <link>http://arxiv.org/abs/1808.10383</link>
    <description rdf:parseType="Literal">&lt;p&gt;Brain functional connectivity (FC) extracted from resting-state fMRI
    (RS-fMRI) has become a popular approach for disease diagnosis, where
    discriminating subjects with mild cognitive impairment (MCI) from normal
    controls (NC) is still one of the most challenging problems. Dynamic functional
    connectivity (dFC), consisting of time-varying spatiotemporal dynamics, may
    characterize &quot;chronnectome&quot; diagnostic information for improving MCI
    classification. However, most of the current dFC studies are based on detecting
    discrete major brain status via spatial clustering, which ignores rich
    spatiotemporal dynamics contained in such chronnectome. We propose Deep
    Chronnectome Learning for exhaustively mining the comprehensive information,
    especially the hidden higher-level features, i.e., the dFC time series that may
    add critical diagnostic power for MCI classification. To this end, we devise a
    new Fully-connected Bidirectional Long Short-Term Memory Network (Full-BiLSTM)
    to effectively learn the periodic brain status changes using both past and
    future information for each brief time segment and then fuse them to form the
    final output. We have applied our method to a rigorously built large-scale
    multi-site database (i.e., with 164 data from NCs and 330 from MCIs, which can
    be further augmented by 25 folds). Our method outperforms other
    state-of-the-art approaches with an accuracy of 73.6% under solid
    cross-validations. We also made extensive comparisons among multiple variants
    of LSTM models. The results suggest high feasibility of our method with
    promising value also for other brain disorder diagnoses.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1&quot;&gt;Weizheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Han Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_J/0/1/0/all/0/1&quot;&gt;Jing Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinggang Shen&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10387">
    <title>Compensated de Casteljau algorithm in $K$ times the working precision. (arXiv:1808.10387v1 [math.NA])</title>
    <link>http://arxiv.org/abs/1808.10387</link>
    <description rdf:parseType="Literal">&lt;p&gt;In computer aided geometric design a polynomial is usually represented in
    Bernstein form. This paper presents a family of compensated algorithms to
    accurately evaluate a polynomial in Bernstein form with floating point
    coefficients. The principle is to apply error-free transformations to improve
    the traditional de Casteljau algorithm. At each stage of computation, round-off
    error is passed on to first order errors, then to second order errors, and so
    on. After the computation has been &quot;filtered&quot; $(K - 1)$ times via this process,
    the resulting output is as accurate as the de Casteljau algorithm performed in
    $K$ times the working precision. Forward error analysis and numerical
    experiments illustrate the accuracy of this family of algorithms.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hermes_D/0/1/0/all/0/1&quot;&gt;Danny Hermes&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10389">
    <title>Types of Fireballs (Extended Version). (arXiv:1808.10389v1 [cs.LO])</title>
    <link>http://arxiv.org/abs/1808.10389</link>
    <description rdf:parseType="Literal">&lt;p&gt;The good properties of Plotkin&apos;s call-by-value lambda-calculus crucially rely
    on the restriction to weak evaluation and closed terms. Open call-by-value is
    the more general setting where evaluation is weak but terms may be open. Such
    an extension is delicate, and the literature contains a number of proposals.
    Recently, Accattoli and Guerrieri provided detailed operational and
    implementative studies of these proposals, showing that they are equivalent
    from the point of view of termination, and also at the level of time cost
    models.
    &lt;/p&gt;
    &lt;p&gt;This paper explores the denotational semantics of open call-by-value,
    adapting de Carvalho&apos;s analysis of call-by-name via multi types (aka
    non-idempotent intersection types). Our type system characterises normalisation
    and thus provides an adequate relational semantics. Moreover, type derivations
    carry quantitative information about the cost of evaluation: their size bounds
    the number of evaluation steps and the size of the normal form, and we also
    characterise derivations giving exact bounds.
    &lt;/p&gt;
    &lt;p&gt;The study crucially relies on a new, refined presentation of the fireball
    calculus, the simplest proposal for open call-by-value, that is more apt to
    denotational investigations.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Accattoli_B/0/1/0/all/0/1&quot;&gt;Beniamino Accattoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerrieri_G/0/1/0/all/0/1&quot;&gt;Giulio Guerrieri&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10393">
    <title>Learning End-to-end Autonomous Driving using Guided Auxiliary Supervision. (arXiv:1808.10393v1 [cs.LG])</title>
    <link>http://arxiv.org/abs/1808.10393</link>
    <description rdf:parseType="Literal">&lt;p&gt;Learning to drive faithfully in highly stochastic urban settings remains an
    open problem. To that end, we propose a Multi-task Learning from Demonstration
    (MT-LfD) framework which uses supervised auxiliary task prediction to guide the
    main task of predicting the driving commands. Our framework involves an
    end-to-end trainable network for imitating the expert demonstrator&apos;s driving
    commands. The network intermediately predicts visual affordances and action
    primitives through direct supervision which provide the aforementioned
    auxiliary supervised guidance. We demonstrate that such joint learning and
    supervised guidance facilitates hierarchical task decomposition, assisting the
    agent to learn faster, achieve better driving performance and increases
    transparency of the otherwise black-box end-to-end network. We run our
    experiments to validate the MT-LfD framework in CARLA, an open-source urban
    driving simulator. We introduce multiple non-player agents in CARLA and induce
    temporal noise in them for realistic stochasticity.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_A/0/1/0/all/0/1&quot;&gt;Ashish Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1&quot;&gt;Adithya Subramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1&quot;&gt;Anbumani Subramanian&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10394">
    <title>Symbolic regression based genetic approximations of the Colebrook equation for flow friction. (arXiv:1808.10394v1 [cs.CE])</title>
    <link>http://arxiv.org/abs/1808.10394</link>
    <description rdf:parseType="Literal">&lt;p&gt;Widely used in hydraulics, the Colebrook equation for flow friction relates
    implicitly to the input parameters; the Reynolds number, and the relative
    roughness of inner pipe surface, with the output unknown parameter; the flow
    friction factor. In this paper, a few explicit approximations to the Colebrook
    equation are generated using the ability of artificial intelligence to make
    inner patterns to connect input and output parameters in explicit way not
    knowing their nature or the physical law that connects them, but only knowing
    raw numbers. The fact that the used genetic programming tool does not know the
    structure of the Colebrook equation which is based on computationally expensive
    logarithmic law, is used to obtain better structure of the approximations which
    is less demanding for calculation but also enough accurate. All generated
    approximations are with low computational cost because they contain a limited
    number of logarithmic forms used although for normalization of input parameters
    or for acceleration, but they are also sufficiently accurate. The relative
    error regarding the friction factor in best case is up to 0.13% with only two
    logarithmic forms used. As the second logarithm can be accurately approximated
    by the Pade approximation, practically the same error is obtained also using
    only one logarithm.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Praks_P/0/1/0/all/0/1&quot;&gt;Pavel Praks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brkic_D/0/1/0/all/0/1&quot;&gt;Dejan Brkic&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10396">
    <title>A Unified Analysis of Stochastic Momentum Methods for Deep Learning. (arXiv:1808.10396v1 [cs.LG])</title>
    <link>http://arxiv.org/abs/1808.10396</link>
    <description rdf:parseType="Literal">&lt;p&gt;Stochastic momentum methods have been widely adopted in training deep neural
    networks. However, their theoretical analysis of convergence of the training
    objective and the generalization error for prediction is still under-explored.
    This paper aims to bridge the gap between practice and theory by analyzing the
    stochastic gradient (SG) method, and the stochastic momentum methods including
    two famous variants, i.e., the stochastic heavy-ball (SHB) method and the
    stochastic variant of Nesterov&apos;s accelerated gradient (SNAG) method. We propose
    a framework that unifies the three variants. We then derive the convergence
    rates of the norm of gradient for the non-convex optimization problem, and
    analyze the generalization performance through the uniform stability approach.
    Particularly, the convergence analysis of the training objective exhibits that
    SHB and SNAG have no advantage over SG. However, the stability analysis shows
    that the momentum term can improve the stability of the learned model and hence
    improve the generalization performance. These theoretical insights verify the
    common wisdom and are also corroborated by our empirical analysis on deep
    learning.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yan Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianbao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qihang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10399">
    <title>Modeling Empathy and Distress in Reaction to News Stories. (arXiv:1808.10399v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10399</link>
    <description rdf:parseType="Literal">&lt;p&gt;Computational detection and understanding of empathy is an important factor
    in advancing human-computer interaction. Yet to date, text-based empathy
    prediction has the following major limitations: It underestimates the
    psychological complexity of the phenomenon, adheres to a weak notion of ground
    truth where empathic states are ascribed by third parties, and lacks a shared
    corpus. In contrast, this contribution presents the first publicly available
    gold standard for empathy prediction. It is constructed using a novel
    annotation methodology which reliably captures empathy assessments by the
    writer of a statement using multi-item scales. This is also the first
    computational work distinguishing between multiple forms of empathy, empathic
    concern, and personal distress, as recognized throughout psychology. Finally,
    we present experimental results for three different predictive models, of which
    a CNN performs the best.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buechel_S/0/1/0/all/0/1&quot;&gt;Sven Buechel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buffone_A/0/1/0/all/0/1&quot;&gt;Anneke Buffone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slaff_B/0/1/0/all/0/1&quot;&gt;Barry Slaff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1&quot;&gt;Lyle Ungar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Sedoc&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10400">
    <title>A Radix-M Construction for Complementary Sets. (arXiv:1808.10400v1 [cs.IT])</title>
    <link>http://arxiv.org/abs/1808.10400</link>
    <description rdf:parseType="Literal">&lt;p&gt;We extend the paraunitary (PU) theory for complementary pairs to comple-
    mentary sets and complete complementary codes (CCC) by proposing a new PU
    construction. A special, but very important case of complementary sets (and CC-
    C), based on standard delays, is analyzed in details and a new &apos;Radix-M
    generator&apos; (RM-G) is presented. The RM-G can be viewed as a generalization of
    the Boolean generator for complementary pairs. An efficient correlator for
    standard complemen- tary sets and CCC is also presented. Finally, examples of
    polyphase, QAM and hexagonal PU sets of three sequences are given.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budisin_S/0/1/0/all/0/1&quot;&gt;Srdjan Z. Budisin&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10406">
    <title>Towards Reproducible Empirical Research in Meta-Learning. (arXiv:1808.10406v1 [cs.LG])</title>
    <link>http://arxiv.org/abs/1808.10406</link>
    <description rdf:parseType="Literal">&lt;p&gt;Meta-learning is increasingly used to support the recommendation of machine
    learning algorithms and their configurations. Such recommendations are made
    based on meta-data, consisting of performance evaluations of algorithms on
    prior datasets, as well as characterizations of these datasets. These
    characterizations, also called meta-features, describe properties of the data
    which are predictive for the performance of machine learning algorithms trained
    on them. Unfortunately, despite being used in a large number of studies,
    meta-features are not uniformly described and computed, making many empirical
    studies irreproducible and hard to compare. This paper aims to remedy this by
    systematizing and standardizing data characterization measures used in
    meta-learning, and performing an in-depth analysis of their utility. Moreover,
    it presents MFE, a new tool for extracting meta-features from datasets and
    identify more subtle reproducibility issues in the literature, proposing
    guidelines for data characterization that strengthen reproducible empirical
    research in meta-learning.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivolli_A/0/1/0/all/0/1&quot;&gt;Adriano Rivolli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_L/0/1/0/all/0/1&quot;&gt;Lu&amp;#xed;s P. F. Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soares_C/0/1/0/all/0/1&quot;&gt;Carlos Soares&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1&quot;&gt;Joaquin Vanschoren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; C. P. L. F. de Carvalho&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10410">
    <title>The Bounded Laplace Mechanism in Differential Privacy. (arXiv:1808.10410v1 [cs.CR])</title>
    <link>http://arxiv.org/abs/1808.10410</link>
    <description rdf:parseType="Literal">&lt;p&gt;The Laplace mechanism is the workhorse of differential privacy, applied to
    many instances where numerical data is processed. However, the Laplace
    mechanism can return semantically impossible values, such as negative counts,
    due to its infinite support. There are two popular solutions to this: (i)
    bounding/capping the output values and (ii) bounding the mechanism support. In
    this paper, we show that bounding the mechanism support, while using the
    parameters of the pure Laplace mechanism, does not typically preserve
    differential privacy. We also present a robust method to compute the optimal
    mechanism parameters to achieve differential privacy in such a setting.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holohan_N/0/1/0/all/0/1&quot;&gt;Naoise Holohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antonatos_S/0/1/0/all/0/1&quot;&gt;Spiros Antonatos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braghin_S/0/1/0/all/0/1&quot;&gt;Stefano Braghin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aonghusa_P/0/1/0/all/0/1&quot;&gt;P&amp;#xf3;l Mac Aonghusa&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10420">
    <title>A concise frictional contact formulation based on surface potentials and isogeometric discretization. (arXiv:1808.10420v1 [cs.CE])</title>
    <link>http://arxiv.org/abs/1808.10420</link>
    <description rdf:parseType="Literal">&lt;p&gt;This work presents a concise theoretical and computational framework for the
    finite element formulation of frictional contact problems with arbitrarily
    large deformation and sliding. The aim of this work is to extend the contact
    theory based on surface potentials (Sauer and De Lorenzis, 2013) to account for
    friction. Coulomb friction under isothermal conditions is considered here. For
    a consistent friction formulation, we start with the first and second laws of
    thermodynamics and derive the governing equations at the contact interface. A
    so-called interacting gap can then be defined as a kinematic variable unifying
    both sliding/sticking and normal/tangential contact. A variational principle
    for the frictional system can then be formulated based on a purely kinematical
    constraint. The direct elimination approach applied to the tangential part of
    this constraint leads to the so-called moving friction cone approach of
    Wriggers and Haraldsson (2003). Compared with existing friction formulations,
    our approach reduces the theoretical and computational complexity. Several
    numerical examples are presented to demonstrate the accuracy and robustness of
    the proposed friction formulation.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duong_T/0/1/0/all/0/1&quot;&gt;Thang X. Duong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sauer_R/0/1/0/all/0/1&quot;&gt;Roger A. Sauer&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10427">
    <title>Reinforcement Learning Testbed for Power-Consumption Optimization. (arXiv:1808.10427v1 [cs.SY])</title>
    <link>http://arxiv.org/abs/1808.10427</link>
    <description rdf:parseType="Literal">&lt;p&gt;Common approaches to control a data-center cooling system rely on
    approximated system/environment models that are built upon the knowledge of
    mechanical cooling and electrical and thermal management. These models are
    difficult to design and often lead to suboptimal or unstable performance. In
    this paper, we show how deep reinforcement learning techniques can be used to
    control the cooling system of a simulated data center. In contrast to common
    control algorithms, those based on reinforcement learning techniques can
    optimize a system&apos;s performance automatically without the need of explicit
    model knowledge. Instead, only a reward signal needs to be designed. We
    evaluated the proposed algorithm on the open source simulation platform
    EnergyPlus. The experimental results indicate that we can achieve 22%
    improvement compared to a model-based control algorithm built into the
    EnergyPlus. To encourage the reproduction of our work as well as future
    research, we have also publicly released an open-source EnergyPlus wrapper
    interface directly compatible with existing reinforcement learning frameworks.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moriyama_T/0/1/0/all/0/1&quot;&gt;Takao Moriyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magistris_G/0/1/0/all/0/1&quot;&gt;Giovanni De Magistris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tatsubori_M/0/1/0/all/0/1&quot;&gt;Michiaki Tatsubori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1&quot;&gt;Tu-Hoa Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munawar_A/0/1/0/all/0/1&quot;&gt;Asim Munawar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tachibana_R/0/1/0/all/0/1&quot;&gt;Ryuki Tachibana&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10430">
    <title>Nested multi-instance classification. (arXiv:1808.10430v1 [stat.ML])</title>
    <link>http://arxiv.org/abs/1808.10430</link>
    <description rdf:parseType="Literal">&lt;p&gt;There are classification tasks that take as inputs groups of images rather
    than single images. In order to address such situations, we introduce a nested
    multi-instance deep network. The approach is generic in that it is applicable
    to general data instances, not just images. The network has several
    convolutional neural networks grouped together at different stages. This
    primarily differs from other previous works in that we organize instances into
    relevant groups that are treated differently. We also introduce a method to
    replace instances that are missing which successfully creates neutral input
    instances and consistently outperforms standard fill-in methods in real world
    use cases. In addition, we propose a method for manual dropout when a whole
    group of instances is missing that allows us to use richer training data and
    obtain higher accuracy at the end of training. With specific pretraining, we
    find that the model works to great effect on our real world and pub-lic
    datasets in comparison to baseline methods, justifying the different treatment
    among groups of instances.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stec_A/0/1/0/all/0/1&quot;&gt;Alexander Stec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klabjan_D/0/1/0/all/0/1&quot;&gt;Diego Klabjan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Utke_J/0/1/0/all/0/1&quot;&gt;Jean Utke&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10432">
    <title>Attaining the Unattainable? Reassessing Claims of Human Parity in Neural Machine Translation. (arXiv:1808.10432v1 [cs.CL])</title>
    <link>http://arxiv.org/abs/1808.10432</link>
    <description rdf:parseType="Literal">&lt;p&gt;We reassess a recent study (Hassan et al., 2018) that claimed that machine
    translation (MT) has reached human parity for the translation of news from
    Chinese into English, using pairwise ranking and considering three variables
    that were not taken into account in that previous study: the language in which
    the source side of the test set was originally written, the translation
    proficiency of the evaluators, and the provision of inter-sentential context.
    If we consider only original source text (i.e. not translated from another
    language, or translationese), then we find evidence showing that human parity
    has not been achieved. We compare the judgments of professional translators
    against those of non-experts and discover that those of the experts result in
    higher inter-annotator agreement and better discrimination between human and
    machine translations. In addition, we analyse the human translations of the
    test set and identify important translation issues. Finally, based on these
    findings, we provide a set of recommendations for future human evaluations of
    MT.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1&quot;&gt;Antonio Toral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castilho_S/0/1/0/all/0/1&quot;&gt;Sheila Castilho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Ke Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Way_A/0/1/0/all/0/1&quot;&gt;Andy Way&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.10437">
    <title>iCAN: Instance-Centric Attention Network for Human-Object Interaction Detection. (arXiv:1808.10437v1 [cs.CV])</title>
    <link>http://arxiv.org/abs/1808.10437</link>
    <description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed rapid progress in detecting and recognizing
    individual object instances. To understand the situation in a scene, however,
    computers need to recognize how humans interact with surrounding objects. In
    this paper, we tackle the challenging task of detecting human-object
    interactions (HOI). Our core idea is that the appearance of a person or an
    object instance contains informative cues on which relevant parts of an image
    to attend to for facilitating interaction prediction. To exploit these cues, we
    propose an instance-centric attention module that learns to dynamically
    highlight regions in an image conditioned on the appearance of each instance.
    Such an attention-based network allows us to selectively aggregate features
    relevant for recognizing HOIs. We validate the efficacy of the proposed network
    on the Verb in COCO and HICO-DET datasets and show that our approach compares
    favorably with the state-of-the-arts.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chen Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1&quot;&gt;Yuliang Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jia-Bin Huang&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1301.1107">
    <title>Spectral Condition-Number Estimation of Large Sparse Matrices. (arXiv:1301.1107v6 [cs.NA] UPDATED)</title>
    <link>http://arxiv.org/abs/1301.1107</link>
    <description rdf:parseType="Literal">&lt;p&gt;We describe a randomized Krylov-subspace method for estimating the spectral
    condition number of a real matrix A or indicating that it is numerically rank
    deficient. The main difficulty in estimating the condition number is the
    estimation of the smallest singular value \sigma_{\min} of A. Our method
    estimates this value by solving a consistent linear least-squares problem with
    a known solution using a specific Krylov-subspace method called LSQR. In this
    method, the forward error tends to concentrate in the direction of a right
    singular vector corresponding to \sigma_{\min}. Extensive experiments show that
    the method is able to estimate well the condition number of a wide array of
    matrices. It can sometimes estimate the condition number when running a dense
    SVD would be impractical due to the computational cost or the memory
    requirements. The method uses very little memory (it inherits this property
    from LSQR) and it works equally well on square and rectangular matrices.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avron_H/0/1/0/all/0/1&quot;&gt;Haim Avron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Druinsky_A/0/1/0/all/0/1&quot;&gt;Alex Druinsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toledo_S/0/1/0/all/0/1&quot;&gt;Sivan Toledo&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1309.3014">
    <title>Hypercontractivity of spherical averages in Hamming space. (arXiv:1309.3014v2 [math.PR] UPDATED)</title>
    <link>http://arxiv.org/abs/1309.3014</link>
    <description rdf:parseType="Literal">&lt;p&gt;Consider the linear space of functions on the binary hypercube and the linear
    operator $S_\delta$ acting by averaging a function over a Hamming sphere of
    radius $\delta n$ around every point. It is shown that this operator has a
    dimension-independent bound on the norm $L_p \to L_2$ with $p =
    1+(1-2\delta)^2$. This result evidently parallels a classical estimate of
    Bonami and Gross for $L_p \to L_q$ norms for the operator of convolution with a
    Bernoulli noise. The estimate for $S_\delta$ is harder to obtain since the
    latter is neither a part of a semigroup, nor a tensor power. The result is
    shown by a detailed study of the eigenvalues of $S_\delta$ and $L_p\to L_2$
    norms of the Fourier multiplier operators $\Pi_a$ with symbol equal to a
    characteristic function of the Hamming sphere of radius $a$ (in the notation
    common in boolean analysis $\Pi_a f=f^{=a}$, where $f^{=a}$ is a degree-$a$
    component of function $f$). A sample application of the result is given: Any
    set $A\subset \FF_2^n$ with the property that $A+A$ contains a large portion of
    some Hamming sphere (counted with multiplicity) must have cardinality a
    constant multiple of $2^n$.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Polyanskiy_Y/0/1/0/all/0/1&quot;&gt;Yury Polyanskiy&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1602.05901">
    <title>Development of A Platform for Large-scale Reservoir Simulations on Parallel computers. (arXiv:1602.05901v6 [cs.CE] UPDATED)</title>
    <link>http://arxiv.org/abs/1602.05901</link>
    <description rdf:parseType="Literal">&lt;p&gt;This paper presents our work on designing a platform for large-scale
    reservoir simulations. Detailed components, such as grid and linear solver, and
    data structures are introduced, which can serve as a guide to parallel
    reservoir simulations and other parallel applications. The main objective of
    platform is to support implementation of various parallel reservoir simulators
    on distributed-memory parallel systems, where MPI (Message Passing Interface)
    is employed for communications among computation nodes. It provides structured
    grid due to its simplicity and cell-centered data is applied for each cell. The
    platform has a distributed matrix and vector module and a map module. The
    matrix and vector module is the base of our parallel linear systems. The map
    connects grid and linear system modules, which defines various mappings between
    grid and linear systems. Commonly-used Krylov subspace linear solvers are
    implemented, including the restarted GMRES method and the BiCGSTAB method. It
    also has an interface to a parallel algebraic multigrid solver, BoomerAMG from
    HYPRE. Parallel general-purpose preconditioners and special preconditioners for
    reservoir simulations are also developed. Various data structures are designed,
    such as grid, cell, data, linear solver and preconditioner, and some key
    default parameters are presented in this paper. The numerical experiments show
    that our platform has excellent scalability and it can simulate giant reservoir
    models with hundreds of millions of grid cells using thousands of CPU cores.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhangxin Chen&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1602.07273">
    <title>Performance guarantees for model-based Approximate Dynamic Programming in continuous spaces. (arXiv:1602.07273v3 [cs.SY] UPDATED)</title>
    <link>http://arxiv.org/abs/1602.07273</link>
    <description rdf:parseType="Literal">&lt;p&gt;We study both the value function and Q-function formulation of the Linear
    Programming approach to Approximate Dynamic Programming. The approach is
    model-based and optimizes over a restricted function space to approximate the
    value function or Q-function. Working in the discrete time, continuous space
    setting, we provide guarantees for the fitting error and online performance of
    the policy. In particular, the online performance guarantee is obtained by
    analyzing an iterated version of the greedy policy, and the fitting error
    guarantee by analyzing an iterated version of the Bellman inequality. These
    guarantees complement the existing bounds that appear in the literature. The
    Q-function formulation offers benefits, for example, in decentralized
    controller design, however it can lead to computationally demanding
    optimization problems. To alleviate this drawback, we provide a condition that
    simplifies the formulation, resulting in improved computational times.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beuchat_P/0/1/0/all/0/1&quot;&gt;Paul N. Beuchat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Georghiou_A/0/1/0/all/0/1&quot;&gt;Angelos Georghiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lygeros_J/0/1/0/all/0/1&quot;&gt;John Lygeros&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1604.04967">
    <title>On recovering missing values in a pathwise setting. (arXiv:1604.04967v4 [cs.IT] UPDATED)</title>
    <link>http://arxiv.org/abs/1604.04967</link>
    <description rdf:parseType="Literal">&lt;p&gt;The paper suggests a frequency criterion of error-free recoverability of a
    missing value for sequences, i.e. discrete time processes, in a pathwise
    setting without probabilistic assumptions. The paper establishes error-free
    recoverability for classes of square-summable sequences with Z-transform
    vanishing at isolated points with a mild rate; the case of non-summable
    sequences is not excluded. The transfer functions for recovering algorithm are
    presented explicitly.
    &lt;/p&gt;
    &lt;p&gt;Some robustness with respect to noise contamination is established for the
    suggested recovering algorithm.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dokuchaev_N/0/1/0/all/0/1&quot;&gt;Nikolai Dokuchaev&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1607.02951">
    <title>Design Patterns in Beeping Algorithms: Examples, Emulation, and Analysis. (arXiv:1607.02951v4 [cs.DC] UPDATED)</title>
    <link>http://arxiv.org/abs/1607.02951</link>
    <description rdf:parseType="Literal">&lt;p&gt;We consider networks of processes which interact with beeps. In the basic
    model defined by Cornejo and Kuhn (2010), processes can choose in each round
    either to beep or to listen. Those who beep are unable to detect simultaneous
    beeps. Those who listen can only distinguish between silence and the presence
    of at least one beep. We refer to this model as $BL$ (beep or listen). Stronger
    models exist where the nodes can detect collision while they are beeping
    ($B_{cd}L$), listening ($BL_{cd}$), or both ($B_{cd}L_{cd}$). Beeping models
    are weak in essence and even simple tasks are difficult or unfeasible within.
    &lt;/p&gt;
    &lt;p&gt;We present a set of generic building blocks (design patterns) which seem to
    occur frequently in the design of beeping algorithms. They include multi-slot
    phases: the fact of dividing the main loop into a number of specialised slots;
    exclusive beeps: having a single node beep at a time in a neighbourhood (within
    one or two hops); adaptive probability: increasing or decreasing the
    probability of beeping to produce more exclusive beeps; internal (resp.
    peripheral) collision detection: for detecting collision while beeping (resp.
    listening). Based on these patterns, we provide algorithms for a number of
    basic problems, including colouring, 2-hop colouring, degree computation, 2-hop
    MIS, and collision detection (in $BL$). The patterns make it possible to
    formulate these algorithms in a rather concise and elegant way. Their analyses
    are more technical; one of them improves significantly upon that of the best
    known MIS algorithm by Jeavons et al. (2016). Finally, inspired by a technique
    from Afek et al. (2013), our last contribution is to show that any Las Vegas
    algorithm relying on collision detection can be transposed into a Monte Carlo
    algorithm without collision detection at the cost of a logarithmic slowdown,
    which we prove is optimal.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casteigts_A/0/1/0/all/0/1&quot;&gt;Arnaud Casteigts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metivier_Y/0/1/0/all/0/1&quot;&gt;Yves M&amp;#xe9;tivier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robson_J/0/1/0/all/0/1&quot;&gt;John Michael Robson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemmari_A/0/1/0/all/0/1&quot;&gt;Akka Zemmari&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1701.05408">
    <title>Ologisms. (arXiv:1701.05408v5 [cs.LO] UPDATED)</title>
    <link>http://arxiv.org/abs/1701.05408</link>
    <description rdf:parseType="Literal">&lt;p&gt;We introduce ologisms. They generate from ologs by extending their logical
    expressivity, from the possibility of considering constraints of equational
    nature only to the possibility of considering constraints of syllogistic
    nature, in addition. This is obtained by taking advantage of the peculiar
    features of an original diagrammatic logical calculus for the syllogistic, that
    make it well-behaved with respect to the design of ologs.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pagnan_R/0/1/0/all/0/1&quot;&gt;Ruggero Pagnan&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1703.03391">
    <title>First-order logic with incomplete information. (arXiv:1703.03391v12 [math.LO] UPDATED)</title>
    <link>http://arxiv.org/abs/1703.03391</link>
    <description rdf:parseType="Literal">&lt;p&gt;We develop first-order logic and some extensions for incomplete information
    scenarios and consider related complexity issues.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kuusisto_A/0/1/0/all/0/1&quot;&gt;Antti Kuusisto&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1703.08750">
    <title>Game-Theoretic Vaccination Against Networked SIS Epidemics and Impacts of Human Decision-Making. (arXiv:1703.08750v2 [cs.GT] UPDATED)</title>
    <link>http://arxiv.org/abs/1703.08750</link>
    <description rdf:parseType="Literal">&lt;p&gt;We study decentralized protection strategies against
    Susceptible-Infected-Susceptible (SIS) epidemics on networks. We consider a
    population game framework where nodes choose whether or not to vaccinate
    themselves, and the epidemic risk is defined as the infection probability at
    the endemic state of the epidemic under a degree-based mean-field
    approximation. Motivated by studies in behavioral economics showing that humans
    perceive probabilities and risks in a nonlinear fashion, we specifically
    examine the impacts of such misperceptions on the Nash equilibrium protection
    strategies. We first establish the existence and uniqueness of a threshold
    equilibrium where nodes with degrees larger than a certain threshold vaccinate.
    When the vaccination cost is sufficiently high, we show that behavioral biases
    cause fewer players to vaccinate, and vice versa. We quantify this effect for a
    class of networks with power-law degree distributions by proving tight bounds
    on the ratio of equilibrium thresholds under behavioral and true perceptions of
    probabilities. We further characterize the socially optimal vaccination policy
    and investigate the inefficiency of Nash equilibrium.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hota_A/0/1/0/all/0/1&quot;&gt;Ashish R. Hota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1&quot;&gt;Shreyas Sundaram&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1703.10593">
    <title>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. (arXiv:1703.10593v5 [cs.CV] UPDATED)</title>
    <link>http://arxiv.org/abs/1703.10593</link>
    <description rdf:parseType="Literal">&lt;p&gt;Image-to-image translation is a class of vision and graphics problems where
    the goal is to learn the mapping between an input image and an output image
    using a training set of aligned image pairs. However, for many tasks, paired
    training data will not be available. We present an approach for learning to
    translate an image from a source domain $X$ to a target domain $Y$ in the
    absence of paired examples. Our goal is to learn a mapping $G: X \rightarrow Y$
    such that the distribution of images from $G(X)$ is indistinguishable from the
    distribution $Y$ using an adversarial loss. Because this mapping is highly
    under-constrained, we couple it with an inverse mapping $F: Y \rightarrow X$
    and introduce a cycle consistency loss to push $F(G(X)) \approx X$ (and vice
    versa). Qualitative results are presented on several tasks where paired
    training data does not exist, including collection style transfer, object
    transfiguration, season transfer, photo enhancement, etc. Quantitative
    comparisons against several prior methods demonstrate the superiority of our
    approach.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1&quot;&gt;Taesung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1&quot;&gt;Phillip Isola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1704.04587">
    <title>Deep Learning for Photoacoustic Tomography from Sparse Data. (arXiv:1704.04587v3 [cs.CV] UPDATED)</title>
    <link>http://arxiv.org/abs/1704.04587</link>
    <description rdf:parseType="Literal">&lt;p&gt;The development of fast and accurate image reconstruction algorithms is a
    central aspect of computed tomography. In this paper, we investigate this issue
    for the sparse data problem in photoacoustic tomography (PAT). We develop a
    direct and highly efficient reconstruction algorithm based on deep learning. In
    our approach image reconstruction is performed with a deep convolutional neural
    network (CNN), whose weights are adjusted prior to the actual image
    reconstruction based on a set of training data. The proposed reconstruction
    approach can be interpreted as a network that uses the PAT filtered
    backprojection algorithm for the first layer, followed by the U-net
    architecture for the remaining layers. Actual image reconstruction with deep
    learning consists in one evaluation of the trained CNN, which does not require
    time consuming solution of the forward and adjoint problems. At the same time,
    our numerical results demonstrate that the proposed deep learning approach
    reconstructs images with a quality comparable to state of the art iterative
    approaches for PAT from sparse data.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antholzer_S/0/1/0/all/0/1&quot;&gt;Stephan Antholzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haltmeier_M/0/1/0/all/0/1&quot;&gt;Markus Haltmeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwab_J/0/1/0/all/0/1&quot;&gt;Johannes Schwab&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1705.04253">
    <title>Sketching Word Vectors Through Hashing. (arXiv:1705.04253v2 [cs.CL] UPDATED)</title>
    <link>http://arxiv.org/abs/1705.04253</link>
    <description rdf:parseType="Literal">&lt;p&gt;We propose a new fast word embedding technique using hash functions. The
    method is a derandomization of a new type of random projections: By
    disregarding the classic constraint used in designing random projections (i.e.,
    preserving pairwise distances in a particular normed space), our solution
    exploits extremely sparse non-negative random projections. Our experiments show
    that the proposed method can achieve competitive results, comparable to neural
    embedding learning techniques, however, with only a fraction of the
    computational complexity of these methods. While the proposed derandomization
    enhances the computational and space complexity of our method, the possibility
    of applying weighting methods such as positive pointwise mutual information
    (PPMI) to our models after their construction (and at a reduced dimensionality)
    imparts a high discriminatory power to the resulting embeddings. Obviously,
    this method comes with other known benefits of random projection-based
    techniques such as ease of update.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+QasemiZadeh_B/0/1/0/all/0/1&quot;&gt;Behrang QasemiZadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kallmeyer_L/0/1/0/all/0/1&quot;&gt;Laura Kallmeyer&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1706.02586">
    <title>DSOS and SDSOS Optimization: More Tractable Alternatives to Sum of Squares and Semidefinite Optimization. (arXiv:1706.02586v3 [math.OC] UPDATED)</title>
    <link>http://arxiv.org/abs/1706.02586</link>
    <description rdf:parseType="Literal">&lt;p&gt;In recent years, optimization theory has been greatly impacted by the advent
    of sum of squares (SOS) optimization. The reliance of this technique on
    large-scale semidefinite programs however, has limited the scale of problems to
    which it can be applied. In this paper, we introduce DSOS and SDSOS
    optimization as linear programming and second-order cone programming-based
    alternatives to sum of squares optimization that allow one to trade off
    computation time with solution quality. These are optimization problems over
    certain subsets of sum of squares polynomials (or equivalently subsets of
    positive semidefinite matrices), which can be of interest in general
    applications of semidefinite programming where scalability is a limitation. We
    show that some basic theorems from SOS optimization which rely on results from
    real algebraic geometry are still valid for DSOS and SDSOS optimization.
    Furthermore, we show with numerical experiments from diverse application
    areas---polynomial optimization, statistics and machine learning, derivative
    pricing, and control theory---that with reasonable tradeoffs in accuracy, we
    can handle problems at scales that are currently significantly beyond the reach
    of traditional sum of squares approaches. Finally, we provide a review of
    recent techniques that bridge the gap between our DSOS/SDSOS approach and the
    SOS approach at the expense of additional running time. The Supplementary
    Material of the paper introduces an accompanying MATLAB package for DSOS and
    SDSOS optimization.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ahmadi_A/0/1/0/all/0/1&quot;&gt;Amir Ali Ahmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Majumdar_A/0/1/0/all/0/1&quot;&gt;Anirudha Majumdar&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1706.09806">
    <title>Robust Face Tracking using Multiple Appearance Models and Graph Relational Learning. (arXiv:1706.09806v2 [cs.CV] UPDATED)</title>
    <link>http://arxiv.org/abs/1706.09806</link>
    <description rdf:parseType="Literal">&lt;p&gt;This paper addresses the problem of appearance matching across different
    challenges while doing visual face tracking in real-world scenarios. In this
    paper, FaceTrack is proposed that utilizes multiple appearance models with its
    long-term and short-term appearance memory for efficient face tracking. It
    demonstrates robustness to deformation, in-plane and out-of-plane rotation,
    scale, distractors and background clutter. It capitalizes on the advantages of
    the tracking-by-detection, by using a face detector that tackles drastic scale
    appearance change of a face. The detector also helps to reinitialize FaceTrack
    during drift. A weighted score-level fusion strategy is proposed to obtain the
    face tracking output having the highest fusion score by generating candidates
    around possible face locations. The tracker showcases impressive performance
    when initiated automatically by outperforming many state-of-the-art trackers,
    except Struck by a very minute margin: 0.001 in precision and 0.017 in success
    respectively.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakravorty_T/0/1/0/all/0/1&quot;&gt;Tanushri Chakravorty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1&quot;&gt;Guillaume-Alexandre Bilodeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1&quot;&gt;Eric Granger&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1707.02572">
    <title>Assortment Optimization under the Sequential Multinomial Logit Model. (arXiv:1707.02572v2 [cs.DM] UPDATED)</title>
    <link>http://arxiv.org/abs/1707.02572</link>
    <description rdf:parseType="Literal">&lt;p&gt;We study the assortment optimization problem under the Sequential Multinomial
    Logit (SML), a discrete choice model that generalizes the multinomial logit
    (MNL). Under the SML model, products are partitioned into two levels, to
    capture differences in attractiveness, brand awareness and, or visibility of
    the products in the market. When a consumer is presented with an assortment of
    products, she first considers products in the first level and, if none of them
    is purchased, products in the second level are considered. This model is a
    special case of the Perception-Adjusted Luce Model (PALM) recently proposed by
    Echenique et al (2018). It can explain many behavioural phenomena such as the
    attraction, compromise, similarity effects and choice overload which cannot be
    explained by the MNL model or any discrete choice model based on random
    utility. In particular, the SML model allows violations to regularity which
    states that the probability of choosing a product cannot increase if the offer
    set is enlarged.
    &lt;/p&gt;
    &lt;p&gt;This paper shows that the seminal concept of revenue-ordered assortment sets,
    which contain an optimal assortment under the MNL model, can be generalized to
    the SML model. More precisely, the paper proves that all optimal assortments
    under the SML are revenue-ordered by level, a natural generalization of
    revenue-ordered assortments that contains, at most, a quadratic number of
    assortments. As a corollary, assortment optimization under the SML is
    polynomial-time solvable. This result is particularly interesting given that
    the SML model does not satisfy the regularity condition and, therefore, it can
    explain choice behaviours that cannot be explained by any choice model based on
    random utility.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flores_A/0/1/0/all/0/1&quot;&gt;Alvaro Flores&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berbeglia_G/0/1/0/all/0/1&quot;&gt;Gerardo Berbeglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hentenryck_P/0/1/0/all/0/1&quot;&gt;Pascal van Hentenryck&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1707.02933">
    <title>802.11 Wireless Simulation and Anomaly Detection using HMM and UBM. (arXiv:1707.02933v2 [cs.NI] UPDATED)</title>
    <link>http://arxiv.org/abs/1707.02933</link>
    <description rdf:parseType="Literal">&lt;p&gt;Despite the growing popularity of 802.11 wireless networks, users often
    suffer from connectivity problems and performance issues due to unstable radio
    conditions and dynamic user behavior among other reasons. Anomaly detection and
    distinction are in the thick of major challenges that network managers
    encounter. Complication of monitoring the broaden and complex WLANs, that often
    requires heavy instrumentation of the user devices, makes the anomaly detection
    analysis even harder. In this paper we exploit 802.11 access point usage data
    and propose an anomaly detection technique based on Hidden Markov Model (HMM)
    and Universal Background Model (UBM) on data that is inexpensive to obtain. We
    then generate a number of network anomalous scenarios in OMNeT++/INET network
    simulator and compare the detection outcomes with those in baseline approaches
    (RawData and PCA). The experimental results show the superiority of HMM and
    HMM-UBM models in detection precision and sensitivity.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allahdadi_A/0/1/0/all/0/1&quot;&gt;Anisa Allahdadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morla_R/0/1/0/all/0/1&quot;&gt;Ricardo Morla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1&quot;&gt;Jaime S. Cardoso&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1707.07821">
    <title>Concept Drift Detection and Adaptation with Hierarchical Hypothesis Testing. (arXiv:1707.07821v4 [stat.ML] UPDATED)</title>
    <link>http://arxiv.org/abs/1707.07821</link>
    <description rdf:parseType="Literal">&lt;p&gt;When using statistical models (such as a classifier) in a streaming
    environment, there is often a need to detect and adapt to concept drifts to
    mitigate any deterioration in the model&apos;s predictive performance over time.
    Unfortunately, the ability of popular concept drift approaches in detecting
    these drifts in the relationship of the response and predictor variable is
    often dependent on the distribution characteristics of the data streams, as
    well as its sensitivity on parameter tuning. This paper presents Hierarchical
    Linear Four Rates (HLFR), a framework that detects concept drifts for different
    data stream distributions (including imbalanced data) by leveraging a
    hierarchical set of hypothesis tests in an online setting. The performance of
    HLFR is compared to benchmark approaches using both simulated and real-world
    datasets spanning the breadth of concept drift types. HLFR significantly
    outperforms benchmark approaches in terms of accuracy, G-mean, recall, delay in
    detection and adaptability across the various datasets.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shujian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abraham_Z/0/1/0/all/0/1&quot;&gt;Zubin Abraham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mohak Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+You_X/0/1/0/all/0/1&quot;&gt;Xinge You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Principe_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; C. Pr&amp;#xed;ncipe&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1709.05369">
    <title>Foundations of Complex Event Processing. (arXiv:1709.05369v2 [cs.DB] UPDATED)</title>
    <link>http://arxiv.org/abs/1709.05369</link>
    <description rdf:parseType="Literal">&lt;p&gt;Complex Event Processing (CEP) has emerged as the unifying field for
    technologies that require processing and correlating distributed data sources
    in real-time. CEP finds applications in diverse domains, which has resulted in
    a large number of proposals for expressing and processing complex events.
    However, existing CEP languages lack from a clear semantics, making them hard
    to understand and generalize. Moreover, there are no general techniques for
    evaluating CEP query languages with clear performance guarantees.
    &lt;/p&gt;
    &lt;p&gt;In this paper we embark on the task of giving a rigorous and efficient
    framework to CEP. We propose a formal language for specifying complex events,
    called CEL, that contains the main features used in the literature and has a
    denotational and compositional semantics. We also formalize the so-called
    selection strategies, which had only been presented as by-design extensions to
    existing frameworks. With a well-defined semantics at hand, we study how to
    efficiently evaluate CEL for processing complex events in the case of unary
    filters. We start by studying the syntactical properties of CEL and propose
    rewriting optimization techniques for simplifying the evaluation of formulas.
    Then, we introduce a formal computational model for CEP, called complex event
    automata (CEA), and study how to compile CEL formulas into CEA. Furthermore, we
    provide efficient algorithms for evaluating CEA over event streams using
    constant time per event followed by constant-delay enumeration of the results.
    By gathering these results together, we propose a framework for efficiently
    evaluating CEL with unary filters. Finally, we show experimentally that this
    framework consistently outperforms the competition, and even over trivial
    queries can be orders of magnitude more efficient.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bucchi_M/0/1/0/all/0/1&quot;&gt;Marco Bucchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grez_A/0/1/0/all/0/1&quot;&gt;Alejandro Grez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riveros_C/0/1/0/all/0/1&quot;&gt;Cristian Riveros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ugarte_M/0/1/0/all/0/1&quot;&gt;Mart&amp;#xed;n Ugarte&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1709.08294">
    <title>Learning Context-Sensitive Convolutional Filters for Text Processing. (arXiv:1709.08294v3 [cs.CL] UPDATED)</title>
    <link>http://arxiv.org/abs/1709.08294</link>
    <description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (CNNs) have recently emerged as a popular
    building block for natural language processing (NLP). Despite their success,
    most existing CNN models employed in NLP share the same learned (and static)
    set of filters for all input sentences. In this paper, we consider an approach
    of using a small meta network to learn context-sensitive convolutional filters
    for text processing. The role of meta network is to abstract the contextual
    information of a sentence or document into a set of input-aware filters. We
    further generalize this framework to model sentence pairs, where a
    bidirectional filter generation mechanism is introduced to encapsulate
    co-dependent sentence representations. In our benchmarks on four different
    tasks, including ontology classification, sentiment analysis, answer sentence
    selection, and paraphrase identification, our proposed model, a modified CNN
    with context-sensitive filters, consistently outperforms the standard CNN and
    attention-based CNN baselines. By visualizing the learned context-sensitive
    filters, we further validate and rationalize the effectiveness of proposed
    framework.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinghan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_M/0/1/0/all/0/1&quot;&gt;Martin Renqiang Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yitong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1&quot;&gt;Lawrence Carin&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1711.08571">
    <title>Calibrated Audio Steganalysis. (arXiv:1711.08571v2 [cs.MM] UPDATED)</title>
    <link>http://arxiv.org/abs/1711.08571</link>
    <description rdf:parseType="Literal">&lt;p&gt;Calibration is a common practice in image steganalysis for extracting
    prominent features. Based on the idea of reembedding, a new set of calibrated
    features for audio steganalysis applications are proposed. These features are
    extracted from a model that has maximum deviation from human auditory system
    and had been specifically designed for audio steganalysis. Ability of the
    proposed system is tested extensively. Simulations demonstrate that the
    proposed method can accurately detect the presence of hidden messages even in
    very low embedding rates. Proposed method achieves an accuracy of 99.3%
    (StegHide@0.76% BPB) which is 9.5% higher than the previous R-MFCC based
    steganalysis method.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghasemzadeh_H/0/1/0/all/0/1&quot;&gt;Hamzeh Ghasemzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kayvanrad_M/0/1/0/all/0/1&quot;&gt;Mohammad H. Kayvanrad&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1801.04403">
    <title>Social Advantage with Mixed Entangled States. (arXiv:1801.04403v2 [quant-ph] UPDATED)</title>
    <link>http://arxiv.org/abs/1801.04403</link>
    <description rdf:parseType="Literal">&lt;p&gt;It has been extensively shown in past literature that Bayesian Game Theory
    and Quantum Non-locality have strong ties between them. Pure Entangled States
    have been used, in both common and conflict interest games, to gain
    advantageous payoffs, both at the individual and social level. In this paper we
    construct a game for a Mixed Entangled State such that this state gives higher
    payoffs than classically possible, both at the individual level and the social
    level. Also, we use the I-3322 inequality so that states that aren&apos;t helpful as
    advice for Bell-CHSH inequality can also be used. Finally, the measurement
    setting we use is a Restricted Social Welfare Strategy (given this particular
    state).
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Aritra Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chowdhury_P/0/1/0/all/0/1&quot;&gt;Pratyusha Chowdhury&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1801.07939">
    <title>Deep Structured Energy-Based Image Inpainting. (arXiv:1801.07939v2 [cs.CV] UPDATED)</title>
    <link>http://arxiv.org/abs/1801.07939</link>
    <description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a structured image inpainting method employing an
    energy based model. In order to learn structural relationship between patterns
    observed in images and missing regions of the images, we employ an energy-based
    structured prediction method. The structural relationship is learned by
    minimizing an energy function which is defined by a simple convolutional neural
    network. The experimental results on various benchmark datasets show that our
    proposed method significantly outperforms the state-of-the-art methods which
    use Generative Adversarial Networks (GANs). We obtained 497.35 mean squared
    error (MSE) on the Olivetti face dataset compared to 833.0 MSE provided by the
    state-of-the-art method. Moreover, we obtained 28.4 dB peak signal to noise
    ratio (PSNR) on the SVHN dataset and 23.53 dB on the CelebA dataset, compared
    to 22.3 dB and 21.3 dB, provided by the state-of-the-art methods, respectively.
    The code is publicly available.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altinel_F/0/1/0/all/0/1&quot;&gt;Fazil Altinel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozay_M/0/1/0/all/0/1&quot;&gt;Mete Ozay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1&quot;&gt;Takayuki Okatani&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1802.00047">
    <title>Matrix completion with deterministic pattern - a geometric perspective. (arXiv:1802.00047v4 [cs.LG] UPDATED)</title>
    <link>http://arxiv.org/abs/1802.00047</link>
    <description rdf:parseType="Literal">&lt;p&gt;We consider the matrix completion problem with a deterministic pattern of
    observed entries. In this setting, we aim to answer the question: under what
    condition there will be (at least locally) unique solution to the matrix
    completion problem, i.e., the underlying true matrix is identifiable. We answer
    the question from a certain point of view and outline a geometric perspective.
    We give an algebraically verifiable sufficient condition, which we call the
    well-posedness condition, for the local uniqueness of MRMC solutions. We argue
    that this condition is necessary for local stability of MRMC solutions, and we
    show that the condition is generic using the characteristic rank. We also argue
    that the low-rank approximation approaches are more stable than MRMC and
    further propose a sequential statistical testing procedure to determine the
    &quot;true&quot; rank from observed entries. Finally, we provide numerical examples aimed
    at verifying validity of the presented theory.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapiro_A/0/1/0/all/0/1&quot;&gt;Alexander Shapiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1802.01355">
    <title>A Galois connection between Turing jumps and limits. (arXiv:1802.01355v3 [math.LO] UPDATED)</title>
    <link>http://arxiv.org/abs/1802.01355</link>
    <description rdf:parseType="Literal">&lt;p&gt;Limit computable functions can be characterized by Turing jumps on the input
    side or limits on the output side. As a monad of this pair of adjoint
    operations we obtain a problem that characterizes the low functions and dually
    to this another problem that characterizes the functions that are computable
    relative to the halting problem. Correspondingly, these two classes are the
    largest classes of functions that can be pre or post composed to limit
    computable functions without leaving the class of limit computable functions.
    We transfer these observations to the lattice of represented spaces where it
    leads to a formal Galois connection. We also formulate a version of this result
    for computable metric spaces. Limit computability and computability relative to
    the halting problem are notions that coincide for points and sequences, but
    even restricted to continuous functions the former class is strictly larger
    than the latter. On computable metric spaces we can characterize the functions
    that are computable relative to the halting problem as those functions that are
    limit computable with a modulus of continuity that is computable relative to
    the halting problem. As a consequence of this result we obtain, for instance,
    that Lipschitz continuous functions that are limit computable are automatically
    computable relative to the halting problem. We also discuss 1-generic points as
    the canonical points of continuity of limit computable functions, and we prove
    that restricted to these points limit computable functions are computable
    relative to the halting problem. Finally, we demonstrate how these results can
    be applied in computable analysis.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Brattka_V/0/1/0/all/0/1&quot;&gt;Vasco Brattka&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1802.05203">
    <title>Fully Convolutional Network Ensembles for White Matter Hyperintensities Segmentation in MR Images. (arXiv:1802.05203v2 [cs.CV] UPDATED)</title>
    <link>http://arxiv.org/abs/1802.05203</link>
    <description rdf:parseType="Literal">&lt;p&gt;White matter hyperintensities (WMH) are commonly found in the brains of
    healthy elderly individuals and have been associated with various neurological
    and geriatric disorders. In this paper, we present a study using deep fully
    convolutional network and ensemble models to automatically detect such WMH
    using fluid attenuation inversion recovery (FLAIR) and T1 magnetic resonance
    (MR) scans. The algorithm was evaluated and ranked 1 st in the WMH Segmentation
    Challenge at MICCAI 2017. In the evaluation stage, the implementation of the
    algorithm was submitted to the challenge organizers, who then independently
    tested it on a hidden set of 110 cases from 5 scanners. Averaged dice score,
    precision and robust Hausdorff distance obtained on held-out test datasets were
    80%, 84% and 6.30mm respectively. These were the highest achieved in the
    challenge, suggesting the proposed method is the state-of-the-art. In this
    paper, we provide detailed descriptions and quantitative analysis on key
    components of the system. Furthermore, a study of cross-scanner evaluation is
    presented to discuss how the combination of modalities and data augmentation
    affect the generalization capability of the system. The adaptability of the
    system to different scanners and protocols is also investigated. A quantitative
    study is further presented to test the effect of ensemble size. Additionally,
    software and models of our method are made publicly available. The
    effectiveness and generalization capability of the proposed system show its
    potential for real-world clinical practice.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1&quot;&gt;Gongfa Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruixuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianguo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaolei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wei-Shi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1&quot;&gt;Bjoern Menze&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1802.10238">
    <title>DeepSOFA: A Continuous Acuity Score for Critically Ill Patients using Clinically Interpretable Deep Learning. (arXiv:1802.10238v2 [cs.LG] UPDATED)</title>
    <link>http://arxiv.org/abs/1802.10238</link>
    <description rdf:parseType="Literal">&lt;p&gt;Traditional methods for assessing illness severity and predicting in-hospital
    mortality among critically ill patients require time-consuming, error-prone
    calculations using static variable thresholds. These methods do not capitalize
    on the emerging availability of streaming electronic health record data or
    capture time-sensitive individual physiological patterns, a critical task in
    the intensive care unit. We propose a novel acuity score framework (DeepSOFA)
    that leverages temporal measurements and interpretable deep learning models to
    assess illness severity at any point during an ICU stay. We compare DeepSOFA
    with SOFA (Sequential Organ Failure Assessment) baseline models using the same
    model inputs and find that at any point during an ICU admission, DeepSOFA
    yields significantly more accurate predictions of in-hospital mortality. A
    DeepSOFA model developed in a public database and validated in a single
    institutional cohort had a mean AUC for the entire ICU stay of 0.90 (95% CI
    0.90-0.91) compared with baseline SOFA models with mean AUC 0.79 (95% CI
    0.79-0.80) and 0.85 (95% CI 0.85-0.86). Deep models are well-suited to identify
    ICU patients in need of life-saving interventions prior to the occurrence of an
    unexpected adverse event and inform shared decision-making processes among
    patients, providers, and families regarding goals of care and optimal resource
    utilization.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shickel_B/0/1/0/all/0/1&quot;&gt;Benjamin Shickel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loftus_T/0/1/0/all/0/1&quot;&gt;Tyler J. Loftus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adhikari_L/0/1/0/all/0/1&quot;&gt;Lasith Adhikari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozrazgat_Baslanti_T/0/1/0/all/0/1&quot;&gt;Tezcan Ozrazgat-Baslanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bihorac_A/0/1/0/all/0/1&quot;&gt;Azra Bihorac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashidi_P/0/1/0/all/0/1&quot;&gt;Parisa Rashidi&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1803.04337">
    <title>Replication study: Development and validation of deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. (arXiv:1803.04337v3 [cs.CV] UPDATED)</title>
    <link>http://arxiv.org/abs/1803.04337</link>
    <description rdf:parseType="Literal">&lt;p&gt;Replication studies are essential for validation of new methods, and are
    crucial to maintain the high standards of scientific publications, and to use
    the results in practice. We have attempted to replicate the main method in
    &apos;Development and validation of a deep learning algorithm for detection of
    diabetic retinopathy in retinal fundus photographs&apos; published in JAMA 2016;
    316(22). We re-implemented the method since the source code is not available,
    and we used publicly available data sets. The original study used non-public
    fundus images from EyePACS and three hospitals in India for training. We used a
    different EyePACS data set from Kaggle. The original study used the benchmark
    data set Messidor-2 to evaluate the algorithm&apos;s performance. We used the same
    data set. In the original study, ophthalmologists re-graded all images for
    diabetic retinopathy, macular edema, and image gradability. There was one
    diabetic retinopathy grade per image for our data sets, and we assessed image
    gradability ourselves. Hyper-parameter settings were not described in the
    original study. But some of these were later published. We were not able to
    replicate the original study. Our algorithm&apos;s area under the receiver operating
    curve (AUC) of 0.94 on the Kaggle EyePACS test set and 0.80 on Messidor-2 did
    not come close to the reported AUC of 0.99 in the original study. This may be
    caused by the use of a single grade per image, different data, or different not
    described hyper-parameter settings. This study shows the challenges of
    replicating deep learning, and the need for more replication studies to
    validate deep learning methods, especially for medical image analysis.
    &lt;/p&gt;
    &lt;p&gt;Our source code and instructions are available at:
    https://github.com/mikevoets/jama16-retina-replication
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voets_M/0/1/0/all/0/1&quot;&gt;Mike Voets&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mollersen_K/0/1/0/all/0/1&quot;&gt;Kajsa M&amp;#xf8;llersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bongo_L/0/1/0/all/0/1&quot;&gt;Lars Ailo Bongo&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1803.07813">
    <title>Introducing higher order correlations to marginals&apos; subset of multivariate data by means of Archimedean copulas. (arXiv:1803.07813v2 [cs.DS] UPDATED)</title>
    <link>http://arxiv.org/abs/1803.07813</link>
    <description rdf:parseType="Literal">&lt;p&gt;In this paper, we present the algorithm that alters the subset of marginals
    of multivariate standard distributed data into such modelled by an Archimedean
    copula. Proposed algorithm leaves a correlation matrix almost unchanged, but
    introduces a higher order correlation into a subset of marginals. Our data
    transformation algorithm can be used to analyse whether particular machine
    learning algorithm, especially a dimensionality reduction one, utilises higher
    order correlations or not. We present an exemplary application on two features
    selection algorithms, mention that features selection is one of the approaches
    to dimensionality reduction. To measure higher order correlation, we use
    multivariate higher order cumulants, hence to utilises higher order
    correlations be to use the Joint Skewness Band Selection (JSBS) algorithm that
    uses third-order multivariate cumulant. We show the robust performance of the
    JSBS in contrary to the poor performance of the Maximum Ellipsoid Volume (MEV)
    algorithm that does not utilise such higher order correlations. With this
    result, we confirm the potential application of our data generation algorithm
    to analyse a performance of various dimensionality reduction algorithms.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Domino_K/0/1/0/all/0/1&quot;&gt;Krzysztof Domino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glos_A/0/1/0/all/0/1&quot;&gt;Adam Glos&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1803.11475">
    <title>Statistical Non-linear Model, Achievable Rates and Signal Detection for Photon-level Photomultiplier Receiver. (arXiv:1803.11475v2 [cs.IT] UPDATED)</title>
    <link>http://arxiv.org/abs/1803.11475</link>
    <description rdf:parseType="Literal">&lt;p&gt;We characterize the practical receiver in a wide range of signal intensity
    for optical wireless communication, from discrete pulse regime to continuous
    waveform regime. We first propose a statistical non-linear model based on the
    photomultiplier tube (PMT) multi-stage amplification and Poisson channel, and
    then derive the optimal and tractable suboptimal duty cycle with peak-power and
    average-power constraints for on-off key (OOK) modulation in linear regime.
    Subsequently, a threshold-based classifier is proposed to distinguish the PMT
    working regimes based on the non-linear model. Moreover, we derive the
    approximate performance of mean power detection with infinite sampling rate and
    finite over-sampling rate in the linear regime based on small dead time and
    central-limit theorem. We also fomulate a signal model in the non-linar regime.
    Furthermore, the performance of mean power detection and photon counting
    detection with maximum likelihood (ML) detection for different sampling rates
    is evaluated from both theoretical and numerical perspectives. We can conclude
    that the sample interval equivalent to dead time is a good choice, and lower
    sampling rate would significantly degrade the performance.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhimeng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chen Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Xu&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1804.01861">
    <title>A Markov Model of Slice Admission Control. (arXiv:1804.01861v5 [cs.NI] UPDATED)</title>
    <link>http://arxiv.org/abs/1804.01861</link>
    <description rdf:parseType="Literal">&lt;p&gt;The emerging feature of network slicing in future Fifth Generation (5G)
    networks calls for efficient slice management. Recent studies have been
    focusing on the mechanism of slice admission control, which functions in a
    manner of state machine. This paper proposes a general state model for
    synchronous slice admission control, and proves it to be Markovian under a set
    of weak constraints. An analytical approximation of the state transition matrix
    to reduce computational complexity in practical applications is also proposed
    and evaluated.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1&quot;&gt;Di Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schotten_H/0/1/0/all/0/1&quot;&gt;Hans D. Schotten&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1804.01932">
    <title>Density estimation on small datasets. (arXiv:1804.01932v4 [physics.data-an] UPDATED)</title>
    <link>http://arxiv.org/abs/1804.01932</link>
    <description rdf:parseType="Literal">&lt;p&gt;How might a smooth probability distribution be estimated, with accurately
    quantified uncertainty, from a limited amount of sampled data? Here we describe
    a field-theoretic approach that addresses this problem remarkably well in one
    dimension, providing an exact nonparametric Bayesian posterior without relying
    on tunable parameters or large-data approximations. Strong non-Gaussian
    constraints, which require a non-perturbative treatment, are found to play a
    major role in reducing distribution uncertainty. A software implementation of
    this method is provided.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei-Chia Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Tareen_A/0/1/0/all/0/1&quot;&gt;Ammar Tareen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kinney_J/0/1/0/all/0/1&quot;&gt;Justin B. Kinney&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1804.02960">
    <title>Analysis and development of a novel algorithm for the in-vehicle hand-usage of a smartphone. (arXiv:1804.02960v2 [cs.HC] UPDATED)</title>
    <link>http://arxiv.org/abs/1804.02960</link>
    <description rdf:parseType="Literal">&lt;p&gt;Smartphone usage while driving is unanimously considered to be a really
    dangerous habit due to strong correlation with road accidents. In this paper,
    the problem of detecting whether the driver is using the phone during a trip is
    addressed. To do this, high-frequency data from the triaxial inertial
    measurement unit (IMU) integrated in almost all modern phone is processed
    without relying on external inputs so as to provide a self-contained approach.
    By resorting to a frequency-domain analysis, it is possible to extract from the
    raw signals the useful information needed to detect when the driver is using
    the phone, without being affected by the effects that vehicle motion has on the
    same signals. The selected features are used to train a Support Vector Machine
    (SVM) algorithm. The performance of the proposed approach are analyzed and
    tested on experimental data collected during mixed naturalistic driving
    scenarios, proving the effectiveness of the proposed approach.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gelmini_S/0/1/0/all/0/1&quot;&gt;Simone Gelmini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strada_S/0/1/0/all/0/1&quot;&gt;Silvia Strada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanelli_M/0/1/0/all/0/1&quot;&gt;Mara Tanelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savaresi_S/0/1/0/all/0/1&quot;&gt;Sergio Savaresi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biase_V/0/1/0/all/0/1&quot;&gt;Vincenzo Biase&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1804.05484">
    <title>Block Mean Approximation for Efficient Second Order Optimization. (arXiv:1804.05484v3 [cs.LG] UPDATED)</title>
    <link>http://arxiv.org/abs/1804.05484</link>
    <description rdf:parseType="Literal">&lt;p&gt;Advanced optimization algorithms such as Newton method and AdaGrad benefit
    from second order derivative or second order statistics to achieve better
    descent directions and faster convergence rates. At their heart, such
    algorithms need to compute the inverse or inverse square root of a matrix whose
    size is quadratic of the dimensionality of the search space. For high
    dimensional search spaces, the matrix inversion or inversion of square root
    becomes overwhelming which in turn demands for approximate methods. In this
    work, we propose a new matrix approximation method which divides a matrix into
    blocks and represents each block by one or two numbers. The method allows
    efficient computation of matrix inverse and inverse square root. We apply our
    method to AdaGrad in training deep neural networks. Experiments show
    encouraging results compared to the diagonal approximation.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1&quot;&gt;Mehrtash Harandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1&quot;&gt;Richard Hartley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1804.07873">
    <title>3D Human Pose Estimation on a Configurable Bed from a Pressure Image. (arXiv:1804.07873v2 [cs.RO] UPDATED)</title>
    <link>http://arxiv.org/abs/1804.07873</link>
    <description rdf:parseType="Literal">&lt;p&gt;Robots have the potential to assist people in bed, such as in healthcare
    settings, yet bedding materials like sheets and blankets can make observation
    of the human body difficult for robots. A pressure-sensing mat on a bed can
    provide pressure images that are relatively insensitive to bedding materials.
    However, prior work on estimating human pose from pressure images has been
    restricted to 2D pose estimates and flat beds. In this work, we present two
    convolutional neural networks to estimate the 3D joint positions of a person in
    a configurable bed from a single pressure image. The first network directly
    outputs 3D joint positions, while the second outputs a kinematic model that
    includes estimated joint angles and limb lengths. We evaluated our networks on
    data from 17 human participants with two bed configurations: supine and seated.
    Our networks achieved a mean joint position error of 77 mm when tested with
    data from people outside the training set, outperforming several baselines. We
    also present a simple mechanical model that provides insight into ambiguity
    associated with limbs raised off of the pressure mat, and demonstrate that
    Monte Carlo dropout can be used to estimate pose confidence in these
    situations. Finally, we provide a demonstration in which a mobile manipulator
    uses our network&apos;s estimated kinematic model to reach a location on a person&apos;s
    body in spite of the person being seated in a bed and covered by a blanket.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clever_H/0/1/0/all/0/1&quot;&gt;Henry M. Clever&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapusta_A/0/1/0/all/0/1&quot;&gt;Ariel Kapusta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1&quot;&gt;Daehyung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erickson_Z/0/1/0/all/0/1&quot;&gt;Zackory Erickson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chitalia_Y/0/1/0/all/0/1&quot;&gt;Yash Chitalia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kemp_C/0/1/0/all/0/1&quot;&gt;Charles C. Kemp&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1805.02530">
    <title>Near-drowning Early Prediction Technique Using Novel Equations (NEPTUNE) for Swimming Pools. (arXiv:1805.02530v3 [cs.CV] UPDATED)</title>
    <link>http://arxiv.org/abs/1805.02530</link>
    <description rdf:parseType="Literal">&lt;p&gt;Safety is a critical aspect in all swimming pools. This paper describes a
    near drowning early prediction technique using novel equations (NEPTUNE).
    NEPTUNE uses equations or rules that would be able to detect near drowning
    using at least 1 but not more than 5 seconds of video sequence with no false
    positives. The backbone of NEPTUNE encompasses a mix of statistical image
    processing to merge images for a video sequence followed by K means clustering
    to extract segments in the merged image and finally a revisit to statistical
    image processing to derive variables for every segment. These variables would
    be used by the equations to identify near-drowning. NEPTUNE has the potential
    to be integrated into a swimming pool camera system that would send an alarm to
    the lifeguards for early response so that the likelihood of recovery is high.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_B/0/1/0/all/0/1&quot;&gt;Bhaskaran David Prakash&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1805.02836">
    <title>Continuous-time Opinion Dynamics on Multiple Interdependent Topics. (arXiv:1805.02836v2 [cs.SI] UPDATED)</title>
    <link>http://arxiv.org/abs/1805.02836</link>
    <description rdf:parseType="Literal">&lt;p&gt;In this paper, and inspired by the recent discrete-time based works of [1,2],
    we study a continuous-time opinion dynamics model where the individuals discuss
    opinions on multiple logically interdependent topics. The logical
    interdependence between the different topics is captured by a matrix, which is
    distinct from the Laplacian matrix capturing interactions between individuals.
    We obtain a necessary and sufficient condition for the network to reach to a
    consensus on each separate topic. The condition involves both the
    interdependence matrix and Laplacian matrix together, and is thus distinct from
    the result of [1], where in the absence of stubborn individuals, separate
    regularity of the interdependence matrix and influence matrix (the
    discrete-time version of the Laplacian) is enough to ensure a consensus of
    opinions. Assuming that the interdependence matrix is fixed, we generate two
    sufficient conditions on the network, i.e. the Laplacian matrix, to ensure a
    consensus of opinions. For a class of interdependence matrices, we also
    establish the set of Laplacian matrices which guarantee consensus. The model is
    then expanded to include stubborn individuals, who remain attached to their
    initial opinions. Sufficient conditions are obtained for guaranteeing stability
    of the opinion dynamics system, with the final opinions being at a persistent
    disagreement as opposed to having reached a consensus. Simulations are provided
    to illustrate the results.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Mengbin Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trinh_M/0/1/0/all/0/1&quot;&gt;Minh Hoang Trinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_Y/0/1/0/all/0/1&quot;&gt;Young-Hun Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1&quot;&gt;Brian D.O. Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_H/0/1/0/all/0/1&quot;&gt;Hyo-Sung Ahn&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1805.04182">
    <title>On the tightest interval-valued state estimator for linear systems. (arXiv:1805.04182v3 [cs.SY] UPDATED)</title>
    <link>http://arxiv.org/abs/1805.04182</link>
    <description rdf:parseType="Literal">&lt;p&gt;This paper discusses an interval-valued state estimator for linear dynamic
    systems. In particular, we derive an expression of the tightest possible
    interval estimator in the sense that it is the intersection of all
    interval-valued estimators. This estimator appears, in a general setting, to be
    an infinite dimensional dynamic system. Therefore, practical implementation
    requires some over-approximations which would yield a good trade-off between
    computational complexity and tightness.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bako_L/0/1/0/all/0/1&quot;&gt;Laurent Bako&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrieu_V/0/1/0/all/0/1&quot;&gt;Vincent Andrieu&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1805.08716">
    <title>Reducing Disparate Exposure in Ranking: A Learning To Rank Approach. (arXiv:1805.08716v2 [cs.IR] UPDATED)</title>
    <link>http://arxiv.org/abs/1805.08716</link>
    <description rdf:parseType="Literal">&lt;p&gt;In this paper we consider a ranking problem in which we would like to order a
    set of items by utility or relevance, while also considering the visibility of
    different groups of items. To solve this problem, we adopt a supervised
    learning to rank approach that learns a ranking function from a set of training
    examples, which are queries and ranked lists of documents for each query. We
    consider that the elements to be ranked are divided into two groups: protected
    and non-protected. Following long-standing empirical observations showing that
    users of information retrieval systems rarely look past the first few results,
    we consider that some items receive more exposure than others. Our objective is
    to produce a ranker that is able to reproduce the ordering of the training set,
    which is the standard objective in learning to rank, but that additionally
    gives protected elements sufficient exposure, compared to non-protected
    elements. We demonstrate how to describe this objective formally, how to
    achieve it effectively and implement it, and present an experimental study
    describing how large differences in exposure can be reduced without having to
    introduce large distortions in the ranking utility.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zehlike_M/0/1/0/all/0/1&quot;&gt;Meike Zehlike&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castillo_C/0/1/0/all/0/1&quot;&gt;Carlos Castillo&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1805.09190">
    <title>Towards the first adversarially robust neural network model on MNIST. (arXiv:1805.09190v2 [cs.CV] UPDATED)</title>
    <link>http://arxiv.org/abs/1805.09190</link>
    <description rdf:parseType="Literal">&lt;p&gt;Despite much effort, deep neural networks remain highly susceptible to tiny
    input perturbations and even for MNIST, one of the most common toy datasets in
    computer vision, no neural network model exists for which the adversarial
    perturbations are large and make semantic sense to humans. We show that the
    widely recognized and by far most successful defense by Madry et al. (1)
    overfits on the L-infinity metric (it&apos;s highly susceptibility to L2 and L0
    perturbations), (2) a simple defense based on binarization performs almost as
    well and (3) its adversarial perturbations make little sense to humans. These
    results suggest that MNIST is far from being solved in terms of adversarial
    robustness. We present a novel approach that performs analysis by synthesis
    using learned class-conditional data distributions. We go to great length to
    empirically evaluate our model using maximally effective adversarial attacks by
    (a) applying decision-based, score-based, gradient-based and transfer-based
    attacks for several different Lp norms, (b) by designing a new attack that
    exploits the structure of our defended model and (c) by devising a novel
    decision-based attack that seeks to minimize the number of perturbed pixels
    (L0). The results suggest that this approach yields state-of-the-art robustness
    on MNIST against L0, L2 and L-infinity perturbations and we demonstrate that
    most adversarial examples are strongly perturbed towards the perceptual
    boundary between the original and the adversarial class.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schott_L/0/1/0/all/0/1&quot;&gt;Lukas Schott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rauber_J/0/1/0/all/0/1&quot;&gt;Jonas Rauber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1&quot;&gt;Matthias Bethge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1&quot;&gt;Wieland Brendel&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1805.11898">
    <title>Capacity bounds for bandlimited Gaussian channels with peak-to-average-power-ratio constraint. (arXiv:1805.11898v2 [cs.IT] UPDATED)</title>
    <link>http://arxiv.org/abs/1805.11898</link>
    <description rdf:parseType="Literal">&lt;p&gt;We revisit Shannon&apos;s problem of bounding the capacity of bandlimited Gaussian
    channel (BLGC) with peak power constraint, and extend the problem to the
    peak-to-average-power-ratio (PAPR) constrained case. By lower bounding the
    achievable information rate of pulse amplitude modulation with independent and
    identically distributed input under a PAPR constraint, we obtain a general
    capacity lower bound with respect to the shaping pulse. We then evaluate and
    optimize the lower bound by employing some parametric pulses, thereby improving
    the best existing result. Following Shannon&apos;s approach, capacity upper bound
    for PAPR constrained BLGC is also obtained. By combining our upper and lower
    bounds, the capacity of PAPR constrained BLGC is bounded to within a finite gap
    which tends to zero as the PAPR constraint tends to infinity. Using the same
    approach, we also improve existing capacity lower bounds for bandlimited
    optical intensity channel at high SNR.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jing Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenyi Zhang&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1806.01344">
    <title>Data-Driven Participation Factors for Nonlinear Systems Based on Koopman Mode Decomposition. (arXiv:1806.01344v4 [cs.SY] UPDATED)</title>
    <link>http://arxiv.org/abs/1806.01344</link>
    <description rdf:parseType="Literal">&lt;p&gt;This paper develops a novel data-driven technique to compute the
    participation factors for nonlinear systems based on the Koopman mode
    decomposition. Provided that certain conditions are satisfied, it is shown that
    the proposed technique generalizes the original definition of the linear
    mode-in-state participation factors. Two numerical examples are provided to
    demonstrate the performance of our approach: one relying on a canonical
    nonlinear dynamical system, and the other based on the two-area four-machine
    power system. The Koopman mode decomposition is capable of coping with a large
    class of nonlinearity, thereby making our technique able to deal with
    oscillations arising in practice due to nonlinearities while being fast to
    compute and compatible with real-time applications.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Netto_M/0/1/0/all/0/1&quot;&gt;Marcos Netto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Susuki_Y/0/1/0/all/0/1&quot;&gt;Yoshihiko Susuki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mili_L/0/1/0/all/0/1&quot;&gt;Lamine Mili&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1806.02059">
    <title>Problems and solutions of the Fourth International Students&apos; Olympiad in Cryptography NSUCRYPTO. (arXiv:1806.02059v3 [cs.CR] UPDATED)</title>
    <link>http://arxiv.org/abs/1806.02059</link>
    <description rdf:parseType="Literal">&lt;p&gt;Mathematical problems and their solutions of the Fourth International
    Students&apos; Olympiad in cryptography NSUCRYPTO&apos;2017 are presented. We consider
    problems related to attacks on ciphers and hash functions, cryptographic
    Boolean functions, the linear branch number, addition chains, error correction
    codes, etc. We discuss several open problems on algebraic structure of
    cryptographic functions, useful proof-of-work algorithms, the Boolean hidden
    shift problem and quantum computings.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorodilova_A/0/1/0/all/0/1&quot;&gt;Anastasiya Gorodilova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agievich_S/0/1/0/all/0/1&quot;&gt;Sergey Agievich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlet_C/0/1/0/all/0/1&quot;&gt;Claude Carlet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorkunov_E/0/1/0/all/0/1&quot;&gt;Evgeny Gorkunov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Idrisova_V/0/1/0/all/0/1&quot;&gt;Valeriya Idrisova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolomeec_N/0/1/0/all/0/1&quot;&gt;Nikolay Kolomeec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kutsenko_A/0/1/0/all/0/1&quot;&gt;Alexandr Kutsenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikova_S/0/1/0/all/0/1&quot;&gt;Svetla Nikova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oblaukhov_A/0/1/0/all/0/1&quot;&gt;Alexey Oblaukhov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1&quot;&gt;Stjepan Picek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preneel_B/0/1/0/all/0/1&quot;&gt;Bart Preneel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rijmen_V/0/1/0/all/0/1&quot;&gt;Vincent Rijmen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tokareva_N/0/1/0/all/0/1&quot;&gt;Natalia Tokareva&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1806.02711">
    <title>POTs: Protective Optimization Technologies. (arXiv:1806.02711v3 [cs.CY] UPDATED)</title>
    <link>http://arxiv.org/abs/1806.02711</link>
    <description rdf:parseType="Literal">&lt;p&gt;In spite of their many advantages, optimization systems often neglect the
    economic, ethical, moral, social, and political impact they have on populations
    and their environments. In this paper we argue that the frameworks through
    which the discontents of optimization systems have been approached so far cover
    a narrow subset of these problems by (i) assuming that the system provider has
    the incentives and means to mitigate the imbalances optimization causes, (ii)
    disregarding problems that go beyond discrimination due to disparate treatment
    or impact in algorithmic decision making, and (iii) developing solutions
    focused on removing algorithmic biases related to discrimination.
    &lt;/p&gt;
    &lt;p&gt;In response we introduce Protective Optimization Technologies: solutions that
    enable optimization subjects to defend from unwanted consequences. We provide a
    framework that formalizes the design space of POTs and show how it differs from
    other design paradigms in the literature. We show how the framework can capture
    strategies developed in the wild against real optimization systems, and how it
    can be used to design, implement, and evaluate a POT that enables individuals
    and collectives to protect themselves from unbalances in a credit scoring
    application related to loan allocation.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Overdorf_R/0/1/0/all/0/1&quot;&gt;Rebekah Overdorf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulynych_B/0/1/0/all/0/1&quot;&gt;Bogdan Kulynych&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balsa_E/0/1/0/all/0/1&quot;&gt;Ero Balsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Troncoso_C/0/1/0/all/0/1&quot;&gt;Carmela Troncoso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurses_S/0/1/0/all/0/1&quot;&gt;Seda G&amp;#xfc;rses&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1806.02908">
    <title>Is preprocessing of text really worth your time for online comment classification?. (arXiv:1806.02908v2 [cs.CL] UPDATED)</title>
    <link>http://arxiv.org/abs/1806.02908</link>
    <description rdf:parseType="Literal">&lt;p&gt;A large proportion of online comments present on public domains are
    constructive, however a significant proportion are toxic in nature. The
    comments contain lot of typos which increases the number of features manifold,
    making the ML model difficult to train. Considering the fact that the data
    scientists spend approximately 80% of their time in collecting, cleaning and
    organizing their data [1], we explored how much effort should we invest in the
    preprocessing (transformation) of raw comments before feeding it to the
    state-of-the-art classification models. With the help of four models on Jigsaw
    toxic comment classification data, we demonstrated that the training of model
    without any transformation produce relatively decent model. Applying even basic
    transformations, in some cases, lead to worse performance and should be applied
    with caution.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammad_F/0/1/0/all/0/1&quot;&gt;Fahim Mohammad&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1806.06759">
    <title>Lambda-calculus and Reversible Automatic Combinators. (arXiv:1806.06759v2 [cs.LO] UPDATED)</title>
    <link>http://arxiv.org/abs/1806.06759</link>
    <description rdf:parseType="Literal">&lt;p&gt;In 2005, Abramsky introduced various linear/affine combinatory algebras of
    partial involutions over a suitable formal language, to discuss reversible
    computation in a game-theoretic setting. These algebras arise as instances of
    the general paradigm explored by Haghverdi (Abramsky&apos;s Programme), which
    amounts to defining a lambda-algebra starting from a GoI Situation in a traced
    symmetric monoidal category. We investigate this construction from the point of
    view of the model theory of lambda-calculus. We focus on the strictly linear
    and affine parts of Abramsky&apos;s Affine Combinatory Algebras, sketching how to
    encompass the full algebra. The gist of our approach is that the GoI
    interpretation of a term based on involutions is dual to the principal type of
    the term, w.r.t. the type discipline for a linear/affine lambda-calculus. In
    the general case the type discipline and the calculus need to be extended,
    resp., with intersection, !-types, and !-abstractions. Our analysis unveils
    three conceptually independent, but ultimately equivalent, accounts of
    application in the lambda-calculus: beta-reduction, the GoI application of
    involutions based on symmetric feedback (Girard&apos;s Execution Formula), and
    unification of principal types. Thus we provide an answer, in the strictly
    affine case, to the question raised in [1] of characterising the partial
    involutions arising from bi-orthogonal pattern matching automata, which are
    denotations of affine combinators, and we point to the answer to the full
    question. Furthermore, we prove that the strictly linear combinatory algebra of
    partial involutions is a strictly linear lambda-algebra, albeit not a
    combinatory model, while both the strictly affine combinatory algebra and the
    full affine combinatory algebra are not. To check all the equations involved in
    the definition of affine lambda-algebra, we implement in Erlang application of
    involutions.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ciaffaglione_A/0/1/0/all/0/1&quot;&gt;Alberto Ciaffaglione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honsell_F/0/1/0/all/0/1&quot;&gt;Furio Honsell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lenisa_M/0/1/0/all/0/1&quot;&gt;Marina Lenisa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scagnetto_I/0/1/0/all/0/1&quot;&gt;Ivan Scagnetto&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1806.06827">
    <title>PAC-Bayes bounds for stable algorithms with instance-dependent priors. (arXiv:1806.06827v2 [stat.ML] UPDATED)</title>
    <link>http://arxiv.org/abs/1806.06827</link>
    <description rdf:parseType="Literal">&lt;p&gt;PAC-Bayes bounds have been proposed to get risk estimates based on a training
    sample. In this paper the PAC-Bayes approach is combined with stability of the
    hypothesis learned by a Hilbert space valued algorithm. The PAC-Bayes setting
    is used with a Gaussian prior centered at the expected output. Thus a novelty
    of our paper is using priors defined in terms of the data-generating
    distribution. Our main result estimates the risk of the randomized algorithm in
    terms of the hypothesis stability coefficients. We also provide a new bound for
    the SVM classifier, which is compared to other known bounds experimentally.
    Ours appears to be the first stability-based bound that evaluates to
    non-trivial values.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rivasplata_O/0/1/0/all/0/1&quot;&gt;Omar Rivasplata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Parrado_Hernandez_E/0/1/0/all/0/1&quot;&gt;Emilio Parrado-Hernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shawe_Taylor_J/0/1/0/all/0/1&quot;&gt;John Shawe-Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shiliang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Szepesvari_C/0/1/0/all/0/1&quot;&gt;Csaba Szepesvari&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1806.08206">
    <title>Proving Linearizability Using Reduction. (arXiv:1806.08206v3 [cs.PL] UPDATED)</title>
    <link>http://arxiv.org/abs/1806.08206</link>
    <description rdf:parseType="Literal">&lt;p&gt;Lipton&apos;s reduction theory provides an intuitive and simple way for deducing
    the non-interference properties of concurrent programs, but it is difficult to
    directly apply the technique to verify linearizability of sophisticated
    fine-grained concurrent data structures. In this paper, we propose three
    reduction-based proof methods that can handle such data structures. The key
    idea behind our reduction methods is that an irreducible operation can be
    viewed as an atomic operation at a higher level of abstraction. This allows us
    to focus on the reduction properties of an operation related to its abstract
    semantics. We have successfully applied the methods to verify 11 concurrent
    data structures including the most challenging ones: the Herlihy and Wing
    queue, the HSY elimination-based stack, and the time-stamped queue, and the
    lazy list. Our methods inherit intuition and simplicity of Lipton&apos;s reduction,
    and concurrent data structures designers can easily and quickly learn to use
    the methods.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_T/0/1/0/all/0/1&quot;&gt;Tangliu Wen&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1806.09246">
    <title>Generalized Sub-Array-Connected Hybrid Precoding Improves the Energy-Efficiency of Millimeter-Wave Massive MIMO Systems. (arXiv:1806.09246v3 [cs.IT] UPDATED)</title>
    <link>http://arxiv.org/abs/1806.09246</link>
    <description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a generalized sub-array-connected architecture for
    arbitrary radio frequency (RF) chain and antenna configurations, where the
    number of RF chains connected to a sub-array and the number of antennas in each
    sub-array can be arbitrary. Our design objective is to improve the energy
    efficiency of the hybrid precoder of millimeter-wave massive multiple input
    multiple output (MIMO) systems. We also propose a successive interference
    cancellation based hybrid precoding scheme for any given RF chain and antenna
    configuration. This scheme firstly decomposes the total achievable rate
    optimization problem into multiple sub-rate optimization problems, each of
    which is only related to a single sub-array and then it successively maximizes
    these sub-rates. Our simulation results demonstrate that the proposed scheme
    achieves a similar rate as the corresponding optimal unconstrained precoding
    scheme. Furthermore, we show that the energy-efficiency of the proposed scheme
    is better than that of the existing schemes in the fully-connected and
    sub-array-connected architectures.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Da Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanzo_L/0/1/0/all/0/1&quot;&gt;Lajos Hanzo&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1807.00661">
    <title>Shallow Types for Insightful Programs: Grace is Optional, Performance is Not. (arXiv:1807.00661v2 [cs.PL] UPDATED)</title>
    <link>http://arxiv.org/abs/1807.00661</link>
    <description rdf:parseType="Literal">&lt;p&gt;Languages with explicit dynamic type checking are increasing in popularity in
    both practical development and programming education. Unfortunately, current
    implementations of these languages perform worse than either purely statically
    or purely dynamically typed languages. We show how virtual machines can use
    common optimizations to remove redundancy in dynamic type checking, by adding
    shallow structural type checks to Moth, a Truffle-based interpreter for Grace.
    Moth runs programs with dynamic type checks roughly as fast as programs without
    checks, so developers do not need to disable checks in production code, and
    educators can teach types without also teaching that types slow programs down.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_R/0/1/0/all/0/1&quot;&gt;Richard Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marr_S/0/1/0/all/0/1&quot;&gt;Stefan Marr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Homer_M/0/1/0/all/0/1&quot;&gt;Michael Homer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1&quot;&gt;James Noble&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1807.05153">
    <title>Multi-Scale Convolutional-Stack Aggregation for Robust White Matter Hyperintensities Segmentation. (arXiv:1807.05153v2 [cs.CV] UPDATED)</title>
    <link>http://arxiv.org/abs/1807.05153</link>
    <description rdf:parseType="Literal">&lt;p&gt;Segmentation of both large and small white matter hyperintensities/lesions in
    brain MR images is a challenging task which has drawn much attention in recent
    years. We propose a multi-scale aggregation model framework to deal with
    volume-varied lesions. Firstly, we present a specifically-designed network for
    small lesion segmentation called Stack-Net, in which multiple convolutional
    layers are connected, aiming to preserve rich local spatial information of
    small lesions before the sub-sampling layer. Secondly, we aggregate multi-scale
    Stack-Nets with different receptive fields to learn multi-scale contextual
    information of both large and small lesions. Our model is evaluated on recent
    MICCAI WMH Challenge Dataset and outperforms the state-of-the-art on lesion
    recall and lesion F1-score under 5-fold cross validation. In addition, we
    further test our pre-trained models on a Multiple Sclerosis lesion dataset with
    30 subjects under cross-center evaluation. Results show that the aggregation
    model is effective in learning multi-scale spatial information.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianguo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muehlau_M/0/1/0/all/0/1&quot;&gt;Mark Muehlau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirschke_J/0/1/0/all/0/1&quot;&gt;Jan Kirschke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1&quot;&gt;Bjoern Menze&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.02129">
    <title>Probabilistic Causal Analysis of Social Influence. (arXiv:1808.02129v2 [cs.SI] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.02129</link>
    <description rdf:parseType="Literal">&lt;p&gt;Mastering the dynamics of social influence requires separating, in a database
    of information propagation traces, the genuine causal processes from temporal
    correlation, i.e., homophily and other spurious causes. However, most studies
    to characterize social influence, and, in general, most data-science analyses
    focus on correlations, statistical independence, or conditional independence.
    Only recently, there has been a resurgence of interest in &quot;causal data
    science&quot;, e.g., grounded on causality theories. In this paper we adopt a
    principled causal approach to the analysis of social influence from
    information-propagation data, rooted in the theory of probabilistic causation.
    &lt;/p&gt;
    &lt;p&gt;Our approach consists of two phases. In the first one, in order to avoid the
    pitfalls of misinterpreting causation when the data spans a mixture of several
    subtypes (&quot;Simpson&apos;s paradox&quot;), we partition the set of propagation traces into
    groups, in such a way that each group is as less contradictory as possible in
    terms of the hierarchical structure of information propagation. To achieve this
    goal, we borrow the notion of &quot;agony&quot; and define the Agony-bounded Partitioning
    problem, which we prove being hard, and for which we develop two efficient
    algorithms with approximation guarantees. In the second phase, for each group
    from the first phase, we apply a constrained MLE approach to ultimately learn a
    minimal causal topology. Experiments on synthetic data show that our method is
    able to retrieve the genuine causal arcs w.r.t. a ground-truth generative
    model. Experiments on real data show that, by focusing only on the extracted
    causal structures instead of the whole social graph, the effectiveness of
    predicting influence spread is significantly improved.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1&quot;&gt;Francesco Bonchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gullo_F/0/1/0/all/0/1&quot;&gt;Francesco Gullo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1&quot;&gt;Bud Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramazzotti_D/0/1/0/all/0/1&quot;&gt;Daniele Ramazzotti&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.02933">
    <title>(Sequential) Importance Sampling Bandits. (arXiv:1808.02933v2 [stat.ML] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.02933</link>
    <description rdf:parseType="Literal">&lt;p&gt;The multi-armed bandit (MAB) problem is a sequential allocation task where
    the goal is to learn a policy that maximizes long term payoff, where only the
    reward of the executed action is observed; i.e., sequential optimal decisions
    are made, while simultaneously learning how the world operates. In the
    stochastic setting, the reward for each action is generated from an unknown
    distribution. To decide the next optimal action to take, one must compute
    sufficient statistics of this unknown reward distribution, e.g.
    upper-confidence bounds (UCB), or expectations in Thompson sampling.
    Closed-form expressions for these statistics of interest are analytically
    intractable except for simple cases. We here propose to leverage Monte Carlo
    estimation and, in particular, the flexibility of (sequential) importance
    sampling (IS) to allow for accurate estimation of the statistics of interest
    within the MAB problem. IS methods estimate posterior densities or expectations
    in probabilistic models that are analytically intractable. We first show how IS
    can be combined with state-of-the-art MAB algorithms (Thompson sampling and
    Bayes-UCB) for classic (Bernoulli and contextual linear-Gaussian) bandit
    problems. Furthermore, we leverage the power of sequential IS to extend the
    applicability of these algorithms beyond the classic settings, and tackle
    additional useful cases. Specifically, we study the dynamic linear-Gaussian
    bandit, and both the static and dynamic logistic cases too. The flexibility of
    (sequential) importance sampling is shown to be fundamental for obtaining
    efficient estimates of the key sufficient statistics in these challenging
    scenarios.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Urteaga_I/0/1/0/all/0/1&quot;&gt;I&amp;#xf1;igo Urteaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wiggins_C/0/1/0/all/0/1&quot;&gt;Chris H. Wiggins&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.03737">
    <title>Learning Multi-touch Conversion Attribution with Dual-attention Mechanisms for Online Advertising. (arXiv:1808.03737v2 [cs.IR] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.03737</link>
    <description rdf:parseType="Literal">&lt;p&gt;In online advertising, the Internet users may be exposed to a sequence of
    different ad campaigns, i.e., display ads, search, or referrals from multiple
    channels, before led up to any final sales conversion and transaction. For both
    campaigners and publishers, it is fundamentally critical to estimate the
    contribution from ad campaign touch-points during the customer journey
    (conversion funnel) and assign the right credit to the right ad exposure
    accordingly. However, the existing research on the multi-touch attribution
    problem lacks a principled way of utilizing the users&apos; pre-conversion actions
    (i.e., clicks), and quite often fails to model the sequential patterns among
    the touch points from a user&apos;s behavior data. To make it worse, the current
    industry practice is merely employing a set of arbitrary rules as the
    attribution model, e.g., the popular last-touch model assigns 100% credit to
    the final touch-point regardless of actual attributions. In this paper, we
    propose a Dual-attention Recurrent Neural Network (DARNN) for the multi-touch
    attribution problem. It learns the attribution values through an attention
    mechanism directly from the conversion estimation objective. To achieve this,
    we utilize sequence-to-sequence prediction for user clicks, and combine both
    post-view and post-click attribution patterns together for the final conversion
    estimation. To quantitatively benchmark attribution models, we also propose a
    novel yet practical attribution evaluation scheme through the proxy of budget
    allocation (under the estimated attributions) over ad channels. The
    experimental results on two real datasets demonstrate the significant
    performance gains of our attribution model against the state of the art.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1&quot;&gt;Kan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yuchen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuhao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiajun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.05163">
    <title>A Simple but Hard-to-Beat Baseline for Session-based Recommendations. (arXiv:1808.05163v3 [cs.IR] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.05163</link>
    <description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) models have been recently introduced in
    the domain of top-$N$ session-based recommendations. An ordered collection of
    past items the user has interacted with in a session (or sequence) are embedded
    into a 2-dimensional latent matrix, and treated as an image. The convolution
    and pooling operations are then applied to the mapped item embeddings. In this
    paper, we first examine the typical session-based CNN recommender and show that
    both the generative model and network architecture are suboptimal when modeling
    long-range dependencies in the item sequence. To address the issues, we propose
    a simple, but very effective generative model that is capable of learning
    high-level representation from both short- and long-range dependencies. The
    network architecture of the proposed model is formed of a stack of holed
    convolutional layers, which can efficiently increase the receptive fields
    without relying on the pooling operation. Another contribution is the effective
    use of residual block structure in recommender systems, which can ease the
    optimization for much deeper networks. The proposed generative model attains
    state-of-the-art accuracy with less training time in the session-based
    recommendation task. It accordingly can be used as a powerful session-based
    recommendation baseline to beat in future, especially when there are long
    sequences of user feedback.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1&quot;&gt;Fajie Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karatzoglou_A/0/1/0/all/0/1&quot;&gt;Alexandros Karatzoglou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arapakis_I/0/1/0/all/0/1&quot;&gt;Ioannis Arapakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1&quot;&gt;Joemon M Jose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiangnan He&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.05293">
    <title>Design-based Analysis in Difference-In-Differences Settings with Staggered Adoption. (arXiv:1808.05293v2 [econ.EM] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.05293</link>
    <description rdf:parseType="Literal">&lt;p&gt;In this paper we study estimation of and inference for average treatment
    effects in a setting with panel data. We focus on the setting where units,
    e.g., individuals, firms, or states, adopt the policy or treatment of interest
    at a particular point in time, and then remain exposed to this treatment at all
    times afterwards. We take a design perspective where we investigate the
    properties of estimators and procedures given assumptions on the assignment
    process. We show that under random assignment of the adoption date the standard
    Difference-In-Differences estimator is is an unbiased estimator of a particular
    weighted average causal effect. We characterize the proeperties of this
    estimand, and show that the standard variance estimator is conservative.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Athey_S/0/1/0/all/0/1&quot;&gt;Susan Athey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Imbens_G/0/1/0/all/0/1&quot;&gt;Guido Imbens&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.06052">
    <title>Doubly F-Bounded Generics. (arXiv:1808.06052v3 [cs.PL] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.06052</link>
    <description rdf:parseType="Literal">&lt;p&gt;In this paper we suggest how f-bounded generics in nominally-typed OOP can be
    extended to the more general notion we call `doubly f-bounded generics&apos; and we
    suggest how doubly f-bounded generics can be reasoned about.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AbdelGawad_M/0/1/0/all/0/1&quot;&gt;Moez A. AbdelGawad&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.06791">
    <title>LRMM: Learning to Recommend with Missing Modalities. (arXiv:1808.06791v2 [cs.IR] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.06791</link>
    <description rdf:parseType="Literal">&lt;p&gt;Multimodal learning has shown promising performance in content-based
    recommendation due to the auxiliary user and item information of multiple
    modalities such as text and images. However, the problem of incomplete and
    missing modality is rarely explored and most existing methods fail in learning
    a recommendation model with missing or corrupted modalities. In this paper, we
    propose LRMM, a novel framework that mitigates not only the problem of missing
    modalities but also more generally the cold-start problem of recommender
    systems. We propose modality dropout (m-drop) and a multimodal sequential
    autoencoder (m-auto) to learn multimodal representations for complementing and
    imputing missing modalities. Extensive experiments on real-world Amazon data
    show that LRMM achieves state-of-the-art performance on rating prediction
    tasks. More importantly, LRMM is more robust to previous methods in alleviating
    data-sparsity and the cold-start problem.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1&quot;&gt;Mathias Niepert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.07380">
    <title>On the Predictability of non-CGM Diabetes Data for Personalized Recommendation. (arXiv:1808.07380v3 [cs.CY] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.07380</link>
    <description rdf:parseType="Literal">&lt;p&gt;With continuous glucose monitoring (CGM), data-driven models on blood glucose
    prediction have been shown to be effective in related work. However, such (CGM)
    systems are not always available, e.g., for a patient at home. In this work, we
    conduct a study on 9 patients and examine the predictability of data-driven
    (aka. machine learning) based models on patient-level blood glucose prediction;
    with measurements are taken only periodically (i.e., after several hours). To
    this end, we propose several post-prediction methods to account for the noise
    nature of these data, that marginally improves the performance of the end
    system.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rokicki_M/0/1/0/all/0/1&quot;&gt;Markus Rokicki&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.08316">
    <title>A Trio Neural Model for Dynamic Entity Relatedness Ranking. (arXiv:1808.08316v2 [cs.IR] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.08316</link>
    <description rdf:parseType="Literal">&lt;p&gt;Measuring entity relatedness is a fundamental task for many natural language
    processing and information retrieval applications. Prior work often studies
    entity relatedness in static settings and an unsupervised manner. However,
    entities in real-world are often involved in many different relationships,
    consequently entity-relations are very dynamic over time. In this work, we
    propose a neural networkbased approach for dynamic entity relatedness,
    leveraging the collective attention as supervision. Our model is capable of
    learning rich and different entity representations in a joint framework.
    Through extensive experiments on large-scale datasets, we demonstrate that our
    method achieves better results than competitive baselines.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Tuan Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nejdl_W/0/1/0/all/0/1&quot;&gt;Wolfgang Nejdl&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.08518">
    <title>Word Sense Induction with Neural biLM and Symmetric Patterns. (arXiv:1808.08518v2 [cs.CL] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.08518</link>
    <description rdf:parseType="Literal">&lt;p&gt;An established method for Word Sense Induction (WSI) uses a language model to
    predict probable substitutes for target words, and induces senses by clustering
    these resulting substitute vectors.
    &lt;/p&gt;
    &lt;p&gt;We replace the ngram-based language model (LM) with a recurrent one. Beyond
    being more accurate, the use of the recurrent LM allows us to effectively query
    it in a creative way, using what we call dynamic symmetric patterns.
    &lt;/p&gt;
    &lt;p&gt;The combination of the RNN-LM and the dynamic symmetric patterns results in
    strong substitute vectors for WSI, allowing to surpass the current
    state-of-the-art on the SemEval 2013 WSI shared task by a large margin.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amrami_A/0/1/0/all/0/1&quot;&gt;Asaf Amrami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1&quot;&gt;Yoav Goldberg&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.08575">
    <title>Title-Guided Encoding for Keyphrase Generation. (arXiv:1808.08575v2 [cs.CL] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.08575</link>
    <description rdf:parseType="Literal">&lt;p&gt;Keyphrase generation (KG) aims to generate a set of keyphrases given a
    document, which is a fundamental task in natural language processing (NLP).
    Most previous methods solve this problem in an extractive manner, while
    recently, several attempts are made under the generative setting using deep
    neural networks. However, the state-of-the-art generative methods simply treat
    the document title and the document main body equally, ignoring the leading
    role of the title to the overall document. To solve this problem, we introduce
    a new model called Title-Guided Network (TG-Net) for automatic keyphrase
    generation task based on the encoder-decoder architecture with two new
    features: (i) the title is additionally employed as a query-like input, and
    (ii) a title-guided encoder gathers the relevant information from the title to
    each word in the document. Experiments on a range of KG datasets demonstrate
    that our model outperforms the state-of-the-art models with a large margin,
    especially for documents with either very low or very high title length ratios.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yifan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiani Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1&quot;&gt;Irwin King&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1&quot;&gt;Michael R. Lyu&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.08671">
    <title>Approach for Video Classification with Multi-label on YouTube-8M Dataset. (arXiv:1808.08671v2 [cs.CV] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.08671</link>
    <description rdf:parseType="Literal">&lt;p&gt;Video traffic is increasing at a considerable rate due to the spread of
    personal media and advancements in media technology. Accordingly, there is a
    growing need for techniques to automatically classify moving images. This paper
    use NetVLAD and NetFV models and the Huber loss function for video
    classification problem and YouTube-8M dataset to verify the experiment. We
    tried various attempts according to the dataset and optimize hyperparameters,
    ultimately obtain a GAP score of 0.8668.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1&quot;&gt;Kwangsoo Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1&quot;&gt;Junhyeong Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seungbin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_B/0/1/0/all/0/1&quot;&gt;Boyoung Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1&quot;&gt;Minsoo Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nang_J/0/1/0/all/0/1&quot;&gt;Jongho Nang&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.08763">
    <title>On the convergence of optimistic policy iteration for stochastic shortest path problem. (arXiv:1808.08763v2 [cs.LG] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.08763</link>
    <description rdf:parseType="Literal">&lt;p&gt;In this paper, we prove some convergence results of a special case of
    optimistic policy iteration algorithm for stochastic shortest path problem. We
    consider both Monte Carlo and $TD(\lambda)$ methods for the policy evaluation
    step under the condition that the termination state will eventually be reached
    almost surely.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuanlong Chen&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.08794">
    <title>Theoretical Foundations of the A2RD Project: Part I. (arXiv:1808.08794v3 [cs.AI] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.08794</link>
    <description rdf:parseType="Literal">&lt;p&gt;This article identifies and discusses the theoretical foundations that were
    considered in the design of the A2RD model. In addition to the points
    considered, references are made to the studies available and considered in the
    approach.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braga_J/0/1/0/all/0/1&quot;&gt;Juliao Braga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1&quot;&gt;Joao Nuno Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Endo_P/0/1/0/all/0/1&quot;&gt;Patricia Takako Endo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omar_N/0/1/0/all/0/1&quot;&gt;Nizam Omar&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.09034">
    <title>Importance Weighting and Variational Inference. (arXiv:1808.09034v2 [cs.LG] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.09034</link>
    <description rdf:parseType="Literal">&lt;p&gt;Recent work used importance sampling ideas for better variational bounds on
    likelihoods. We clarify the applicability of these ideas to pure probabilistic
    inference, by showing the resulting Importance Weighted Variational Inference
    (IWVI) technique is an instance of augmented variational inference, thus
    identifying the looseness in previous work. Experiments confirm IWVI&apos;s
    practicality for probabilistic inference. As a second contribution, we
    investigate inference with elliptical distributions, which improves accuracy in
    low dimensions, and convergence in high dimensions.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Domke_J/0/1/0/all/0/1&quot;&gt;Justin Domke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheldon_D/0/1/0/all/0/1&quot;&gt;Daniel Sheldon&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.09124">
    <title>Analysis of Frequency Agile Radar via Compressed Sensing. (arXiv:1808.09124v2 [cs.IT] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.09124</link>
    <description rdf:parseType="Literal">&lt;p&gt;Frequency agile radar (FAR) is known to have excellent electronic
    counter-countermeasures (ECCM) performance and the potential to realize
    spectrum sharing in dense electromagnetic environments. Many compressed sensing
    (CS) based algorithms have been developed for joint range and Doppler
    estimation in FAR. This paper considers theoretical analysis of FAR via CS
    algorithms. In particular, we analyze the properties of the sensing matrix,
    which is a highly structured random matrix. We then derive bounds on the number
    of recoverable targets. Numerical simulations and field experiments validate
    the theoretical findings and demonstrate the effectiveness of CS approaches to
    FAR.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tianyao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yimin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xingyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1&quot;&gt;Yonina C. Eldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiqin Wang&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.09218">
    <title>On Microtargeting Socially Divisive Ads: A Case Study of Russia-Linked Ad Campaigns on Facebook. (arXiv:1808.09218v2 [cs.SI] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.09218</link>
    <description rdf:parseType="Literal">&lt;p&gt;Targeted advertising is meant to improve the efficiency of matching
    advertisers to their customers. However, targeted advertising can also be
    abused by malicious advertisers to efficiently reach people susceptible to
    false stories, stoke grievances, and incite social conflict. Since targeted ads
    are not seen by non-targeted and non-vulnerable people, malicious ads are
    likely to go unreported and their effects undetected. This work examines a
    specific case of malicious advertising, exploring the extent to which political
    ads from the Russian Intelligence Research Agency (IRA) run prior to 2016 U.S.
    elections exploited Facebook&apos;s targeted advertising infrastructure to
    efficiently target ads on divisive or polarizing topics (e.g., immigration,
    race-based policing) at vulnerable sub-populations. In particular, we do the
    following: (a) We conduct U.S. census-representative surveys to characterize
    how users with different political ideologies report, approve, and perceive
    truth in the content of the IRA ads. Our surveys show that many ads are
    &apos;divisive&apos;: they elicit very different reactions from people belonging to
    different socially salient groups. (b) We characterize how these divisive ads
    are targeted to sub-populations that feel particularly aggrieved by the status
    quo. Our findings support existing calls for greater transparency of content
    and targeting of political ads. (c) We particularly focus on how the Facebook
    ad API facilitates such targeting. We show how the enormous amount of personal
    data Facebook aggregates about users and makes available to advertisers enables
    such malicious targeting.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_F/0/1/0/all/0/1&quot;&gt;Filipe N. Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_K/0/1/0/all/0/1&quot;&gt;Koustuv Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babaei_M/0/1/0/all/0/1&quot;&gt;Mahmoudreza Babaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henrique_L/0/1/0/all/0/1&quot;&gt;Lucas Henrique&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Messias_J/0/1/0/all/0/1&quot;&gt;Johnnatan Messias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goga_O/0/1/0/all/0/1&quot;&gt;Oana Goga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benevenuto_F/0/1/0/all/0/1&quot;&gt;Fabricio Benevenuto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gummadi_K/0/1/0/all/0/1&quot;&gt;Krishna P. Gummadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redmiles_E/0/1/0/all/0/1&quot;&gt;Elissa M. Redmiles&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.09492">
    <title>Learning to Attend On Essential Terms: An Enhanced Retriever-Reader Model for Scientific Question Answering. (arXiv:1808.09492v2 [cs.CL] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.09492</link>
    <description rdf:parseType="Literal">&lt;p&gt;Scientific Question Answering (SQA) is a challenging open-domain task which
    requires the capability to understand questions and choices, collect useful
    information, and reason over evidence. Previous work typically formulates this
    task as a reading comprehension or entailment problem given evidence retrieved
    from search engines. However, existing techniques struggle to retrieve
    indirectly related evidence when no directly related evidence is provided,
    especially for complex questions where it is hard to parse precisely what the
    question asks. In this paper we propose a retriever-reader model that learns to
    attend on essential terms during the question answering process. We build 1) an
    essential-term-aware `retriever&apos; which first identifies the most important
    words in a question, then reformulates the queries and searches for related
    evidence 2) an enhanced `reader&apos; to distinguish between essential terms and
    distracting words to predict the answer. We experimentally evaluate our model
    on the ARC dataset where it outperforms the existing state-of-the-art model by
    7.4%.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1&quot;&gt;Jianmo Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chenguang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weizhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1&quot;&gt;Julian McAuley&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.09526">
    <title>Deep Lidar CNN to Understand the Dynamics of Moving Vehicles. (arXiv:1808.09526v2 [cs.CV] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.09526</link>
    <description rdf:parseType="Literal">&lt;p&gt;Perception technologies in Autonomous Driving are experiencing their golden
    age due to the advances in Deep Learning. Yet, most of these systems rely on
    the semantically rich information of RGB images. Deep Learning solutions
    applied to the data of other sensors typically mounted on autonomous cars (e.g.
    lidars or radars) are not explored much. In this paper we propose a novel
    solution to understand the dynamics of moving vehicles of the scene from only
    lidar information. The main challenge of this problem stems from the fact that
    we need to disambiguate the proprio-motion of the &apos;observer&apos; vehicle from that
    of the external &apos;observed&apos; vehicles. For this purpose, we devise a CNN
    architecture which at testing time is fed with pairs of consecutive lidar
    scans. However, in order to properly learn the parameters of this network,
    during training we introduce a series of so-called pretext tasks which also
    leverage on image data. These tasks include semantic information about
    vehicleness and a novel lidar-flow feature which combines standard image-based
    optical flow with lidar scans. We obtain very promising results and show that
    including distilled image information only during training, allows improving
    the inference results of the network at test time, even when image data is no
    longer used.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaquero_V/0/1/0/all/0/1&quot;&gt;Victor Vaquero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanfeliu_A/0/1/0/all/0/1&quot;&gt;Alberto Sanfeliu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1&quot;&gt;Francesc Moreno-Noguer&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.09772">
    <title>Notes on Deep Learning for NLP. (arXiv:1808.09772v2 [cs.CL] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.09772</link>
    <description rdf:parseType="Literal">&lt;p&gt;My notes on Deep Learning for NLP.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tixier_A/0/1/0/all/0/1&quot;&gt;Antoine J.-P. Tixier&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.09830">
    <title>Searching Toward Pareto-Optimal Device-Aware Neural Architectures. (arXiv:1808.09830v2 [cs.LG] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.09830</link>
    <description rdf:parseType="Literal">&lt;p&gt;Recent breakthroughs in Neural Architectural Search (NAS) have achieved
    state-of-the-art performance in many tasks such as image classification and
    language understanding. However, most existing works only optimize for model
    accuracy and largely ignore other important factors imposed by the underlying
    hardware and devices, such as latency and energy, when making inference. In
    this paper, we first introduce the problem of NAS and provide a survey on
    recent works. Then we deep dive into two recent advancements on extending NAS
    into multiple-objective frameworks: MONAS and DPP-Net. Both MONAS and DPP-Net
    are capable of optimizing accuracy and other objectives imposed by devices,
    searching for neural architectures that can be best deployed on a wide spectrum
    of devices: from embedded systems and mobile devices to workstations.
    Experimental results are poised to show that architectures found by MONAS and
    DPP-Net achieves Pareto optimality w.r.t the given objectives for various
    devices.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1&quot;&gt;An-Chieh Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jin-Dong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1&quot;&gt;Chi-Hung Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shu-Huan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Min Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shih-Chieh Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jia-Yu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu-Ting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1&quot;&gt;Da-Cheng Juan&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.09888">
    <title>KDSL: a Knowledge-Driven Supervised Learning Framework for Word Sense Disambiguation. (arXiv:1808.09888v2 [cs.CL] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.09888</link>
    <description rdf:parseType="Literal">&lt;p&gt;We propose KDSL, a new word sense disambiguation (WSD) framework that
    utilizes knowledge to automatically generate sense-labeled data for supervised
    learning. First, from WordNet, we automatically construct a semantic knowledge
    base called DisDict, which provides refined feature words that highlight the
    differences among word senses, i.e., synsets. Second, we automatically generate
    new sense-labeled data by DisDict from unlabeled corpora. Third, these
    generated data, together with manually labeled data and unlabeled data, are fed
    to a neural framework conducting supervised and unsupervised learning jointly
    to model the semantic relations among synsets, feature words and their
    contexts. The experimental results show that KDSL outperforms several
    representative state-of-the-art methods on various major benchmarks.
    Interestingly, it performs relatively well even when manually labeled data is
    unavailable, thus provides a new promising backoff strategy for WSD.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1&quot;&gt;Shi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenguang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shangfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jianmin Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoping Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruili Wang&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.09891">
    <title>A Quantum Many-body Wave Function Inspired Language Modeling Approach. (arXiv:1808.09891v2 [cs.CL] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.09891</link>
    <description rdf:parseType="Literal">&lt;p&gt;The recently proposed quantum language model (QLM) aimed at a principled
    approach to modeling term dependency by applying the quantum probability
    theory. The latest development for a more effective QLM has adopted word
    embeddings as a kind of global dependency information and integrated the
    quantum-inspired idea in a neural network architecture. While these
    quantum-inspired LMs are theoretically more general and also practically
    effective, they have two major limitations. First, they have not taken into
    account the interaction among words with multiple meanings, which is common and
    important in understanding natural language text. Second, the integration of
    the quantum-inspired LM with the neural network was mainly for effective
    training of parameters, yet lacking a theoretical foundation accounting for
    such integration. To address these two issues, in this paper, we propose a
    Quantum Many-body Wave Function (QMWF) inspired language modeling approach. The
    QMWF inspired LM can adopt the tensor product to model the aforesaid
    interaction among words. It also enables us to reveal the inherent necessity of
    using Convolutional Neural Network (CNN) in QMWF language modeling.
    Furthermore, our approach delivers a simple algorithm to represent and match
    text/sentence pairs. Systematic evaluation shows the effectiveness of the
    proposed QMWF-LM algorithm, in comparison with the state of the art
    quantum-inspired LMs and a couple of CNN-based methods, on three typical
    Question Answering (QA) datasets.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1&quot;&gt;Zhan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Benyou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dawei Song&lt;/a&gt;</dc:creator>
  </item>
  <item rdf:about="http://arxiv.org/abs/1808.09902">
    <title>Extreme Value Theory for Open Set Classification - GPD and GEV Classifiers. (arXiv:1808.09902v2 [stat.ML] UPDATED)</title>
    <link>http://arxiv.org/abs/1808.09902</link>
    <description rdf:parseType="Literal">&lt;p&gt;Classification tasks usually assume that all possible classes are present
    during the training phase. This is restrictive if the algorithm is used over a
    long time and possibly encounters samples from unknown classes. The recently
    introduced extreme value machine, a classifier motivated by extreme value
    theory, addresses this problem and achieves competitive performance in specific
    cases. We show that this algorithm can fail when the geometries of known and
    unknown classes differ. To overcome this problem, we propose two new algorithms
    relying on approximations from extreme value theory. We show the effectiveness
    of our classifiers in simulations and on the LETTER and MNIST data sets.
    &lt;/p&gt;
    </description>
    <dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vignotto_E/0/1/0/all/0/1&quot;&gt;Edoardo Vignotto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Engelke_S/0/1/0/all/0/1&quot;&gt;Sebastian Engelke&lt;/a&gt;</dc:creator>
  </item>
</rdf:RDF>
